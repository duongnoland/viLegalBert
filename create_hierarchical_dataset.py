import json
import pandas as pd
import random
from pathlib import Path
from collections import defaultdict, Counter
import re

def clean_text(text):
    """L√†m s·∫°ch vƒÉn b·∫£n"""
    if not text:
        return ""
    
    # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát v√† chu·∫©n h√≥a kho·∫£ng tr·∫Øng
    text = re.sub(r'\s+', ' ', text.strip())
    text = re.sub(r'[^\w\s\-\.\,\;\:\!\?\(\)\[\]\{\}]', '', text)
    return text

def extract_document_type(type_text):
    """Tr√≠ch xu·∫•t lo·∫°i vƒÉn b·∫£n c∆° b·∫£n t·ª´ tr∆∞·ªùng type"""
    if not type_text:
        return "KH√ÅC"
    
    type_text = type_text.upper().strip()
    
    # Mapping c√°c lo·∫°i vƒÉn b·∫£n c∆° b·∫£n
    type_mapping = {
        "LU·∫¨T": "LU·∫¨T",
        "NGH·ªä ƒê·ªäNH": "NGH·ªä ƒê·ªäNH", 
        "TH√îNG T∆Ø": "TH√îNG T∆Ø",
        "NGH·ªä QUY·∫æT": "NGH·ªä QUY·∫æT",
        "QUY·∫æT ƒê·ªäNH": "QUY·∫æT ƒê·ªäNH",
        "CH·ªà TH·ªä": "CH·ªà TH·ªä",
        "PH√ÅP L·ªÜNH": "PH√ÅP L·ªÜNH",
        "NGH·ªä QUY·∫æT LI√äN T·ªäCH": "NGH·ªä QUY·∫æT LI√äN T·ªäCH",
        "TH√îNG T∆Ø LI√äN T·ªäCH": "TH√îNG T∆Ø LI√äN T·ªäCH",
        "NGH·ªä ƒê·ªäNH LI√äN T·ªäCH": "NGH·ªä ƒê·ªäNH LI√äN T·ªäCH"
    }
    
    for key, value in type_mapping.items():
        if key in type_text:
            return value
    
    return "KH√ÅC"

def extract_legal_domain(content, name, chapter_name):
    """Tr√≠ch xu·∫•t domain ph√°p l√Ω t·ª´ n·ªôi dung"""
    if not content:
        return "KH√ÅC"
    
    # K·∫øt h·ª£p n·ªôi dung ƒë·ªÉ ph√¢n t√≠ch
    full_text = f"{name} {chapter_name} {content}".upper()
    
    # Mapping c√°c domain ph√°p l√Ω v·ªõi t·ª´ kh√≥a ti·∫øng Vi·ªát
    domain_keywords = {
        "H√åNH S·ª∞": [
            "h√¨nh s·ª±", "t·ªôi ph·∫°m", "x·ª≠ l√Ω vi ph·∫°m", "ph·∫°t t√π", "c·∫£i t·∫°o", 
            "truy c·ª©u tr√°ch nhi·ªám", "h√¨nh ph·∫°t", "t·ªôi danh", "v·ª• √°n", "b·ªã can",
            "b·ªã c√°o", "th·∫©m ph√°n", "ki·ªÉm s√°t vi√™n", "lu·∫≠t s∆∞", "t√≤a √°n"
        ],
        "D√ÇN S·ª∞": [
            "d√¢n s·ª±", "h·ª£p ƒë·ªìng", "quy·ªÅn s·ªü h·ªØu", "th·ª´a k·∫ø", "h√¥n nh√¢n gia ƒë√¨nh", 
            "b·ªìi th∆∞·ªùng", "tranh ch·∫•p", "quy·ªÅn l·ª£i", "nghƒ©a v·ª•", "t√†i s·∫£n",
            "quy·ªÅn t√†i s·∫£n", "quy·ªÅn nh√¢n th√¢n", "b·∫£o v·ªá quy·ªÅn l·ª£i"
        ],
        "H√ÄNH CH√çNH": [
            "h√†nh ch√≠nh", "x·ª≠ ph·∫°t vi ph·∫°m", "th·ªß t·ª•c h√†nh ch√≠nh", "quy·∫øt ƒë·ªãnh h√†nh ch√≠nh",
            "khi·∫øu n·∫°i", "t·ªë c√°o", "c∆° quan h√†nh ch√≠nh", "ch√≠nh quy·ªÅn", "·ªßy ban",
            "s·ªü", "ph√≤ng", "ban", "c∆° quan nh√† n∆∞·ªõc"
        ],
        "LAO ƒê·ªòNG": [
            "lao ƒë·ªông", "h·ª£p ƒë·ªìng lao ƒë·ªông", "ti·ªÅn l∆∞∆°ng", "b·∫£o hi·ªÉm x√£ h·ªôi", 
            "an to√†n lao ƒë·ªông", "th·ªùi gian l√†m vi·ªác", "ngh·ªâ ph√©p", "ƒë√¨nh c√¥ng",
            "ng∆∞·ªùi lao ƒë·ªông", "ng∆∞·ªùi s·ª≠ d·ª•ng lao ƒë·ªông", "quan h·ªá lao ƒë·ªông"
        ],
        "THU·∫æ": [
            "thu·∫ø", "thu·∫ø thu nh·∫≠p", "thu·∫ø gi√° tr·ªã gia tƒÉng", "thu·∫ø xu·∫•t nh·∫≠p kh·∫©u", 
            "khai thu·∫ø", "n·ªôp thu·∫ø", "ho√†n thu·∫ø", "mi·ªÖn thu·∫ø", "gi·∫£m thu·∫ø",
            "c∆° quan thu·∫ø", "t·ªïng c·ª•c thu·∫ø", "chi c·ª•c thu·∫ø"
        ],
        "DOANH NGHI·ªÜP": [
            "doanh nghi·ªáp", "c√¥ng ty", "th√†nh l·∫≠p doanh nghi·ªáp", "qu·∫£n l√Ω doanh nghi·ªáp",
            "ƒëƒÉng k√Ω kinh doanh", "gi·∫•y ph√©p kinh doanh", "v·ªën ƒëi·ªÅu l·ªá", "c·ªï ƒë√¥ng",
            "h·ªôi ƒë·ªìng qu·∫£n tr·ªã", "gi√°m ƒë·ªëc", "ph√≥ gi√°m ƒë·ªëc"
        ],
        "ƒê·∫§T ƒêAI": [
            "ƒë·∫•t ƒëai", "quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t", "th·ªß t·ª•c ƒë·∫•t ƒëai", "b·ªìi th∆∞·ªùng ƒë·∫•t ƒëai",
            "gi·∫•y ch·ª©ng nh·∫≠n quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t", "quy ho·∫°ch ƒë·∫•t ƒëai", "thu h·ªìi ƒë·∫•t",
            "giao ƒë·∫•t", "cho thu√™ ƒë·∫•t", "chuy·ªÉn ƒë·ªïi m·ª•c ƒë√≠ch s·ª≠ d·ª•ng ƒë·∫•t"
        ],
        "X√ÇY D·ª∞NG": [
            "x√¢y d·ª±ng", "gi·∫•y ph√©p x√¢y d·ª±ng", "quy ho·∫°ch", "ki·∫øn tr√∫c", "thi·∫øt k·∫ø",
            "thi c√¥ng", "gi√°m s√°t", "nghi·ªám thu", "b·∫£o h√†nh", "b·∫£o tr√¨",
            "c√¥ng tr√¨nh x√¢y d·ª±ng", "d·ª± √°n x√¢y d·ª±ng"
        ],
        "GIAO TH√îNG": [
            "giao th√¥ng", "lu·∫≠t giao th√¥ng", "vi ph·∫°m giao th√¥ng", "ph∆∞∆°ng ti·ªán giao th√¥ng",
            "ƒë∆∞·ªùng b·ªô", "ƒë∆∞·ªùng s·∫Øt", "ƒë∆∞·ªùng th·ªßy", "ƒë∆∞·ªùng h√†ng kh√¥ng", "bi·ªÉn b√°o",
            "ƒë√®n t√≠n hi·ªáu", "v·∫°ch k·∫ª ƒë∆∞·ªùng", "c·∫ßu ƒë∆∞·ªùng"
        ],
        "Y T·∫æ": [
            "y t·∫ø", "kh√°m ch·ªØa b·ªánh", "d∆∞·ª£c ph·∫©m", "v·ªá sinh an to√†n th·ª±c ph·∫©m",
            "b·ªánh vi·ªán", "ph√≤ng kh√°m", "b√°c sƒ©", "y t√°", "d∆∞·ª£c sƒ©", "thu·ªëc",
            "thi·∫øt b·ªã y t·∫ø", "d·ªãch v·ª• y t·∫ø", "b·∫£o hi·ªÉm y t·∫ø"
        ],
        "GI√ÅO D·ª§C": [
            "gi√°o d·ª•c", "ƒë√†o t·∫°o", "ch∆∞∆°ng tr√¨nh gi√°o d·ª•c", "b·∫±ng c·∫•p", "ch·ª©ng ch·ªâ",
            "tr∆∞·ªùng h·ªçc", "gi√°o vi√™n", "h·ªçc sinh", "sinh vi√™n", "gi·∫£ng vi√™n",
            "ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o", "c∆° s·ªü gi√°o d·ª•c"
        ],
        "T√ÄI CH√çNH": [
            "t√†i ch√≠nh", "ng√¢n h√†ng", "t√≠n d·ª•ng", "ti·ªÅn t·ªá", "ƒë·∫ßu t∆∞", "cho vay",
            "ti·∫øt ki·ªám", "b·∫£o hi·ªÉm", "ch·ª©ng kho√°n", "qu·ªπ ƒë·∫ßu t∆∞", "c√¥ng ty t√†i ch√≠nh",
            "ng√¢n h√†ng nh√† n∆∞·ªõc", "ng√¢n h√†ng th∆∞∆°ng m·∫°i"
        ],
        "M√îI TR∆Ø·ªúNG": [
            "m√¥i tr∆∞·ªùng", "b·∫£o v·ªá m√¥i tr∆∞·ªùng", "√¥ nhi·ªÖm", "x·ª≠ l√Ω ch·∫•t th·∫£i",
            "kh√≠ th·∫£i", "n∆∞·ªõc th·∫£i", "r√°c th·∫£i", "ti·∫øng ·ªìn", "b·ª•i", "h√≥a ch·∫•t",
            "ƒë√°nh gi√° t√°c ƒë·ªông m√¥i tr∆∞·ªùng", "gi·∫•y ph√©p m√¥i tr∆∞·ªùng"
        ],
        "AN NINH": [
            "an ninh", "qu·ªëc ph√≤ng", "b·∫£o v·ªá an ninh", "tr·∫≠t t·ª± an to√†n x√£ h·ªôi",
            "c√¥ng an", "b·ªô ƒë·ªôi", "qu√¢n ƒë·ªôi", "c·∫£nh s√°t", "an ninh qu·ªëc gia",
            "an ninh tr·∫≠t t·ª±", "ph√≤ng ch·ªëng t·ªôi ph·∫°m"
        ]
    }
    
    # ƒê·∫øm s·ªë t·ª´ kh√≥a xu·∫•t hi·ªán cho m·ªói domain
    domain_scores = {}
    for domain, keywords in domain_keywords.items():
        score = 0
        for keyword in keywords:
            # T√¨m ki·∫øm t·ª´ kh√≥a trong vƒÉn b·∫£n (kh√¥ng ph√¢n bi·ªát d·∫•u)
            if keyword.upper() in full_text:
                score += 1
            # T√¨m ki·∫øm v·ªõi c√°c bi·∫øn th·ªÉ d·∫•u
            elif keyword.replace(' ', '').upper() in full_text.replace(' ', ''):
                score += 1
        
        if score > 0:
            domain_scores[domain] = score
    
    # Tr·∫£ v·ªÅ domain c√≥ ƒëi·ªÉm cao nh·∫•t
    if domain_scores:
        best_domain = max(domain_scores, key=domain_scores.get)
        print(f"üîç Domain ƒë∆∞·ª£c ch·ªçn: {best_domain} (ƒëi·ªÉm: {domain_scores[best_domain]})")
        return best_domain
    
    return "KH√ÅC"

def create_hierarchical_dataset(json_file_path, output_csv_path, target_size=10000):
    """T·∫°o dataset ph√¢n c·∫•p 2 t·∫ßng t·ª´ file JSON"""
    
    print(f"üöÄ B·∫Øt ƒë·∫ßu t·∫°o dataset t·ª´ file: {json_file_path}")
    
    # Ki·ªÉm tra file t·ªìn t·∫°i
    if not Path(json_file_path).exists():
        raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {json_file_path}")
    
    # Load d·ªØ li·ªáu JSON
    with open(json_file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"‚úÖ Load th√†nh c√¥ng {len(data)} items t·ª´ JSON")
    
    # Chu·∫©n b·ªã d·ªØ li·ªáu cho dataset
    dataset_items = []
    
    # L·∫•y m·∫´u ng·∫´u nhi√™n ƒë·ªÉ ƒë·∫£m b·∫£o ƒëa d·∫°ng
    if len(data) > target_size:
        sampled_data = random.sample(data, target_size)
    else:
        sampled_data = data
    
    print(f"üìä X·ª≠ l√Ω {len(sampled_data)} items...")
    
    for item in sampled_data:
        try:
            # Tr√≠ch xu·∫•t th√¥ng tin c∆° b·∫£n
            doc_id = item.get('id', '')
            doc_type = extract_document_type(item.get('type', ''))
            doc_name = clean_text(item.get('name', ''))
            ministry = clean_text(item.get('ministry', ''))
            chapter_name = clean_text(item.get('chapter_name', ''))
            article = clean_text(item.get('article', ''))
            content = clean_text(item.get('content', ''))
            
            # T·∫°o vƒÉn b·∫£n ƒë·∫ßy ƒë·ªß ƒë·ªÉ ph√¢n lo·∫°i
            full_text = f"{doc_name} {chapter_name} {article} {content}"
            
            # Tr√≠ch xu·∫•t domain ph√°p l√Ω
            legal_domain = extract_legal_domain(content, doc_name, chapter_name)
            
            # T·∫°o item cho dataset
            dataset_item = {
                'id': doc_id,
                'text': full_text,
                'type_level1': doc_type,  # T·∫ßng 1: Lo·∫°i vƒÉn b·∫£n c∆° b·∫£n
                'domain_level2': legal_domain,  # T·∫ßng 2: Domain ph√°p l√Ω
                'ministry': ministry,
                'name': doc_name,
                'chapter': chapter_name,
                'article': article,
                'content_length': len(content)
            }
            
            dataset_items.append(dataset_item)
            
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω item: {e}")
            continue
    
    # T·∫°o DataFrame
    df = pd.DataFrame(dataset_items)
    
    # Th·ªëng k√™ dataset
    print(f"\nüìà TH·ªêNG K√ä DATASET:")
    print(f"T·ªïng s·ªë samples: {len(df)}")
    
    print(f"\nüè∑Ô∏è PH√ÇN LO·∫†I T·∫¶NG 1 (Lo·∫°i vƒÉn b·∫£n):")
    type_counts = df['type_level1'].value_counts()
    for doc_type, count in type_counts.items():
        print(f"  - {doc_type}: {count}")
    
    print(f"\nüè∑Ô∏è PH√ÇN LO·∫†I T·∫¶NG 2 (Domain ph√°p l√Ω):")
    domain_counts = df['domain_level2'].value_counts()
    for domain, count in domain_counts.items():
        print(f"  - {domain}: {count}")
    
    # T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a c√≥
    output_dir = Path(output_csv_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # L∆∞u dataset
    df.to_csv(output_csv_path, index=False, encoding='utf-8')
    print(f"\n‚úÖ ƒê√£ l∆∞u dataset v√†o: {output_csv_path}")
    
    return df

def create_training_splits(dataset_path, output_dir):
    """T·∫°o c√°c t·∫≠p train/validation/test t·ª´ dataset"""
    
    print(f"\nüîÑ T·∫°o c√°c t·∫≠p train/validation/test...")
    
    # Load dataset
    df = pd.read_csv(dataset_path, encoding='utf-8')
    
    # T·∫°o th∆∞ m·ª•c output
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # Chia d·ªØ li·ªáu theo t·ª∑ l·ªá 70/15/15
    total_size = len(df)
    train_size = int(0.7 * total_size)
    val_size = int(0.15 * total_size)
    
    # Shuffle d·ªØ li·ªáu
    df_shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)
    
    # Chia d·ªØ li·ªáu
    train_df = df_shuffled[:train_size]
    val_df = df_shuffled[train_size:train_size + val_size]
    test_df = df_shuffled[train_size + val_size:]
    
    # L∆∞u c√°c t·∫≠p
    train_path = output_path / "train.csv"
    val_path = output_path / "validation.csv"
    test_path = output_path / "test.csv"
    
    train_df.to_csv(train_path, index=False, encoding='utf-8')
    val_df.to_csv(val_path, index=False, encoding='utf-8')
    test_df.to_csv(test_path, index=False, encoding='utf-8')
    
    print(f"‚úÖ Train set: {len(train_df)} samples -> {train_path}")
    print(f"‚úÖ Validation set: {len(val_df)} samples -> {val_path}")
    print(f"‚úÖ Test set: {len(test_df)} samples -> {test_path}")
    
    return train_df, val_df, test_df

if __name__ == "__main__":
    # ƒê∆∞·ªùng d·∫´n file - ƒê√É S·ª¨A
    json_file = "data/raw/vbpl_crawl.json"  # ‚úÖ ƒê∆∞·ªùng d·∫´n ƒë√∫ng
    output_csv = "data/processed/hierarchical_legal_dataset.csv"  # ‚úÖ L∆∞u v√†o processed
    splits_dir = "data/processed/dataset_splits"  # ‚úÖ L∆∞u v√†o processed
    
    print("üîç Ki·ªÉm tra c·∫•u tr√∫c th∆∞ m·ª•c...")
    
    # Ki·ªÉm tra file JSON
    if not Path(json_file).exists():
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {json_file}")
        print("üí° H√£y ƒë·∫£m b·∫£o file vbpl_crawl.json ƒë√£ ƒë∆∞·ª£c di chuy·ªÉn v√†o data/raw/")
        exit(1)
    
    print(f"‚úÖ T√¨m th·∫•y file JSON: {json_file}")
    
    try:
        # T·∫°o dataset ch√≠nh
        df = create_hierarchical_dataset(json_file, output_csv, target_size=10000)
        
        # T·∫°o c√°c t·∫≠p train/validation/test
        create_training_splits(output_csv, splits_dir)
        
        print(f"\nüéâ HO√ÄN TH√ÄNH! Dataset ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng:")
        print(f"  - Dataset ch√≠nh: {output_csv}")
        print(f"  - C√°c t·∫≠p chia: {splits_dir}/")
        print(f"  - T·ªïng s·ªë samples: {len(df)}")
        
        # Th√¥ng tin v·ªÅ c·∫•u tr√∫c th∆∞ m·ª•c
        print(f"\nüìÅ C·∫•u tr√∫c th∆∞ m·ª•c ƒë√£ t·∫°o:")
        print(f"  - data/processed/hierarchical_legal_dataset.csv")
        print(f"  - data/processed/dataset_splits/train.csv")
        print(f"  - data/processed/dataset_splits/validation.csv")
        print(f"  - data/processed/dataset_splits/test.csv")
        
    except Exception as e:
        print(f"‚ùå L·ªói trong qu√° tr√¨nh t·∫°o dataset: {e}")
        exit(1) 