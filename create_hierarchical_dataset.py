import json
import pandas as pd
import random
from pathlib import Path
from collections import defaultdict, Counter
import re

def clean_text(text):
    """LÃ m sáº¡ch vÄƒn báº£n"""
    if not text:
        return ""
    
    # Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t vÃ  chuáº©n hÃ³a khoáº£ng tráº¯ng
    text = re.sub(r'\s+', ' ', text.strip())
    text = re.sub(r'[^\w\s\-\.\,\;\:\!\?\(\)\[\]\{\}]', '', text)
    return text

def extract_document_type(type_text):
    """TrÃ­ch xuáº¥t loáº¡i vÄƒn báº£n cÆ¡ báº£n tá»« trÆ°á»ng type"""
    if not type_text:
        return "KHÃC"
    
    type_text = type_text.upper().strip()
    
    # Mapping cÃ¡c loáº¡i vÄƒn báº£n cÆ¡ báº£n
    type_mapping = {
        "LUáº¬T": "LUáº¬T",
        "NGHá»Š Äá»ŠNH": "NGHá»Š Äá»ŠNH", 
        "THÃ”NG TÆ¯": "THÃ”NG TÆ¯",
        "NGHá»Š QUYáº¾T": "NGHá»Š QUYáº¾T",
        "QUYáº¾T Äá»ŠNH": "QUYáº¾T Äá»ŠNH",
        "CHá»ˆ THá»Š": "CHá»ˆ THá»Š",
        "PHÃP Lá»†NH": "PHÃP Lá»†NH",
        "NGHá»Š QUYáº¾T LIÃŠN Tá»ŠCH": "NGHá»Š QUYáº¾T LIÃŠN Tá»ŠCH",
        "THÃ”NG TÆ¯ LIÃŠN Tá»ŠCH": "THÃ”NG TÆ¯ LIÃŠN Tá»ŠCH",
        "NGHá»Š Äá»ŠNH LIÃŠN Tá»ŠCH": "NGHá»Š Äá»ŠNH LIÃŠN Tá»ŠCH"
    }
    
    for key, value in type_mapping.items():
        if key in type_text:
            return value
    
    return "KHÃC"

def extract_legal_domain(content, name, chapter_name):
    """TrÃ­ch xuáº¥t domain phÃ¡p lÃ½ tá»« ná»™i dung"""
    if not content:
        return "KHÃC"
    
    # Káº¿t há»£p ná»™i dung Ä‘á»ƒ phÃ¢n tÃ­ch
    full_text = f"{name} {chapter_name} {content}".upper()
    
    # Mapping cÃ¡c domain phÃ¡p lÃ½ vá»›i tá»« khÃ³a tiáº¿ng Viá»‡t
    domain_keywords = {
        "HÃŒNH Sá»°": [
            "hÃ¬nh sá»±", "tá»™i pháº¡m", "xá»­ lÃ½ vi pháº¡m", "pháº¡t tÃ¹", "cáº£i táº¡o", 
            "truy cá»©u trÃ¡ch nhiá»‡m", "hÃ¬nh pháº¡t", "tá»™i danh", "vá»¥ Ã¡n", "bá»‹ can",
            "bá»‹ cÃ¡o", "tháº©m phÃ¡n", "kiá»ƒm sÃ¡t viÃªn", "luáº­t sÆ°", "tÃ²a Ã¡n"
        ],
        "DÃ‚N Sá»°": [
            "dÃ¢n sá»±", "há»£p Ä‘á»“ng", "quyá»n sá»Ÿ há»¯u", "thá»«a káº¿", "hÃ´n nhÃ¢n gia Ä‘Ã¬nh", 
            "bá»“i thÆ°á»ng", "tranh cháº¥p", "quyá»n lá»£i", "nghÄ©a vá»¥", "tÃ i sáº£n",
            "quyá»n tÃ i sáº£n", "quyá»n nhÃ¢n thÃ¢n", "báº£o vá»‡ quyá»n lá»£i"
        ],
        "HÃ€NH CHÃNH": [
            "hÃ nh chÃ­nh", "xá»­ pháº¡t vi pháº¡m", "thá»§ tá»¥c hÃ nh chÃ­nh", "quyáº¿t Ä‘á»‹nh hÃ nh chÃ­nh",
            "khiáº¿u náº¡i", "tá»‘ cÃ¡o", "cÆ¡ quan hÃ nh chÃ­nh", "chÃ­nh quyá»n", "á»§y ban",
            "sá»Ÿ", "phÃ²ng", "ban", "cÆ¡ quan nhÃ  nÆ°á»›c"
        ],
        "LAO Äá»˜NG": [
            "lao Ä‘á»™ng", "há»£p Ä‘á»“ng lao Ä‘á»™ng", "tiá»n lÆ°Æ¡ng", "báº£o hiá»ƒm xÃ£ há»™i", 
            "an toÃ n lao Ä‘á»™ng", "thá»i gian lÃ m viá»‡c", "nghá»‰ phÃ©p", "Ä‘Ã¬nh cÃ´ng",
            "ngÆ°á»i lao Ä‘á»™ng", "ngÆ°á»i sá»­ dá»¥ng lao Ä‘á»™ng", "quan há»‡ lao Ä‘á»™ng"
        ],
        "THUáº¾": [
            "thuáº¿", "thuáº¿ thu nháº­p", "thuáº¿ giÃ¡ trá»‹ gia tÄƒng", "thuáº¿ xuáº¥t nháº­p kháº©u", 
            "khai thuáº¿", "ná»™p thuáº¿", "hoÃ n thuáº¿", "miá»…n thuáº¿", "giáº£m thuáº¿",
            "cÆ¡ quan thuáº¿", "tá»•ng cá»¥c thuáº¿", "chi cá»¥c thuáº¿"
        ],
        "DOANH NGHIá»†P": [
            "doanh nghiá»‡p", "cÃ´ng ty", "thÃ nh láº­p doanh nghiá»‡p", "quáº£n lÃ½ doanh nghiá»‡p",
            "Ä‘Äƒng kÃ½ kinh doanh", "giáº¥y phÃ©p kinh doanh", "vá»‘n Ä‘iá»u lá»‡", "cá»• Ä‘Ã´ng",
            "há»™i Ä‘á»“ng quáº£n trá»‹", "giÃ¡m Ä‘á»‘c", "phÃ³ giÃ¡m Ä‘á»‘c"
        ],
        "Äáº¤T ÄAI": [
            "Ä‘áº¥t Ä‘ai", "quyá»n sá»­ dá»¥ng Ä‘áº¥t", "thá»§ tá»¥c Ä‘áº¥t Ä‘ai", "bá»“i thÆ°á»ng Ä‘áº¥t Ä‘ai",
            "giáº¥y chá»©ng nháº­n quyá»n sá»­ dá»¥ng Ä‘áº¥t", "quy hoáº¡ch Ä‘áº¥t Ä‘ai", "thu há»“i Ä‘áº¥t",
            "giao Ä‘áº¥t", "cho thuÃª Ä‘áº¥t", "chuyá»ƒn Ä‘á»•i má»¥c Ä‘Ã­ch sá»­ dá»¥ng Ä‘áº¥t"
        ],
        "XÃ‚Y Dá»°NG": [
            "xÃ¢y dá»±ng", "giáº¥y phÃ©p xÃ¢y dá»±ng", "quy hoáº¡ch", "kiáº¿n trÃºc", "thiáº¿t káº¿",
            "thi cÃ´ng", "giÃ¡m sÃ¡t", "nghiá»‡m thu", "báº£o hÃ nh", "báº£o trÃ¬",
            "cÃ´ng trÃ¬nh xÃ¢y dá»±ng", "dá»± Ã¡n xÃ¢y dá»±ng"
        ],
        "GIAO THÃ”NG": [
            "giao thÃ´ng", "luáº­t giao thÃ´ng", "vi pháº¡m giao thÃ´ng", "phÆ°Æ¡ng tiá»‡n giao thÃ´ng",
            "Ä‘Æ°á»ng bá»™", "Ä‘Æ°á»ng sáº¯t", "Ä‘Æ°á»ng thá»§y", "Ä‘Æ°á»ng hÃ ng khÃ´ng", "biá»ƒn bÃ¡o",
            "Ä‘Ã¨n tÃ­n hiá»‡u", "váº¡ch káº» Ä‘Æ°á»ng", "cáº§u Ä‘Æ°á»ng"
        ],
        "Y Táº¾": [
            "y táº¿", "khÃ¡m chá»¯a bá»‡nh", "dÆ°á»£c pháº©m", "vá»‡ sinh an toÃ n thá»±c pháº©m",
            "bá»‡nh viá»‡n", "phÃ²ng khÃ¡m", "bÃ¡c sÄ©", "y tÃ¡", "dÆ°á»£c sÄ©", "thuá»‘c",
            "thiáº¿t bá»‹ y táº¿", "dá»‹ch vá»¥ y táº¿", "báº£o hiá»ƒm y táº¿"
        ],
        "GIÃO Dá»¤C": [
            "giÃ¡o dá»¥c", "Ä‘Ã o táº¡o", "chÆ°Æ¡ng trÃ¬nh giÃ¡o dá»¥c", "báº±ng cáº¥p", "chá»©ng chá»‰",
            "trÆ°á»ng há»c", "giÃ¡o viÃªn", "há»c sinh", "sinh viÃªn", "giáº£ng viÃªn",
            "chÆ°Æ¡ng trÃ¬nh Ä‘Ã o táº¡o", "cÆ¡ sá»Ÿ giÃ¡o dá»¥c"
        ],
        "TÃ€I CHÃNH": [
            "tÃ i chÃ­nh", "ngÃ¢n hÃ ng", "tÃ­n dá»¥ng", "tiá»n tá»‡", "Ä‘áº§u tÆ°", "cho vay",
            "tiáº¿t kiá»‡m", "báº£o hiá»ƒm", "chá»©ng khoÃ¡n", "quá»¹ Ä‘áº§u tÆ°", "cÃ´ng ty tÃ i chÃ­nh",
            "ngÃ¢n hÃ ng nhÃ  nÆ°á»›c", "ngÃ¢n hÃ ng thÆ°Æ¡ng máº¡i"
        ],
        "MÃ”I TRÆ¯á»œNG": [
            "mÃ´i trÆ°á»ng", "báº£o vá»‡ mÃ´i trÆ°á»ng", "Ã´ nhiá»…m", "xá»­ lÃ½ cháº¥t tháº£i",
            "khÃ­ tháº£i", "nÆ°á»›c tháº£i", "rÃ¡c tháº£i", "tiáº¿ng á»“n", "bá»¥i", "hÃ³a cháº¥t",
            "Ä‘Ã¡nh giÃ¡ tÃ¡c Ä‘á»™ng mÃ´i trÆ°á»ng", "giáº¥y phÃ©p mÃ´i trÆ°á»ng"
        ],
        "AN NINH": [
            "an ninh", "quá»‘c phÃ²ng", "báº£o vá»‡ an ninh", "tráº­t tá»± an toÃ n xÃ£ há»™i",
            "cÃ´ng an", "bá»™ Ä‘á»™i", "quÃ¢n Ä‘á»™i", "cáº£nh sÃ¡t", "an ninh quá»‘c gia",
            "an ninh tráº­t tá»±", "phÃ²ng chá»‘ng tá»™i pháº¡m"
        ]
    }
    
    # Äáº¿m sá»‘ tá»« khÃ³a xuáº¥t hiá»‡n cho má»—i domain
    domain_scores = {}
    for domain, keywords in domain_keywords.items():
        score = 0
        for keyword in keywords:
            # TÃ¬m kiáº¿m tá»« khÃ³a trong vÄƒn báº£n (khÃ´ng phÃ¢n biá»‡t dáº¥u)
            if keyword.upper() in full_text:
                score += 1
            # TÃ¬m kiáº¿m vá»›i cÃ¡c biáº¿n thá»ƒ dáº¥u
            elif keyword.replace(' ', '').upper() in full_text.replace(' ', ''):
                score += 1
        
        if score > 0:
            domain_scores[domain] = score
    
    # Tráº£ vá» domain cÃ³ Ä‘iá»ƒm cao nháº¥t
    if domain_scores:
        best_domain = max(domain_scores, key=domain_scores.get)
        print(f"ğŸ” Domain Ä‘Æ°á»£c chá»n: {best_domain} (Ä‘iá»ƒm: {domain_scores[best_domain]})")
        return best_domain
    
    return "KHÃC"

def create_hierarchical_dataset(json_file_path, output_csv_path, target_size=10000):
    """Táº¡o dataset phÃ¢n cáº¥p 2 táº§ng tá»« file JSON"""
    
    print(f"ğŸš€ Báº¯t Ä‘áº§u táº¡o dataset tá»« file: {json_file_path}")
    
    # Kiá»ƒm tra file tá»“n táº¡i
    if not Path(json_file_path).exists():
        raise FileNotFoundError(f"âŒ KhÃ´ng tÃ¬m tháº¥y file: {json_file_path}")
    
    # Load dá»¯ liá»‡u JSON
    with open(json_file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"âœ… Load thÃ nh cÃ´ng {len(data)} items tá»« JSON")
    
    # Chuáº©n bá»‹ dá»¯ liá»‡u cho dataset
    dataset_items = []
    
    # Láº¥y máº«u ngáº«u nhiÃªn Ä‘á»ƒ Ä‘áº£m báº£o Ä‘a dáº¡ng
    if len(data) > target_size:
        sampled_data = random.sample(data, target_size)
    else:
        sampled_data = data
    
    print(f"ğŸ“Š Xá»­ lÃ½ {len(sampled_data)} items...")
    
    for item in sampled_data:
        try:
            # TrÃ­ch xuáº¥t thÃ´ng tin cÆ¡ báº£n
            doc_id = item.get('id', '')
            doc_type = extract_document_type(item.get('type', ''))
            doc_name = clean_text(item.get('name', ''))
            ministry = clean_text(item.get('ministry', ''))
            chapter_name = clean_text(item.get('chapter_name', ''))
            article = clean_text(item.get('article', ''))
            content = clean_text(item.get('content', ''))
            
            # Táº¡o vÄƒn báº£n Ä‘áº§y Ä‘á»§ Ä‘á»ƒ phÃ¢n loáº¡i
            full_text = f"{doc_name} {chapter_name} {article} {content}"
            
            # TrÃ­ch xuáº¥t domain phÃ¡p lÃ½
            legal_domain = extract_legal_domain(content, doc_name, chapter_name)
            
            # Táº¡o item cho dataset
            dataset_item = {
                'id': doc_id,
                'text': full_text,
                'type_level1': doc_type,  # Táº§ng 1: Loáº¡i vÄƒn báº£n cÆ¡ báº£n
                'domain_level2': legal_domain,  # Táº§ng 2: Domain phÃ¡p lÃ½
                'ministry': ministry,
                'name': doc_name,
                'chapter': chapter_name,
                'article': article,
                'content_length': len(content)
            }
            
            dataset_items.append(dataset_item)
            
        except Exception as e:
            print(f"âš ï¸ Lá»—i khi xá»­ lÃ½ item: {e}")
            continue
    
    # Táº¡o DataFrame
    df = pd.DataFrame(dataset_items)
    
    # Thá»‘ng kÃª dataset
    print(f"\nğŸ“ˆ THá»NG KÃŠ DATASET:")
    print(f"Tá»•ng sá»‘ samples: {len(df)}")
    
    print(f"\nğŸ·ï¸ PHÃ‚N LOáº I Táº¦NG 1 (Loáº¡i vÄƒn báº£n):")
    type_counts = df['type_level1'].value_counts()
    for doc_type, count in type_counts.items():
        print(f"  - {doc_type}: {count}")
    
    print(f"\nğŸ·ï¸ PHÃ‚N LOáº I Táº¦NG 2 (Domain phÃ¡p lÃ½):")
    domain_counts = df['domain_level2'].value_counts()
    for domain, count in domain_counts.items():
        print(f"  - {domain}: {count}")
    
    # Táº¡o thÆ° má»¥c output náº¿u chÆ°a cÃ³
    output_dir = Path(output_csv_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # LÆ°u dataset
    df.to_csv(output_csv_path, index=False, encoding='utf-8')
    print(f"\nâœ… ÄÃ£ lÆ°u dataset vÃ o: {output_csv_path}")
    
    return df

def create_training_splits(dataset_path, output_dir):
    """Táº¡o cÃ¡c táº­p train/validation/test tá»« dataset"""
    
    print(f"\nğŸ”„ Táº¡o cÃ¡c táº­p train/validation/test...")
    
    # Load dataset
    df = pd.read_csv(dataset_path, encoding='utf-8')
    
    # Táº¡o thÆ° má»¥c output
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # Chia dá»¯ liá»‡u theo tá»· lá»‡ 70/15/15
    total_size = len(df)
    train_size = int(0.7 * total_size)
    val_size = int(0.15 * total_size)
    
    # Shuffle dá»¯ liá»‡u
    df_shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)
    
    # Chia dá»¯ liá»‡u
    train_df = df_shuffled[:train_size]
    val_df = df_shuffled[train_size:train_size + val_size]
    test_df = df_shuffled[train_size + val_size:]
    
    # LÆ°u cÃ¡c táº­p
    train_path = output_path / "train.csv"
    val_path = output_path / "validation.csv"
    test_path = output_path / "test.csv"
    
    train_df.to_csv(train_path, index=False, encoding='utf-8')
    val_df.to_csv(val_path, index=False, encoding='utf-8')
    test_df.to_csv(test_path, index=False, encoding='utf-8')
    
    print(f"âœ… Train set: {len(train_df)} samples -> {train_path}")
    print(f"âœ… Validation set: {len(val_df)} samples -> {val_path}")
    print(f"âœ… Test set: {len(test_df)} samples -> {test_path}")
    
    return train_df, val_df, test_df

if __name__ == "__main__":
    # ÄÆ°á»ng dáº«n file - ÄÃƒ Sá»¬A
    json_file = "data/raw/vbpl_crawl.json"  # âœ… ÄÆ°á»ng dáº«n Ä‘Ãºng
    output_csv = "data/processed/hierarchical_legal_dataset.csv"  # âœ… LÆ°u vÃ o processed
    splits_dir = "data/processed/dataset_splits"  # âœ… LÆ°u vÃ o processed
    
    print("ğŸ” Kiá»ƒm tra cáº¥u trÃºc thÆ° má»¥c...")
    
    # Kiá»ƒm tra file JSON
    if not Path(json_file).exists():
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file: {json_file}")
        print("ğŸ’¡ HÃ£y Ä‘áº£m báº£o file vbpl_crawl.json Ä‘Ã£ Ä‘Æ°á»£c di chuyá»ƒn vÃ o data/raw/")
        exit(1)
    
    print(f"âœ… TÃ¬m tháº¥y file JSON: {json_file}")
    
    try:
        # Táº¡o dataset chÃ­nh
        df = create_hierarchical_dataset(json_file, output_csv, target_size=10000)
        
        # Táº¡o cÃ¡c táº­p train/validation/test
        create_training_splits(output_csv, splits_dir)
        
        print(f"\nğŸ‰ HOÃ€N THÃ€NH! Dataset Ä‘Ã£ Ä‘Æ°á»£c táº¡o thÃ nh cÃ´ng:")
        print(f"  - Dataset chÃ­nh: {output_csv}")
        print(f"  - CÃ¡c táº­p chia: {splits_dir}/")
        print(f"  - Tá»•ng sá»‘ samples: {len(df)}")
        
        # ThÃ´ng tin vá» cáº¥u trÃºc thÆ° má»¥c
        print(f"\nğŸ“ Cáº¥u trÃºc thÆ° má»¥c Ä‘Ã£ táº¡o:")
        print(f"  - data/processed/hierarchical_legal_dataset.csv")
        print(f"  - data/processed/dataset_splits/train.csv")
        print(f"  - data/processed/dataset_splits/validation.csv")
        print(f"  - data/processed/dataset_splits/test.csv")
        
    except Exception as e:
        print(f"âŒ Lá»—i trong quÃ¡ trÃ¬nh táº¡o dataset: {e}")
        exit(1) 