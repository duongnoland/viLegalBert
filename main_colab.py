#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ viLegalBert - Main Pipeline cho Google Colab (Dataset CÃ³ Sáºµn)
PhÃ¢n loáº¡i vÄƒn báº£n phÃ¡p luáº­t Viá»‡t Nam vá»›i kiáº¿n trÃºc phÃ¢n cáº¥p 2 táº§ng
"""

import os
import sys
import yaml
import pickle
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime
import logging
from typing import Dict, List, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# ğŸ“¦ INSTALL & IMPORT DEPENDENCIES
# ============================================================================

def install_dependencies():
    """CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t"""
    try:
        import sklearn
        print("âœ… scikit-learn Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t")
    except ImportError:
        print("ğŸ“¦ Äang cÃ i Ä‘áº·t scikit-learn...")
        os.system("pip install scikit-learn")
    
    try:
        import transformers
        print("âœ… transformers Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t")
    except ImportError:
        print("ğŸ“¦ Äang cÃ i Ä‘áº·t transformers...")
        os.system("pip install transformers")
    
    try:
        import torch
        print("âœ… PyTorch Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t")
    except ImportError:
        print("ğŸ“¦ Äang cÃ i Ä‘áº·t PyTorch...")
        os.system("pip install torch")
    
    try:
        import yaml
        print("âœ… PyYAML Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t")
    except ImportError:
        print("ğŸ“¦ Äang cÃ i Ä‘áº·t PyYAML...")
        os.system("pip install PyYAML")

# CÃ i Ä‘áº·t dependencies
install_dependencies()

# Import sau khi cÃ i Ä‘áº·t
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split
from sklearn.metrics import (
    classification_report, confusion_matrix, accuracy_score,
    precision_recall_fscore_support, roc_auc_score
)
from sklearn.calibration import CalibratedClassifierCV
from sklearn.pipeline import Pipeline
from sklearn.ensemble import VotingClassifier
import joblib

# ============================================================================
# ğŸ—ï¸ Cáº¤U TRÃšC PROJECT
# ============================================================================

def create_project_structure():
    """Táº¡o cáº¥u trÃºc thÆ° má»¥c cho project"""
    directories = [
        'models/saved_models/level1_classifier/svm_level1',
        'models/saved_models/level2_classifier/svm_level2',
        'models/saved_models/level1_classifier/phobert_level1',
        'models/saved_models/level2_classifier/phobert_level2',
        'models/saved_models/level1_classifier/bilstm_level1',
        'models/saved_models/level2_classifier/bilstm_level2',
        'models/saved_models/hierarchical_models',
        'results/training_results',
        'results/evaluation_results',
        'logs'
    ]
    
    for directory in directories:
        Path(directory).mkdir(parents=True, exist_ok=True)
        print(f"âœ… Táº¡o thÆ° má»¥c: {directory}")

# ============================================================================
# ğŸ“Š DATASET LOADING (CÃ³ Sáºµn)
# ============================================================================

def load_existing_dataset(dataset_path: str = "data/processed/hierarchical_legal_dataset.csv"):
    """Load dataset cÃ³ sáºµn"""
    print("ğŸ“Š Loading dataset cÃ³ sáºµn...")
    
    try:
        # Kiá»ƒm tra file dataset
        if not Path(dataset_path).exists():
            print(f"âŒ KhÃ´ng tÃ¬m tháº¥y dataset: {dataset_path}")
            print("ğŸ” TÃ¬m kiáº¿m dataset trong cÃ¡c thÆ° má»¥c...")
            
            # TÃ¬m kiáº¿m dataset trong cÃ¡c thÆ° má»¥c khÃ¡c
            possible_paths = [
                "hierarchical_legal_dataset.csv",
                "data/hierarchical_legal_dataset.csv",
                "dataset.csv",
                "legal_dataset.csv"
            ]
            
            for path in possible_paths:
                if Path(path).exists():
                    dataset_path = path
                    print(f"âœ… TÃ¬m tháº¥y dataset: {dataset_path}")
                    break
            else:
                print("âŒ KhÃ´ng tÃ¬m tháº¥y dataset nÃ o. Vui lÃ²ng upload dataset vÃ o Colab")
                return None
        
        # Load dataset
        df = pd.read_csv(dataset_path, encoding='utf-8')
        print(f"âœ… ÄÃ£ load dataset: {len(df)} samples")
        
        # Hiá»ƒn thá»‹ thÃ´ng tin dataset
        print(f"\nğŸ“ˆ THÃ”NG TIN DATASET:")
        print(f"Shape: {df.shape}")
        print(f"Columns: {list(df.columns)}")
        
        # Kiá»ƒm tra columns cáº§n thiáº¿t
        required_columns = ['text', 'type_level1', 'domain_level2']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            print(f"âŒ Thiáº¿u columns: {missing_columns}")
            print(f"ğŸ“‹ Columns cÃ³ sáºµn: {list(df.columns)}")
            return None
        
        # Hiá»ƒn thá»‹ thá»‘ng kÃª
        print(f"\nğŸ·ï¸ PHÃ‚N LOáº I Táº¦NG 1 (Loáº¡i vÄƒn báº£n):")
        level1_counts = df['type_level1'].value_counts()
        for doc_type, count in level1_counts.items():
            print(f"  - {doc_type}: {count}")
        
        print(f"\nğŸ·ï¸ PHÃ‚N LOáº I Táº¦NG 2 (Domain phÃ¡p lÃ½):")
        level2_counts = df['domain_level2'].value_counts()
        for domain, count in level2_counts.items():
            print(f"  - {domain}: {count}")
        
        return df
        
    except Exception as e:
        print(f"âŒ Lá»—i khi load dataset: {e}")
        return None

def check_dataset_splits():
    """Kiá»ƒm tra dataset splits cÃ³ sáºµn"""
    print("ğŸ” Kiá»ƒm tra dataset splits...")
    
    splits_dir = "data/processed/dataset_splits"
    train_path = Path(splits_dir) / "train.csv"
    val_path = Path(splits_dir) / "validation.csv"
    test_path = Path(splits_dir) / "test.csv"
    
    if train_path.exists() and val_path.exists() and test_path.exists():
        print("âœ… Dataset splits Ä‘Ã£ cÃ³ sáºµn")
        
        # Load vÃ  hiá»ƒn thá»‹ thÃ´ng tin splits
        train_df = pd.read_csv(train_path, encoding='utf-8')
        val_df = pd.read_csv(val_path, encoding='utf-8')
        test_df = pd.read_csv(test_path, encoding='utf-8')
        
        print(f"ğŸ“Š Train set: {len(train_df)} samples")
        print(f"ğŸ“Š Validation set: {len(val_df)} samples")
        print(f"ğŸ“Š Test set: {len(test_df)} samples")
        
        return True
    else:
        print("âš ï¸ Dataset splits chÆ°a cÃ³, sáº½ táº¡o má»›i...")
        return False

def create_training_splits_from_existing(dataset_path: str, splits_dir: str):
    """Táº¡o training splits tá»« dataset cÃ³ sáºµn"""
    print("ğŸ”„ Táº¡o training splits tá»« dataset cÃ³ sáºµn...")
    
    # Load dataset
    df = pd.read_csv(dataset_path, encoding='utf-8')
    
    # Chia dá»¯ liá»‡u
    train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['type_level1'])
    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['type_level1'])
    
    # LÆ°u cÃ¡c táº­p
    train_path = Path(splits_dir) / "train.csv"
    val_path = Path(splits_dir) / "validation.csv"
    test_path = Path(splits_dir) / "test.csv"
    
    train_df.to_csv(train_path, index=False, encoding='utf-8')
    val_df.to_csv(val_path, index=False, encoding='utf-8')
    test_df.to_csv(test_path, index=False, encoding='utf-8')
    
    print(f"âœ… Train set: {len(train_df)} samples -> {train_path}")
    print(f"âœ… Validation set: {len(val_df)} samples -> {val_path}")
    print(f"âœ… Test set: {len(test_df)} samples -> {test_path}")

# ============================================================================
# ğŸ‹ï¸ SVM TRAINER
# ============================================================================

class SVMTrainer:
    """Trainer cho mÃ´ hÃ¬nh SVM"""
    
    def __init__(self):
        """Khá»Ÿi táº¡o trainer"""
        self.config = {
            'feature_extraction': {
                'tfidf': {
                    'max_features': 10000,
                    'min_df': 2,
                    'max_df': 0.95,
                    'ngram_range': [1, 2],
                    'stop_words': None
                }
            },
            'svm': {
                'kernel': 'rbf',
                'C': 1.0,
                'gamma': 'scale'
            },
            'feature_selection': {
                'k_best': 5000
            }
        }
        self.models = {}
        self.vectorizers = {}
        self.feature_selectors = {}
    
    def train_level1(self, data_path: str):
        """Training cho Level 1 (Loáº¡i vÄƒn báº£n)"""
        print("ğŸ·ï¸ Training Level 1 (Loáº¡i vÄƒn báº£n)...")
        
        # Load data
        df = pd.read_csv(data_path, encoding='utf-8')
        X = df['text'].fillna('')
        y = df['type_level1']
        
        # Chia train/validation
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # TF-IDF Vectorization
        print("ğŸ“Š TF-IDF Vectorization...")
        vectorizer = TfidfVectorizer(
            max_features=self.config['feature_extraction']['tfidf']['max_features'],
            min_df=self.config['feature_extraction']['tfidf']['min_df'],
            max_df=self.config['feature_extraction']['tfidf']['max_df'],
            ngram_range=tuple(self.config['feature_extraction']['tfidf']['ngram_range'])
        )
        
        X_train_tfidf = vectorizer.fit_transform(X_train)
        X_val_tfidf = vectorizer.transform(X_val)
        
        # Feature Selection
        print("ğŸ” Feature Selection...")
        feature_selector = SelectKBest(chi2, k=self.config['feature_selection']['k_best'])
        X_train_selected = feature_selector.fit_transform(X_train_tfidf, y_train)
        X_val_selected = feature_selector.transform(X_val_tfidf)
        
        # SVM Training
        print("ğŸ‹ï¸ Training SVM...")
        svm = SVC(
            kernel=self.config['svm']['kernel'],
            C=self.config['svm']['C'],
            gamma=self.config['svm']['gamma'],
            random_state=42,
            probability=True
        )
        
        svm.fit(X_train_selected, y_train)
        
        # Evaluation
        y_pred = svm.predict(X_val_selected)
        accuracy = accuracy_score(y_val, y_pred)
        
        print(f"âœ… Level 1 Training hoÃ n thÃ nh!")
        print(f"ğŸ“Š Accuracy: {accuracy:.4f}")
        print(f"ğŸ“Š Classification Report:")
        print(classification_report(y_val, y_pred))
        
        # LÆ°u model
        self.models['level1'] = svm
        self.vectorizers['level1'] = vectorizer
        self.feature_selectors['level1'] = feature_selector
        
        # LÆ°u model
        model_path = "models/saved_models/level1_classifier/svm_level1/svm_level1_model.pkl"
        Path(model_path).parent.mkdir(parents=True, exist_ok=True)
        
        model_data = {
            'model': svm,
            'vectorizer': vectorizer,
            'feature_selector': feature_selector
        }
        
        with open(model_path, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"ğŸ’¾ Model Ä‘Ã£ Ä‘Æ°á»£c lÆ°u: {model_path}")
        
        return {
            'accuracy': accuracy,
            'model_path': model_path,
            'classification_report': classification_report(y_val, y_pred, output_dict=True)
        }
    
    def train_level2(self, data_path: str):
        """Training cho Level 2 (Domain phÃ¡p lÃ½)"""
        print("ğŸ·ï¸ Training Level 2 (Domain phÃ¡p lÃ½)...")
        
        # Load data
        df = pd.read_csv(data_path, encoding='utf-8')
        X = df['text'].fillna('')
        y = df['domain_level2']
        
        # Chia train/validation
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # TF-IDF Vectorization
        print("ğŸ“Š TF-IDF Vectorization...")
        vectorizer = TfidfVectorizer(
            max_features=self.config['feature_extraction']['tfidf']['max_features'],
            min_df=self.config['feature_extraction']['tfidf']['min_df'],
            max_df=self.config['feature_extraction']['tfidf']['max_df'],
            ngram_range=tuple(self.config['feature_extraction']['tfidf']['ngram_range'])
        )
        
        X_train_tfidf = vectorizer.fit_transform(X_train)
        X_val_tfidf = vectorizer.transform(X_val)
        
        # Feature Selection
        print("ğŸ” Feature Selection...")
        feature_selector = SelectKBest(chi2, k=self.config['feature_selection']['k_best'])
        X_train_selected = feature_selector.fit_transform(X_train_tfidf, y_train)
        X_val_selected = feature_selector.transform(X_val_tfidf)
        
        # SVM Training
        print("ğŸ‹ï¸ Training SVM...")
        svm = SVC(
            kernel=self.config['svm']['kernel'],
            C=self.config['svm']['C'],
            gamma=self.config['svm']['gamma'],
            random_state=42,
            probability=True
        )
        
        svm.fit(X_train_selected, y_train)
        
        # Evaluation
        y_pred = svm.predict(X_val_selected)
        accuracy = accuracy_score(y_val, y_pred)
        
        print(f"âœ… Level 2 Training hoÃ n thÃ nh!")
        print(f"ğŸ“Š Accuracy: {accuracy:.4f}")
        print(f"ğŸ“Š Classification Report:")
        print(classification_report(y_val, y_pred))
        
        # LÆ°u model
        self.models['level2'] = svm
        self.vectorizers['level2'] = vectorizer
        self.feature_selectors['level2'] = feature_selector
        
        # LÆ°u model
        model_path = "models/saved_models/level2_classifier/svm_level2/svm_level2_model.pkl"
        Path(model_path).parent.mkdir(parents=True, exist_ok=True)
        
        model_data = {
            'model': svm,
            'vectorizer': vectorizer,
            'feature_selector': feature_selector
        }
        
        with open(model_path, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"ğŸ’¾ Model Ä‘Ã£ Ä‘Æ°á»£c lÆ°u: {model_path}")
        
        return {
            'accuracy': accuracy,
            'model_path': model_path,
            'classification_report': classification_report(y_val, y_pred, output_dict=True)
        }

# ============================================================================
# ğŸ“Š EVALUATION
# ============================================================================

def evaluate_svm_models(test_data_path: str):
    """ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh SVM trÃªn test set"""
    print("ğŸ“Š ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh SVM...")
    
    # Load test data
    test_df = pd.read_csv(test_data_path, encoding='utf-8')
    X_test = test_df['text'].fillna('')
    y_test_level1 = test_df['type_level1']
    y_test_level2 = test_df['domain_level2']
    
    # Load models
    try:
        # Level 1
        with open("models/saved_models/level1_classifier/svm_level1/svm_level1_model.pkl", 'rb') as f:
            level1_data = pickle.load(f)
        
        level1_model = level1_data['model']
        level1_vectorizer = level1_data['vectorizer']
        level1_feature_selector = level1_data['feature_selector']
        
        # Level 2
        with open("models/saved_models/level2_classifier/svm_level2/svm_level2_model.pkl", 'rb') as f:
            level2_data = pickle.load(f)
        
        level2_model = level2_data['model']
        level2_vectorizer = level2_data['vectorizer']
        level2_feature_selector = level2_data['feature_selector']
        
        # Evaluation Level 1
        print("\nğŸ·ï¸ EVALUATION LEVEL 1 (Loáº¡i vÄƒn báº£n):")
        X_test_level1 = level1_vectorizer.transform(X_test)
        X_test_level1_selected = level1_feature_selector.transform(X_test_level1)
        y_pred_level1 = level1_model.predict(X_test_level1_selected)
        
        accuracy_level1 = accuracy_score(y_test_level1, y_pred_level1)
        print(f"ğŸ“Š Accuracy: {accuracy_level1:.4f}")
        print(f"ğŸ“Š Classification Report:")
        print(classification_report(y_test_level1, y_pred_level1))
        
        # Evaluation Level 2
        print("\nğŸ·ï¸ EVALUATION LEVEL 2 (Domain phÃ¡p lÃ½):")
        X_test_level2 = level2_vectorizer.transform(X_test)
        X_test_level2_selected = level2_feature_selector.transform(X_test_level2)
        y_pred_level2 = level2_model.predict(X_test_level2_selected)
        
        accuracy_level2 = accuracy_score(y_test_level2, y_pred_level2)
        print(f"ğŸ“Š Accuracy: {accuracy_level2:.4f}")
        print(f"ğŸ“Š Classification Report:")
        print(classification_report(y_test_level2, y_pred_level2))
        
        # LÆ°u káº¿t quáº£
        results = {
            'level1': {
                'accuracy': accuracy_level1,
                'classification_report': classification_report(y_test_level1, y_pred_level1, output_dict=True)
            },
            'level2': {
                'accuracy': accuracy_level2,
                'classification_report': classification_report(y_test_level2, y_pred_level2, output_dict=True)
            }
        }
        
        results_path = "results/evaluation_results/svm_evaluation_results.pkl"
        Path(results_path).parent.mkdir(parents=True, exist_ok=True)
        
        with open(results_path, 'wb') as f:
            pickle.dump(results, f)
        
        print(f"\nğŸ’¾ Káº¿t quáº£ evaluation Ä‘Ã£ Ä‘Æ°á»£c lÆ°u: {results_path}")
        
        return results
        
    except Exception as e:
        print(f"âŒ Lá»—i khi Ä‘Ã¡nh giÃ¡: {e}")
        return None

# ============================================================================
# ğŸš€ MAIN PIPELINE
# ============================================================================

def main():
    """HÃ m chÃ­nh cháº¡y pipeline"""
    print("ğŸš€ KHá»I Äá»˜NG VILEGALBERT PIPELINE CHO GOOGLE COLAB!")
    print("ğŸ“Š Sá»¬ Dá»¤NG DATASET CÃ“ Sáº´N")
    print("=" * 80)
    
    # BÆ°á»›c 1: Táº¡o cáº¥u trÃºc project
    create_project_structure()
    
    # BÆ°á»›c 2: Load dataset cÃ³ sáºµn
    print("\nğŸ“Š BÆ¯á»šC 1: LOAD DATASET CÃ“ Sáº´N")
    print("-" * 50)
    
    df = load_existing_dataset()
    if df is None:
        print("âŒ KhÃ´ng thá»ƒ load dataset")
        return
    
    # BÆ°á»›c 3: Kiá»ƒm tra vÃ  táº¡o dataset splits
    print("\nğŸ”„ BÆ¯á»šC 2: KIá»‚M TRA DATASET SPLITS")
    print("-" * 50)
    
    if not check_dataset_splits():
        # Táº¡o splits má»›i tá»« dataset cÃ³ sáºµn
        dataset_path = "data/processed/hierarchical_legal_dataset.csv"
        splits_dir = "data/processed/dataset_splits"
        create_training_splits_from_existing(dataset_path, splits_dir)
    
    # BÆ°á»›c 4: Training SVM
    print("\nğŸ‹ï¸ BÆ¯á»šC 3: TRAINING SVM")
    print("-" * 50)
    
    trainer = SVMTrainer()
    
    # Training Level 1
    results_level1 = trainer.train_level1("data/processed/hierarchical_legal_dataset.csv")
    
    # Training Level 2
    results_level2 = trainer.train_level2("data/processed/hierarchical_legal_dataset.csv")
    
    # BÆ°á»›c 5: Evaluation
    print("\nğŸ“Š BÆ¯á»šC 4: EVALUATION")
    print("-" * 50)
    
    test_data_path = "data/processed/dataset_splits/test.csv"
    evaluation_results = evaluate_svm_models(test_data_path)
    
    # TÃ³m táº¯t káº¿t quáº£
    print("\nğŸ‰ TÃ“M Táº®T Káº¾T QUáº¢")
    print("=" * 80)
    print(f"ğŸ“Š Dataset: {len(df)} samples")
    print(f"ğŸ·ï¸ Level 1 Accuracy: {results_level1['accuracy']:.4f}")
    print(f"ğŸ·ï¸ Level 2 Accuracy: {results_level2['accuracy']:.4f}")
    print(f"ğŸ’¾ Models Ä‘Ã£ Ä‘Æ°á»£c lÆ°u trong thÆ° má»¥c models/")
    print(f"ğŸ“Š Káº¿t quáº£ evaluation Ä‘Ã£ Ä‘Æ°á»£c lÆ°u trong thÆ° má»¥c results/")
    
    print("\nâœ… PIPELINE HOÃ€N THÃ€NH THÃ€NH CÃ”NG!")
    print("ğŸš€ Báº¡n cÃ³ thá»ƒ tiáº¿p tá»¥c vá»›i training PhoBERT, BiLSTM hoáº·c Ensemble!")

if __name__ == "__main__":
    main() 