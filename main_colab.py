#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ viLegalBert - Main Pipeline cho Google Colab (GPU Optimized)
PhÃ¢n loáº¡i vÄƒn báº£n phÃ¡p luáº­t Viá»‡t Nam vá»›i kiáº¿n trÃºc phÃ¢n cáº¥p 2 táº§ng
"""

import os
import pickle
import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# ğŸš€ GPU SETUP & DEPENDENCIES
# ============================================================================

def setup_gpu():
    """Setup GPU environment cho Linux"""
    import torch
    
    if torch.cuda.is_available():
        print("ğŸš€ GPU CUDA available!")
        print(f"ğŸ“Š GPU Device: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ“Š GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        
        # Set default device
        torch.cuda.set_device(0)
        return True
    else:
        print("âš ï¸ GPU CUDA khÃ´ng available, sá»­ dá»¥ng CPU")
        return False

def install_deps():
    """CÃ i Ä‘áº·t dependencies cho Linux"""
    import subprocess
    import sys
    
    packages = [
        "scikit-learn",
        "pandas",
        "numpy",
        "joblib"
    ]
    
    for package in packages:
        try:
            __import__(package.replace("-", "_"))
            print(f"âœ… {package} Ä‘Ã£ cÃ³ sáºµn")
        except ImportError:
            print(f"ğŸ“¦ CÃ i Ä‘áº·t {package}...")
            subprocess.check_call([sys.executable, "-m", "pip", "install", package])
            print(f"âœ… {package} Ä‘Ã£ cÃ i Ä‘áº·t xong")

# Import sau khi cÃ i Ä‘áº·t
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import accuracy_score, classification_report
import joblib

# ============================================================================
# ğŸ—ï¸ PROJECT STRUCTURE
# ============================================================================

def create_dirs():
    """Táº¡o thÆ° má»¥c cho Linux tá»« /content/viLegalBert"""
    import os
    
    # Base directory cho Google Colab
    base_dir = "/content/viLegalBert"
    
    dirs = [
        f"{base_dir}/models/saved_models/level1_classifier/svm_level1",
        f"{base_dir}/models/saved_models/level2_classifier/svm_level2",
        f"{base_dir}/data/processed/dataset_splits"
    ]
    
    for dir_path in dirs:
        os.makedirs(dir_path, exist_ok=True)
        print(f"âœ… ÄÃ£ táº¡o thÆ° má»¥c: {dir_path}")

# ============================================================================
# ğŸ“Š DATASET LOADING
# ============================================================================

def check_splits():
    """Kiá»ƒm tra dataset splits cÃ³ sáºµn cho Linux tá»« /content/viLegalBert"""
    import os
    
    # Base directory cho Google Colab
    base_dir = "/content/viLegalBert"
    
    splits_dir = f"{base_dir}/data/processed/dataset_splits"
    train_path = os.path.join(splits_dir, "train.csv")
    val_path = os.path.join(splits_dir, "validation.csv")
    test_path = os.path.join(splits_dir, "test.csv")
    
    if os.path.exists(train_path) and os.path.exists(val_path) and os.path.exists(test_path):
        # Load vÃ  hiá»ƒn thá»‹ thÃ´ng tin splits
        import pandas as pd
        train_df = pd.read_csv(train_path, encoding='utf-8')
        val_df = pd.read_csv(val_path, encoding='utf-8')
        test_df = pd.read_csv(test_path, encoding='utf-8')
        
        print(f"âœ… Dataset splits Ä‘Ã£ cÃ³ sáºµn:")
        print(f"ğŸ“Š Train set: {len(train_df)} samples")
        print(f"ğŸ“Š Validation set: {len(val_df)} samples")
        print(f"ğŸ“Š Test set: {len(test_df)} samples")
        return True
    else:
        print("âš ï¸ Dataset splits chÆ°a cÃ³, sáº½ táº¡o má»›i...")
        return False

# ============================================================================
# ğŸ‹ï¸ SVM TRAINER
# ============================================================================

class SVMTrainer:
    """Trainer cho mÃ´ hÃ¬nh SVM vá»›i GPU optimization"""
    
    def __init__(self):
        self.use_gpu = setup_gpu()
        
        # Cáº¥u hÃ¬nh tá»‘i Æ°u cho GPU/CPU
        if self.use_gpu:
            self.config = {
                'max_features': 15000,
                'k_best': 8000,
                'cv': 5,
                'verbose': 2
            }
        else:
            self.config = {
                'max_features': 10000,
                'k_best': 5000,
                'cv': 3,
                'verbose': 1
            }
        
        self.models = {}
        self.vectorizers = {}
        self.feature_selectors = {}
        
        print(f"ğŸš€ SVMTrainer - GPU: {'âœ…' if self.use_gpu else 'âŒ'}")
    
    def train_level1(self, data_path, val_path):
        """Training cho Level 1"""
        print("ğŸ·ï¸ Training Level 1...")
        
        # Load data
        df_train = pd.read_csv(data_path, encoding='utf-8')
        df_val = pd.read_csv(val_path, encoding='utf-8')
        
        X_train = df_train['text'].fillna('')
        y_train = df_train['type_level1']
        
        X_val = df_val['text'].fillna('')
        y_val = df_val['type_level1']
        
        # TF-IDF + Feature Selection
        vectorizer = TfidfVectorizer(max_features=self.config['max_features'], ngram_range=(1, 2))
        X_train_tfidf = vectorizer.fit_transform(X_train)
        X_val_tfidf = vectorizer.transform(X_val)
        
        feature_selector = SelectKBest(chi2, k=self.config['k_best'])
        X_train_selected = feature_selector.fit_transform(X_train_tfidf, y_train)
        X_val_selected = feature_selector.transform(X_val_tfidf)
        
        # Training vá»›i hyperparameter tuning náº¿u cÃ³ GPU
        print("ğŸ‹ï¸ Báº¯t Ä‘áº§u training SVM...")
        if self.use_gpu:
            param_grid = {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto'], 'kernel': ['rbf', 'linear']}
            grid_search = GridSearchCV(SVC(random_state=42, probability=True), param_grid, 
                                     cv=self.config['cv'], n_jobs=-1, verbose=self.config['verbose'])
            
            print("ğŸ“Š Progress: Training vá»›i Grid Search...")
            print("â³ 0% - Khá»Ÿi táº¡o Grid Search...")
            grid_search.fit(X_train_selected, y_train)
            print("âœ… 100% - Grid Search hoÃ n thÃ nh!")
            
            svm = grid_search.best_estimator_
            print(f"âœ… Best params: {grid_search.best_params_}")
        else:
            print("ğŸ“Š Progress: Training SVM cÆ¡ báº£n...")
            print("â³ 0% - Khá»Ÿi táº¡o SVM...")
            svm = SVC(kernel='rbf', random_state=42, probability=True)
            print("â³ 50% - Äang training...")
            svm.fit(X_train_selected, y_train)
            print("âœ… 100% - SVM training hoÃ n thÃ nh!")
        
        # Evaluation
        print("ğŸ“Š Progress: ÄÃ¡nh giÃ¡ model...")
        print("â³ 80% - Prediction trÃªn validation set...")
        y_pred = svm.predict(X_val_selected)
        print("â³ 90% - TÃ­nh toÃ¡n accuracy...")
        accuracy = accuracy_score(y_val, y_pred)
        print("âœ… 100% - Evaluation hoÃ n thÃ nh!")
        
        print(f"âœ… Level 1 Accuracy: {accuracy:.4f}")
        print(classification_report(y_val, y_pred))
        
        # LÆ°u model
        self.models['level1'] = svm
        self.vectorizers['level1'] = vectorizer
        self.feature_selectors['level1'] = feature_selector
        
        # ÄÃšNG: Sá»­ dá»¥ng base_dir cho model path
        base_dir = "/content/viLegalBert"
        model_path = f"{base_dir}/models/saved_models/level1_classifier/svm_level1/svm_level1_model.pkl"
        model_data = {
            'model': svm, 'vectorizer': vectorizer, 'feature_selector': feature_selector,
            'config': self.config, 'gpu_optimized': self.use_gpu
        }
        
        with open(model_path, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"ğŸ’¾ Model Ä‘Ã£ lÆ°u: {model_path}")
        return {'accuracy': accuracy, 'model_path': model_path, 'gpu_optimized': self.use_gpu}
    
    def train_level2(self, data_path, val_path):
        """Training cho Level 2"""
        print("ğŸ·ï¸ Training Level 2...")
        
        # Load data
        df_train = pd.read_csv(data_path, encoding='utf-8')
        df_val = pd.read_csv(val_path, encoding='utf-8')
        
        X_train = df_train['text'].fillna('')
        y_train = df_train['domain_level2']
        
        X_val = df_val['text'].fillna('')
        y_val = df_val['domain_level2']
        
        # TF-IDF + Feature Selection
        vectorizer = TfidfVectorizer(max_features=self.config['max_features'], ngram_range=(1, 2))
        X_train_tfidf = vectorizer.fit_transform(X_train)
        X_val_tfidf = vectorizer.transform(X_val)
        
        feature_selector = SelectKBest(chi2, k=self.config['k_best'])
        X_train_selected = feature_selector.fit_transform(X_train_tfidf, y_train)
        X_val_selected = feature_selector.transform(X_val_tfidf)
        
        # Training
        print("ğŸ‹ï¸ Báº¯t Ä‘áº§u training SVM Level 2...")
        if self.use_gpu:
            param_grid = {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto'], 'kernel': ['rbf', 'linear']}
            grid_search = GridSearchCV(SVC(random_state=42, probability=True), param_grid, 
                                     cv=self.config['cv'], n_jobs=-1, verbose=self.config['verbose'])
            
            print("ğŸ“Š Progress: Training vá»›i Grid Search...")
            print("â³ 0% - Khá»Ÿi táº¡o Grid Search...")
            grid_search.fit(X_train_selected, y_train)
            print("âœ… 100% - Grid Search hoÃ n thÃ nh!")
            
            svm = grid_search.best_estimator_
        else:
            print("ğŸ“Š Progress: Training SVM cÆ¡ báº£n...")
            print("â³ 0% - Khá»Ÿi táº¡o SVM...")
            svm = SVC(kernel='rbf', random_state=42, probability=True)
            print("â³ 50% - Äang training...")
            svm.fit(X_train_selected, y_train)
            print("âœ… 100% - SVM training hoÃ n thÃ nh!")
        
        # Evaluation
        print("ğŸ“Š Progress: ÄÃ¡nh giÃ¡ model...")
        print("â³ 80% - Prediction trÃªn validation set...")
        y_pred = svm.predict(X_val_selected)
        print("â³ 90% - TÃ­nh toÃ¡n accuracy...")
        accuracy = accuracy_score(y_val, y_pred)
        print("âœ… 100% - Evaluation hoÃ n thÃ nh!")
        
        print(f"âœ… Level 2 Accuracy: {accuracy:.4f}")
        print(classification_report(y_val, y_pred))
        
        # LÆ°u model
        self.models['level2'] = svm
        self.vectorizers['level2'] = vectorizer
        self.feature_selectors['level2'] = feature_selector
        
        # ÄÃšNG: Sá»­ dá»¥ng base_dir cho model path
        base_dir = "/content/viLegalBert"
        model_path = f"{base_dir}/models/saved_models/level2_classifier/svm_level2/svm_level2_model.pkl"
        model_data = {
            'model': svm, 'vectorizer': vectorizer, 'feature_selector': feature_selector,
            'config': self.config, 'gpu_optimized': self.use_gpu
        }
        
        with open(model_path, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"ğŸ’¾ Model Ä‘Ã£ lÆ°u: {model_path}")
        return {'accuracy': accuracy, 'model_path': model_path, 'gpu_optimized': self.use_gpu}
# ============================================================================
# ğŸš€ MAIN PIPELINE
# ============================================================================

def main():
    """HÃ m chÃ­nh cháº¡y pipeline"""
    print("ğŸš€ VILEGALBERT PIPELINE - GPU OPTIMIZED")
    print("=" * 60)
    
    # Base directory cho Google Colab
    base_dir = "/content/viLegalBert"
    
    # BÆ°á»›c 1: GPU setup
    print("\nğŸš€ BÆ¯á»šC 1: GPU SETUP")
    gpu_available = setup_gpu()
    
    # BÆ°á»›c 2: CÃ i Ä‘áº·t dependencies
    print("\nğŸ“¦ BÆ¯á»šC 2: CÃ€I Äáº¶T DEPENDENCIES")
    install_deps()
    
    # BÆ°á»›c 3: Táº¡o thÆ° má»¥c
    print("\nğŸ—ï¸ BÆ¯á»šC 3: Táº O THÆ¯ Má»¤C")
    create_dirs()
    
    # BÆ°á»›c 4: Kiá»ƒm tra splits
    print("\nğŸ”„ BÆ¯á»šC 4: KIá»‚M TRA SPLITS")
    if not check_splits():
        print("âŒ Pipeline dá»«ng do khÃ´ng cÃ³ dataset splits")
        return
    
    # BÆ°á»›c 5: Training SVM
    print("\nğŸ‹ï¸ BÆ¯á»šC 5: TRAINING SVM")
    trainer = SVMTrainer()
    
    # ÄÃšNG: Training sá»­ dá»¥ng train.csv vÃ  validation.csv cÃ³ sáºµn
    train_path = f"{base_dir}/data/processed/dataset_splits/train.csv"
    val_path = f"{base_dir}/data/processed/dataset_splits/validation.csv"
    
    results_level1 = trainer.train_level1(train_path, val_path)  # Truyá»n cáº£ train vÃ  val
    results_level2 = trainer.train_level2(train_path, val_path)  # Truyá»n cáº£ train vÃ  val
    
    # TÃ³m táº¯t
    print("\nğŸ‰ PIPELINE HOÃ€N THÃ€NH!")
    print(f"ğŸ“Š Dataset: {train_path}") # Changed to train_path
    print(f"ğŸ·ï¸ Level 1 Accuracy: {results_level1['accuracy']:.4f}")
    print(f"ğŸ·ï¸ Level 2 Accuracy: {results_level2['accuracy']:.4f}")
    print(f"ğŸš€ GPU Status: {'âœ… Available' if gpu_available else 'âŒ Not Available'}")

if __name__ == "__main__":
    main() 