#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üöÄ viLegalBert - Main Pipeline cho Google Colab (GPU Optimized)
Ph√¢n lo·∫°i vƒÉn b·∫£n ph√°p lu·∫≠t Vi·ªát Nam v·ªõi ki·∫øn tr√∫c ph√¢n c·∫•p 2 t·∫ßng
"""

import os
import pickle
import numpy as np
import pandas as pd
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# üöÄ GPU CONFIGURATION
# ============================================================================

def setup_gpu():
    """Thi·∫øt l·∫≠p GPU cho Colab"""
    try:
        import torch
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            
            print(f"‚úÖ GPU: {gpu_name} ({gpu_memory:.1f} GB)")
            
            # Optimize PyTorch
            torch.backends.cudnn.benchmark = True
            os.environ['CUDA_LAUNCH_BLOCKING'] = '0'
            
            return True
        else:
            print("‚ö†Ô∏è GPU kh√¥ng kh·∫£ d·ª•ng, s·ª≠ d·ª•ng CPU")
            return False
            
    except ImportError:
        print("‚ö†Ô∏è PyTorch ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t")
        return False

# ============================================================================
# üì¶ INSTALL DEPENDENCIES
# ============================================================================

def install_deps():
    """C√†i ƒë·∫∑t dependencies c·∫ßn thi·∫øt"""
    try:
        import sklearn
        print("‚úÖ scikit-learn ƒë√£ s·∫µn s√†ng")
    except ImportError:
        os.system("pip install scikit-learn")
        print("üì¶ ƒê√£ c√†i ƒë·∫∑t scikit-learn")
    
    try:
        import torch
        if torch.cuda.is_available():
            print("‚úÖ PyTorch v·ªõi CUDA ƒë√£ s·∫µn s√†ng")
        else:
            os.system("pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
    except ImportError:
        os.system("pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")

# Import sau khi c√†i ƒë·∫∑t
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import accuracy_score, classification_report
import joblib

# ============================================================================
# üèóÔ∏è PROJECT STRUCTURE
# ============================================================================

def create_dirs():
    """T·∫°o th∆∞ m·ª•c c·∫ßn thi·∫øt"""
    dirs = [
        'models/saved_models/level1_classifier/svm_level1',
        'models/saved_models/level2_classifier/svm_level2',
        'results/evaluation_results',
        'logs'
    ]
    
    for d in dirs:
        Path(d).mkdir(parents=True, exist_ok=True)
        print(f"‚úÖ T·∫°o th∆∞ m·ª•c: {d}")

# ============================================================================
# üìä DATASET LOADING
# ============================================================================

def load_dataset():
    """Load dataset t·ª´ data/processed"""
    dataset_path = "data/processed/hierarchical_legal_dataset.csv"
    
    if not Path(dataset_path).exists():
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y dataset: {dataset_path}")
        return None
    
    df = pd.read_csv(dataset_path, encoding='utf-8')
    print(f"‚úÖ ƒê√£ load dataset: {len(df)} samples")
    
    # Ki·ªÉm tra columns c·∫ßn thi·∫øt
    required_cols = ['text', 'type_level1', 'domain_level2']
    if not all(col in df.columns for col in required_cols):
        print(f"‚ùå Thi·∫øu columns: {required_cols}")
        return None
    
    # Hi·ªÉn th·ªã th·ªëng k√™
    print(f"üè∑Ô∏è Level 1: {df['type_level1'].value_counts().to_dict()}")
    print(f"üè∑Ô∏è Level 2: {df['domain_level2'].value_counts().to_dict()}")
    
    return df

def create_splits(df):
    """T·∫°o training splits"""
    splits_dir = "data/processed/dataset_splits"
    Path(splits_dir).mkdir(parents=True, exist_ok=True)
    
    # Chia d·ªØ li·ªáu
    train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['type_level1'])
    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['type_level1'])
    
    # L∆∞u splits
    train_df.to_csv(f"{splits_dir}/train.csv", index=False, encoding='utf-8')
    val_df.to_csv(f"{splits_dir}/validation.csv", index=False, encoding='utf-8')
    test_df.to_csv(f"{splits_dir}/test.csv", index=False, encoding='utf-8')
    
    print(f"‚úÖ Splits: Train({len(train_df)}) Val({len(val_df)}) Test({len(test_df)})")

# ============================================================================
# üèãÔ∏è SVM TRAINER
# ============================================================================

class SVMTrainer:
    """Trainer cho m√¥ h√¨nh SVM v·ªõi GPU optimization"""
    
    def __init__(self):
        self.use_gpu = setup_gpu()
        
        # C·∫•u h√¨nh t·ªëi ∆∞u cho GPU/CPU
        if self.use_gpu:
            self.config = {
                'max_features': 15000,
                'k_best': 8000,
                'cv': 5,
                'verbose': 2
            }
        else:
            self.config = {
                'max_features': 10000,
                'k_best': 5000,
                'cv': 3,
                'verbose': 1
            }
        
        self.models = {}
        self.vectorizers = {}
        self.feature_selectors = {}
        
        print(f"üöÄ SVMTrainer - GPU: {'‚úÖ' if self.use_gpu else '‚ùå'}")
    
    def train_level1(self, data_path):
        """Training cho Level 1"""
        print("üè∑Ô∏è Training Level 1...")
        
        # Load data
        df = pd.read_csv(data_path, encoding='utf-8')
        X = df['text'].fillna('')
        y = df['type_level1']
        
        # Chia data
        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
        
        # TF-IDF + Feature Selection
        vectorizer = TfidfVectorizer(max_features=self.config['max_features'], ngram_range=(1, 2))
        X_train_tfidf = vectorizer.fit_transform(X_train)
        X_val_tfidf = vectorizer.transform(X_val)
        
        feature_selector = SelectKBest(chi2, k=self.config['k_best'])
        X_train_selected = feature_selector.fit_transform(X_train_tfidf, y_train)
        X_val_selected = feature_selector.transform(X_val_tfidf)
        
        # Training v·ªõi hyperparameter tuning n·∫øu c√≥ GPU
        if self.use_gpu:
            param_grid = {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto'], 'kernel': ['rbf', 'linear']}
            grid_search = GridSearchCV(SVC(random_state=42, probability=True), param_grid, 
                                     cv=self.config['cv'], n_jobs=-1, verbose=self.config['verbose'])
            grid_search.fit(X_train_selected, y_train)
            svm = grid_search.best_estimator_
            print(f"‚úÖ Best params: {grid_search.best_params_}")
        else:
            svm = SVC(kernel='rbf', random_state=42, probability=True)
            svm.fit(X_train_selected, y_train)
        
        # Evaluation
        y_pred = svm.predict(X_val_selected)
        accuracy = accuracy_score(y_val, y_pred)
        
        print(f"‚úÖ Level 1 Accuracy: {accuracy:.4f}")
        print(classification_report(y_val, y_pred))
        
        # L∆∞u model
        self.models['level1'] = svm
        self.vectorizers['level1'] = vectorizer
        self.feature_selectors['level1'] = feature_selector
        
        model_path = "models/saved_models/level1_classifier/svm_level1/svm_level1_model.pkl"
        model_data = {
            'model': svm, 'vectorizer': vectorizer, 'feature_selector': feature_selector,
            'config': self.config, 'gpu_optimized': self.use_gpu
        }
        
        with open(model_path, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"üíæ Model ƒë√£ l∆∞u: {model_path}")
        return {'accuracy': accuracy, 'model_path': model_path, 'gpu_optimized': self.use_gpu}
    
    def train_level2(self, data_path):
        """Training cho Level 2"""
        print("üè∑Ô∏è Training Level 2...")
        
        # Load data
        df = pd.read_csv(data_path, encoding='utf-8')
        X = df['text'].fillna('')
        y = df['domain_level2']
        
        # Chia data
        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
        
        # TF-IDF + Feature Selection
        vectorizer = TfidfVectorizer(max_features=self.config['max_features'], ngram_range=(1, 2))
        X_train_tfidf = vectorizer.fit_transform(X_train)
        X_val_tfidf = vectorizer.transform(X_val)
        
        feature_selector = SelectKBest(chi2, k=self.config['k_best'])
        X_train_selected = feature_selector.fit_transform(X_train_tfidf, y_train)
        X_val_selected = feature_selector.transform(X_val_tfidf)
        
        # Training
        if self.use_gpu:
            param_grid = {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto'], 'kernel': ['rbf', 'linear']}
            grid_search = GridSearchCV(SVC(random_state=42, probability=True), param_grid, 
                                     cv=self.config['cv'], n_jobs=-1, verbose=self.config['verbose'])
            grid_search.fit(X_train_selected, y_train)
            svm = grid_search.best_estimator_
        else:
            svm = SVC(kernel='rbf', random_state=42, probability=True)
            svm.fit(X_train_selected, y_train)
        
        # Evaluation
        y_pred = svm.predict(X_val_selected)
        accuracy = accuracy_score(y_val, y_pred)
        
        print(f"‚úÖ Level 2 Accuracy: {accuracy:.4f}")
        print(classification_report(y_val, y_pred))
        
        # L∆∞u model
        self.models['level2'] = svm
        self.vectorizers['level2'] = vectorizer
        self.feature_selectors['level2'] = feature_selector
        
        model_path = "models/saved_models/level2_classifier/svm_level2/svm_level2_model.pkl"
        model_data = {
            'model': svm, 'vectorizer': vectorizer, 'feature_selector': feature_selector,
            'config': self.config, 'gpu_optimized': self.use_gpu
        }
        
        with open(model_path, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"üíæ Model ƒë√£ l∆∞u: {model_path}")
        return {'accuracy': accuracy, 'model_path': model_path, 'gpu_optimized': self.use_gpu}

# ============================================================================
# üìä EVALUATION
# ============================================================================

def evaluate_models(test_path):
    """ƒê√°nh gi√° models tr√™n test set"""
    print("üìä ƒê√°nh gi√° models...")
    
    # Load test data
    test_df = pd.read_csv(test_path, encoding='utf-8')
    X_test = test_df['text'].fillna('')
    y_test_level1 = test_df['type_level1']
    y_test_level2 = test_df['domain_level2']
    
    try:
        # Load v√† evaluate Level 1
        with open("models/saved_models/level1_classifier/svm_level1/svm_level1_model.pkl", 'rb') as f:
            level1_data = pickle.load(f)
        
        level1_model = level1_data['model']
        level1_vectorizer = level1_data['vectorizer']
        level1_feature_selector = level1_data['feature_selector']
        
        X_test_level1 = level1_vectorizer.transform(X_test)
        X_test_level1_selected = level1_feature_selector.transform(X_test_level1)
        y_pred_level1 = level1_model.predict(X_test_level1_selected)
        
        accuracy_level1 = accuracy_score(y_test_level1, y_pred_level1)
        print(f"üè∑Ô∏è Level 1 Test Accuracy: {accuracy_level1:.4f}")
        
        # Load v√† evaluate Level 2
        with open("models/saved_models/level2_classifier/svm_level2/svm_level2_model.pkl", 'rb') as f:
            level2_data = pickle.load(f)
        
        level2_model = level2_data['model']
        level2_vectorizer = level2_data['vectorizer']
        level2_feature_selector = level2_data['feature_selector']
        
        X_test_level2 = level2_vectorizer.transform(X_test)
        X_test_level2_selected = level2_feature_selector.transform(X_test_level2)
        y_pred_level2 = level2_model.predict(X_test_level2_selected)
        
        accuracy_level2 = accuracy_score(y_test_level2, y_pred_level2)
        print(f"üè∑Ô∏è Level 2 Test Accuracy: {accuracy_level2:.4f}")
        
        # L∆∞u k·∫øt qu·∫£
        results = {
            'level1': {'accuracy': accuracy_level1, 'gpu_optimized': level1_data.get('gpu_optimized', False)},
            'level2': {'accuracy': accuracy_level2, 'gpu_optimized': level2_data.get('gpu_optimized', False)}
        }
        
        results_path = "results/evaluation_results/svm_evaluation_results.pkl"
        with open(results_path, 'wb') as f:
            pickle.dump(results, f)
        
        print(f"üíæ K·∫øt qu·∫£ ƒë√£ l∆∞u: {results_path}")
        return results
        
    except Exception as e:
        print(f"‚ùå L·ªói khi ƒë√°nh gi√°: {e}")
        return None

# ============================================================================
# üöÄ MAIN PIPELINE
# ============================================================================

def main():
    """H√†m ch√≠nh ch·∫°y pipeline"""
    print("üöÄ VILEGALBERT PIPELINE - GPU OPTIMIZED")
    print("=" * 60)
    
    # B∆∞·ªõc 1: GPU setup
    print("\nüöÄ B∆Ø·ªöC 1: GPU SETUP")
    gpu_available = setup_gpu()
    
    # B∆∞·ªõc 2: C√†i ƒë·∫∑t dependencies
    print("\nüì¶ B∆Ø·ªöC 2: C√ÄI ƒê·∫∂T DEPENDENCIES")
    install_deps()
    
    # B∆∞·ªõc 3: T·∫°o th∆∞ m·ª•c
    print("\nüèóÔ∏è B∆Ø·ªöC 3: T·∫†O TH∆Ø M·ª§C")
    create_dirs()
    
    # B∆∞·ªõc 4: Load dataset
    print("\nüìä B∆Ø·ªöC 4: LOAD DATASET")
    df = load_dataset()
    if df is None:
        return
    
    # B∆∞·ªõc 5: T·∫°o splits
    print("\nüîÑ B∆Ø·ªöC 5: T·∫†O SPLITS")
    create_splits(df)
    
    # B∆∞·ªõc 6: Training SVM
    print("\nüèãÔ∏è B∆Ø·ªöC 6: TRAINING SVM")
    trainer = SVMTrainer()
    
    results_level1 = trainer.train_level1("data/processed/hierarchical_legal_dataset.csv")
    results_level2 = trainer.train_level2("data/processed/hierarchical_legal_dataset.csv")
    
    # B∆∞·ªõc 7: Evaluation
    print("\nüìä B∆Ø·ªöC 7: EVALUATION")
    evaluate_models("data/processed/dataset_splits/test.csv")
    
    # T√≥m t·∫Øt
    print("\nüéâ PIPELINE HO√ÄN TH√ÄNH!")
    print(f"üìä Dataset: {len(df)} samples")
    print(f"üè∑Ô∏è Level 1 Accuracy: {results_level1['accuracy']:.4f}")
    print(f"üè∑Ô∏è Level 2 Accuracy: {results_level2['accuracy']:.4f}")
    print(f"üöÄ GPU Status: {'‚úÖ Available' if gpu_available else '‚ùå Not Available'}")

if __name__ == "__main__":
    main() 