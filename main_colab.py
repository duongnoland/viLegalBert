#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ viLegalBert - Main Pipeline cho Google Colab
PhÃ¢n loáº¡i vÄƒn báº£n phÃ¡p luáº­t Viá»‡t Nam vá»›i kiáº¿n trÃºc phÃ¢n cáº¥p 2 táº§ng
"""

import os
import sys
import yaml
import pickle
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime
import logging
from typing import Dict, List, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# ğŸ“¦ INSTALL & IMPORT DEPENDENCIES
# ============================================================================

def install_dependencies():
    """CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t"""
    try:
        import sklearn
        print("âœ… scikit-learn Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t")
    except ImportError:
        print("ğŸ“¦ Äang cÃ i Ä‘áº·t scikit-learn...")
        os.system("pip install scikit-learn")
    
    try:
        import transformers
        print("âœ… transformers Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t")
    except ImportError:
        print("ğŸ“¦ Äang cÃ i Ä‘áº·t transformers...")
        os.system("pip install transformers")
    
    try:
        import torch
        print("âœ… PyTorch Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t")
    except ImportError:
        print("ğŸ“¦ Äang cÃ i Ä‘áº·t PyTorch...")
        os.system("pip install torch")
    
    try:
        import yaml
        print("âœ… PyYAML Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t")
    except ImportError:
        print("ğŸ“¦ Äang cÃ i Ä‘áº·t PyYAML...")
        os.system("pip install PyYAML")

# CÃ i Ä‘áº·t dependencies
install_dependencies()

# Import sau khi cÃ i Ä‘áº·t
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split
from sklearn.metrics import (
    classification_report, confusion_matrix, accuracy_score,
    precision_recall_fscore_support, roc_auc_score
)
from sklearn.calibration import CalibratedClassifierCV
from sklearn.pipeline import Pipeline
from sklearn.ensemble import VotingClassifier
import joblib

# ============================================================================
# ğŸ—ï¸ Cáº¤U TRÃšC PROJECT
# ============================================================================

def create_project_structure():
    """Táº¡o cáº¥u trÃºc thÆ° má»¥c cho project"""
    directories = [
        'data/raw',
        'data/processed',
        'data/processed/dataset_splits',
        'models/saved_models/level1_classifier/svm_level1',
        'models/saved_models/level2_classifier/svm_level2',
        'models/saved_models/level1_classifier/phobert_level1',
        'models/saved_models/level2_classifier/phobert_level2',
        'models/saved_models/level1_classifier/bilstm_level1',
        'models/saved_models/level2_classifier/bilstm_level2',
        'models/saved_models/hierarchical_models',
        'results/training_results',
        'results/evaluation_results',
        'logs'
    ]
    
    for directory in directories:
        Path(directory).mkdir(parents=True, exist_ok=True)
        print(f"âœ… Táº¡o thÆ° má»¥c: {directory}")

# ============================================================================
# ğŸ“Š DATASET CREATION
# ============================================================================

def create_hierarchical_dataset(json_file: str, output_csv: str, target_size: int = 10000) -> pd.DataFrame:
    """Táº¡o dataset phÃ¢n cáº¥p 2 táº§ng tá»« JSON gá»‘c"""
    print("ğŸ” Äang load file JSON...")
    
    try:
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"âœ… Load thÃ nh cÃ´ng {len(data)} items tá»« {json_file}")
    except Exception as e:
        print(f"âŒ Lá»—i khi load JSON: {e}")
        return None
    
    # Láº¥y máº«u ngáº«u nhiÃªn
    if len(data) > target_size:
        data = np.random.choice(data, target_size, replace=False).tolist()
        print(f"ğŸ“Š ÄÃ£ láº¥y máº«u {target_size} items")
    
    # Xá»­ lÃ½ tá»«ng item
    processed_data = []
    
    for item in data:
        try:
            # TrÃ­ch xuáº¥t thÃ´ng tin cÆ¡ báº£n
            doc_id = item.get('id', 'unknown')
            ministry = item.get('ministry', 'unknown')
            doc_type = item.get('type', 'unknown')
            name = item.get('name', '')
            chapter_name = item.get('chapter_name', '')
            article = item.get('article', '')
            content = item.get('content', '')
            
            # LÃ m sáº¡ch text
            text = f"{name} {chapter_name} {article} {content}".strip()
            text = ' '.join(text.split())  # Loáº¡i bá» khoáº£ng tráº¯ng thá»«a
            
            # PhÃ¢n loáº¡i Level 1 (Loáº¡i vÄƒn báº£n)
            type_level1 = extract_document_type(doc_type, name)
            
            # PhÃ¢n loáº¡i Level 2 (Domain phÃ¡p lÃ½)
            domain_level2 = extract_legal_domain(content, name, chapter_name)
            
            # TÃ­nh Ä‘á»™ dÃ i ná»™i dung
            content_length = len(content) if content else 0
            
            processed_data.append({
                'id': doc_id,
                'text': text,
                'type_level1': type_level1,
                'domain_level2': domain_level2,
                'ministry': ministry,
                'name': name,
                'chapter': chapter_name,
                'article': article,
                'content_length': content_length
            })
            
        except Exception as e:
            print(f"âš ï¸ Lá»—i khi xá»­ lÃ½ item {item.get('id', 'unknown')}: {e}")
            continue
    
    # Táº¡o DataFrame
    df = pd.DataFrame(processed_data)
    
    # LÆ°u dataset
    df.to_csv(output_csv, index=False, encoding='utf-8')
    print(f"âœ… ÄÃ£ lÆ°u dataset vÃ o: {output_csv}")
    
    # Hiá»ƒn thá»‹ thá»‘ng kÃª
    print(f"\nğŸ“ˆ THá»NG KÃŠ DATASET:")
    print(f"Tá»•ng sá»‘ samples: {len(df)}")
    
    print(f"\nğŸ·ï¸ PHÃ‚N LOáº I Táº¦NG 1 (Loáº¡i vÄƒn báº£n):")
    level1_counts = df['type_level1'].value_counts()
    for doc_type, count in level1_counts.items():
        print(f"  - {doc_type}: {count}")
    
    print(f"\nğŸ·ï¸ PHÃ‚N LOáº I Táº¦NG 2 (Domain phÃ¡p lÃ½):")
    level2_counts = df['domain_level2'].value_counts()
    for domain, count in level2_counts.items():
        print(f"  - {domain}: {count}")
    
    return df

def extract_document_type(doc_type: str, name: str) -> str:
    """TrÃ­ch xuáº¥t loáº¡i vÄƒn báº£n tá»« type vÃ  name"""
    if not doc_type and not name:
        return "KHÃC"
    
    text = f"{doc_type} {name}".upper()
    
    if any(keyword in text for keyword in ["LUáº¬T", "LAW"]):
        return "LUáº¬T"
    elif any(keyword in text for keyword in ["NGHá»Š Äá»ŠNH", "DECREE"]):
        return "NGHá»Š Äá»ŠNH"
    elif any(keyword in text for keyword in ["THÃ”NG TÆ¯", "CIRCULAR"]):
        return "THÃ”NG TÆ¯"
    elif any(keyword in text for keyword in ["QUYáº¾T Äá»ŠNH", "DECISION"]):
        return "QUYáº¾T Äá»ŠNH"
    elif any(keyword in text for keyword in ["NGHá»Š QUYáº¾T", "RESOLUTION"]):
        return "NGHá»Š QUYáº¾T"
    elif any(keyword in text for keyword in ["PHÃP Lá»†NH", "ORDINANCE"]):
        return "PHÃP Lá»†NH"
    else:
        return "KHÃC"

def extract_legal_domain(content: str, name: str, chapter_name: str) -> str:
    """TrÃ­ch xuáº¥t domain phÃ¡p lÃ½ tá»« ná»™i dung"""
    if not content:
        return "KHÃC"
    
    # Káº¿t há»£p ná»™i dung Ä‘á»ƒ phÃ¢n tÃ­ch
    full_text = f"{name} {chapter_name} {content}".upper()
    
    # Mapping cÃ¡c domain phÃ¡p lÃ½ vá»›i tá»« khÃ³a tiáº¿ng Viá»‡t
    domain_keywords = {
        "HÃŒNH Sá»°": ["hÃ¬nh sá»±", "tá»™i pháº¡m", "xá»­ lÃ½ vi pháº¡m", "pháº¡t tÃ¹", "cáº£i táº¡o"],
        "DÃ‚N Sá»°": ["dÃ¢n sá»±", "há»£p Ä‘á»“ng", "quyá»n sá»Ÿ há»¯u", "thá»«a káº¿", "hÃ´n nhÃ¢n gia Ä‘Ã¬nh"],
        "HÃ€NH CHÃNH": ["hÃ nh chÃ­nh", "xá»­ pháº¡t vi pháº¡m", "thá»§ tá»¥c hÃ nh chÃ­nh", "quyáº¿t Ä‘á»‹nh hÃ nh chÃ­nh"],
        "LAO Äá»˜NG": ["lao Ä‘á»™ng", "há»£p Ä‘á»“ng lao Ä‘á»™ng", "tiá»n lÆ°Æ¡ng", "báº£o hiá»ƒm xÃ£ há»™i"],
        "THUáº¾": ["thuáº¿", "thuáº¿ thu nháº­p", "thuáº¿ giÃ¡ trá»‹ gia tÄƒng", "khai thuáº¿", "ná»™p thuáº¿"],
        "DOANH NGHIá»†P": ["doanh nghiá»‡p", "cÃ´ng ty", "thÃ nh láº­p doanh nghiá»‡p", "quáº£n lÃ½ doanh nghiá»‡p"],
        "Äáº¤T ÄAI": ["Ä‘áº¥t Ä‘ai", "quyá»n sá»­ dá»¥ng Ä‘áº¥t", "thá»§ tá»¥c Ä‘áº¥t Ä‘ai", "bá»“i thÆ°á»ng Ä‘áº¥t Ä‘ai"],
        "XÃ‚Y Dá»°NG": ["xÃ¢y dá»±ng", "giáº¥y phÃ©p xÃ¢y dá»±ng", "quy hoáº¡ch", "kiáº¿n trÃºc", "thiáº¿t káº¿"],
        "GIAO THÃ”NG": ["giao thÃ´ng", "luáº­t giao thÃ´ng", "vi pháº¡m giao thÃ´ng", "phÆ°Æ¡ng tiá»‡n giao thÃ´ng"],
        "Y Táº¾": ["y táº¿", "khÃ¡m chá»¯a bá»‡nh", "dÆ°á»£c pháº©m", "vá»‡ sinh an toÃ n thá»±c pháº©m"],
        "GIÃO Dá»¤C": ["giÃ¡o dá»¥c", "Ä‘Ã o táº¡o", "chÆ°Æ¡ng trÃ¬nh giÃ¡o dá»¥c", "báº±ng cáº¥p", "chá»©ng chá»‰"],
        "TÃ€I CHÃNH": ["tÃ i chÃ­nh", "ngÃ¢n hÃ ng", "tÃ­n dá»¥ng", "tiá»n tá»‡", "Ä‘áº§u tÆ°"],
        "MÃ”I TRÆ¯á»œNG": ["mÃ´i trÆ°á»ng", "báº£o vá»‡ mÃ´i trÆ°á»ng", "Ã´ nhiá»…m", "xá»­ lÃ½ cháº¥t tháº£i"],
        "AN NINH": ["an ninh", "quá»‘c phÃ²ng", "báº£o vá»‡ an ninh", "tráº­t tá»± an toÃ n xÃ£ há»™i"]
    }
    
    # Äáº¿m sá»‘ tá»« khÃ³a xuáº¥t hiá»‡n cho má»—i domain
    domain_scores = {}
    for domain, keywords in domain_keywords.items():
        score = 0
        for keyword in keywords:
            if keyword.upper() in full_text:
                score += 1
        
        if score > 0:
            domain_scores[domain] = score
    
    # Tráº£ vá» domain cÃ³ Ä‘iá»ƒm cao nháº¥t
    if domain_scores:
        best_domain = max(domain_scores, key=domain_scores.get)
        return best_domain
    
    return "KHÃC"

def create_training_splits(csv_path: str, splits_dir: str):
    """Táº¡o cÃ¡c táº­p train/validation/test"""
    print("ğŸ”„ Táº¡o cÃ¡c táº­p train/validation/test...")
    
    # Load dataset
    df = pd.read_csv(csv_path, encoding='utf-8')
    
    # Chia dá»¯ liá»‡u
    train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['type_level1'])
    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['type_level1'])
    
    # LÆ°u cÃ¡c táº­p
    train_path = Path(splits_dir) / "train.csv"
    val_path = Path(splits_dir) / "validation.csv"
    test_path = Path(splits_dir) / "test.csv"
    
    train_df.to_csv(train_path, index=False, encoding='utf-8')
    val_df.to_csv(val_path, index=False, encoding='utf-8')
    test_df.to_csv(test_path, index=False, encoding='utf-8')
    
    print(f"âœ… Train set: {len(train_df)} samples -> {train_path}")
    print(f"âœ… Validation set: {len(val_df)} samples -> {val_path}")
    print(f"âœ… Test set: {len(test_df)} samples -> {test_path}")

# ============================================================================
# ğŸ‹ï¸ SVM TRAINER
# ============================================================================

class SVMTrainer:
    """Trainer cho mÃ´ hÃ¬nh SVM"""
    
    def __init__(self):
        """Khá»Ÿi táº¡o trainer"""
        self.config = {
            'feature_extraction': {
                'tfidf': {
                    'max_features': 10000,
                    'min_df': 2,
                    'max_df': 0.95,
                    'ngram_range': [1, 2],
                    'stop_words': None
                }
            },
            'svm': {
                'kernel': 'rbf',
                'C': 1.0,
                'gamma': 'scale'
            },
            'feature_selection': {
                'k_best': 5000
            }
        }
        self.models = {}
        self.vectorizers = {}
        self.feature_selectors = {}
    
    def train_level1(self, data_path: str) -> Dict[str, Any]:
        """Training cho Level 1 (Loáº¡i vÄƒn báº£n)"""
        print("ğŸ·ï¸ Training Level 1 (Loáº¡i vÄƒn báº£n)...")
        
        # Load data
        df = pd.read_csv(data_path, encoding='utf-8')
        X = df['text'].fillna('')
        y = df['type_level1']
        
        # Chia train/validation
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # TF-IDF Vectorization
        print("ğŸ“Š TF-IDF Vectorization...")
        vectorizer = TfidfVectorizer(
            max_features=self.config['feature_extraction']['tfidf']['max_features'],
            min_df=self.config['feature_extraction']['tfidf']['min_df'],
            max_df=self.config['feature_extraction']['tfidf']['max_df'],
            ngram_range=tuple(self.config['feature_extraction']['tfidf']['ngram_range'])
        )
        
        X_train_tfidf = vectorizer.fit_transform(X_train)
        X_val_tfidf = vectorizer.transform(X_val)
        
        # Feature Selection
        print("ğŸ” Feature Selection...")
        feature_selector = SelectKBest(chi2, k=self.config['feature_selection']['k_best'])
        X_train_selected = feature_selector.fit_transform(X_train_tfidf, y_train)
        X_val_selected = feature_selector.transform(X_val_tfidf)
        
        # SVM Training
        print("ğŸ‹ï¸ Training SVM...")
        svm = SVC(
            kernel=self.config['svm']['kernel'],
            C=self.config['svm']['C'],
            gamma=self.config['svm']['gamma'],
            random_state=42,
            probability=True
        )
        
        svm.fit(X_train_selected, y_train)
        
        # Evaluation
        y_pred = svm.predict(X_val_selected)
        accuracy = accuracy_score(y_val, y_pred)
        
        print(f"âœ… Level 1 Training hoÃ n thÃ nh!")
        print(f"ğŸ“Š Accuracy: {accuracy:.4f}")
        print(f"ğŸ“Š Classification Report:")
        print(classification_report(y_val, y_pred))
        
        # LÆ°u model
        self.models['level1'] = svm
        self.vectorizers['level1'] = vectorizer
        self.feature_selectors['level1'] = feature_selector
        
        # LÆ°u model
        model_path = "models/saved_models/level1_classifier/svm_level1/svm_level1_model.pkl"
        Path(model_path).parent.mkdir(parents=True, exist_ok=True)
        
        model_data = {
            'model': svm,
            'vectorizer': vectorizer,
            'feature_selector': feature_selector
        }
        
        with open(model_path, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"ğŸ’¾ Model Ä‘Ã£ Ä‘Æ°á»£c lÆ°u: {model_path}")
        
        return {
            'accuracy': accuracy,
            'model_path': model_path,
            'classification_report': classification_report(y_val, y_pred, output_dict=True)
        }
    
    def train_level2(self, data_path: str) -> Dict[str, Any]:
        """Training cho Level 2 (Domain phÃ¡p lÃ½)"""
        print("ğŸ·ï¸ Training Level 2 (Domain phÃ¡p lÃ½)...")
        
        # Load data
        df = pd.read_csv(data_path, encoding='utf-8')
        X = df['text'].fillna('')
        y = df['domain_level2']
        
        # Chia train/validation
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # TF-IDF Vectorization
        print("ğŸ“Š TF-IDF Vectorization...")
        vectorizer = TfidfVectorizer(
            max_features=self.config['feature_extraction']['tfidf']['max_features'],
            min_df=self.config['feature_extraction']['tfidf']['min_df'],
            max_df=self.config['feature_extraction']['tfidf']['max_df'],
            ngram_range=tuple(self.config['feature_extraction']['tfidf']['ngram_range'])
        )
        
        X_train_tfidf = vectorizer.fit_transform(X_train)
        X_val_tfidf = vectorizer.transform(X_val)
        
        # Feature Selection
        print("ğŸ” Feature Selection...")
        feature_selector = SelectKBest(chi2, k=self.config['feature_selection']['k_best'])
        X_train_selected = feature_selector.fit_transform(X_train_tfidf, y_train)
        X_val_selected = feature_selector.transform(X_val_tfidf)
        
        # SVM Training
        print("ğŸ‹ï¸ Training SVM...")
        svm = SVC(
            kernel=self.config['svm']['kernel'],
            C=self.config['svm']['C'],
            gamma=self.config['svm']['gamma'],
            random_state=42,
            probability=True
        )
        
        svm.fit(X_train_selected, y_train)
        
        # Evaluation
        y_pred = svm.predict(X_val_selected)
        accuracy = accuracy_score(y_val, y_pred)
        
        print(f"âœ… Level 2 Training hoÃ n thÃ nh!")
        print(f"ğŸ“Š Accuracy: {accuracy:.4f}")
        print(f"ğŸ“Š Classification Report:")
        print(classification_report(y_val, y_pred))
        
        # LÆ°u model
        self.models['level2'] = svm
        self.vectorizers['level2'] = vectorizer
        self.feature_selectors['level2'] = feature_selector
        
        # LÆ°u model
        model_path = "models/saved_models/level2_classifier/svm_level2/svm_level2_model.pkl"
        Path(model_path).parent.mkdir(parents=True, exist_ok=True)
        
        model_data = {
            'model': svm,
            'vectorizer': vectorizer,
            'feature_selector': feature_selector
        }
        
        with open(model_path, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"ğŸ’¾ Model Ä‘Ã£ Ä‘Æ°á»£c lÆ°u: {model_path}")
        
        return {
            'accuracy': accuracy,
            'model_path': model_path,
            'classification_report': classification_report(y_val, y_pred, output_dict=True)
        }

# ============================================================================
# ğŸ“Š EVALUATION
# ============================================================================

def evaluate_svm_models(test_data_path: str):
    """ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh SVM trÃªn test set"""
    print("ğŸ“Š ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh SVM...")
    
    # Load test data
    test_df = pd.read_csv(test_data_path, encoding='utf-8')
    X_test = test_df['text'].fillna('')
    y_test_level1 = test_df['type_level1']
    y_test_level2 = test_df['domain_level2']
    
    # Load models
    try:
        # Level 1
        with open("models/saved_models/level1_classifier/svm_level1/svm_level1_model.pkl", 'rb') as f:
            level1_data = pickle.load(f)
        
        level1_model = level1_data['model']
        level1_vectorizer = level1_data['vectorizer']
        level1_feature_selector = level1_data['feature_selector']
        
        # Level 2
        with open("models/saved_models/level2_classifier/svm_level2/svm_level2_model.pkl", 'rb') as f:
            level2_data = pickle.load(f)
        
        level2_model = level2_data['model']
        level2_vectorizer = level2_data['vectorizer']
        level2_feature_selector = level2_data['feature_selector']
        
        # Evaluation Level 1
        print("\nğŸ·ï¸ EVALUATION LEVEL 1 (Loáº¡i vÄƒn báº£n):")
        X_test_level1 = level1_vectorizer.transform(X_test)
        X_test_level1_selected = level1_feature_selector.transform(X_test_level1)
        y_pred_level1 = level1_model.predict(X_test_level1_selected)
        
        accuracy_level1 = accuracy_score(y_test_level1, y_pred_level1)
        print(f"ğŸ“Š Accuracy: {accuracy_level1:.4f}")
        print(f"ğŸ“Š Classification Report:")
        print(classification_report(y_test_level1, y_pred_level1))
        
        # Evaluation Level 2
        print("\nğŸ·ï¸ EVALUATION LEVEL 2 (Domain phÃ¡p lÃ½):")
        X_test_level2 = level2_vectorizer.transform(X_test)
        X_test_level2_selected = level2_feature_selector.transform(X_test_level2)
        y_pred_level2 = level2_model.predict(X_test_level2_selected)
        
        accuracy_level2 = accuracy_score(y_test_level2, y_pred_level2)
        print(f"ğŸ“Š Accuracy: {accuracy_level2:.4f}")
        print(f"ğŸ“Š Classification Report:")
        print(classification_report(y_test_level2, y_pred_level2))
        
        # LÆ°u káº¿t quáº£
        results = {
            'level1': {
                'accuracy': accuracy_level1,
                'classification_report': classification_report(y_test_level1, y_pred_level1, output_dict=True)
            },
            'level2': {
                'accuracy': accuracy_level2,
                'classification_report': classification_report(y_test_level2, y_pred_level2, output_dict=True)
            }
        }
        
        results_path = "results/evaluation_results/svm_evaluation_results.pkl"
        Path(results_path).parent.mkdir(parents=True, exist_ok=True)
        
        with open(results_path, 'wb') as f:
            pickle.dump(results, f)
        
        print(f"\nğŸ’¾ Káº¿t quáº£ evaluation Ä‘Ã£ Ä‘Æ°á»£c lÆ°u: {results_path}")
        
        return results
        
    except Exception as e:
        print(f"âŒ Lá»—i khi Ä‘Ã¡nh giÃ¡: {e}")
        return None

# ============================================================================
# ğŸš€ MAIN PIPELINE
# ============================================================================

def main():
    """HÃ m chÃ­nh cháº¡y pipeline"""
    print("ğŸš€ KHá»I Äá»˜NG VILEGALBERT PIPELINE CHO GOOGLE COLAB!")
    print("=" * 80)
    
    # Táº¡o cáº¥u trÃºc project
    create_project_structure()
    
    # BÆ°á»›c 1: Táº¡o dataset
    print("\nğŸ“Š BÆ¯á»šC 1: Táº O DATASET")
    print("-" * 50)
    
    # Kiá»ƒm tra xem cÃ³ file JSON khÃ´ng
    json_files = list(Path('.').glob('*.json'))
    if json_files:
        json_file = str(json_files[0])
        print(f"ğŸ” TÃ¬m tháº¥y file JSON: {json_file}")
    else:
        print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y file JSON. Vui lÃ²ng upload file vbpl_crawl.json vÃ o Colab")
        return
    
    # Táº¡o dataset
    output_csv = "data/processed/hierarchical_legal_dataset.csv"
    df = create_hierarchical_dataset(json_file, output_csv, target_size=10000)
    
    if df is None:
        print("âŒ KhÃ´ng thá»ƒ táº¡o dataset")
        return
    
    # Táº¡o splits
    splits_dir = "data/processed/dataset_splits"
    create_training_splits(output_csv, splits_dir)
    
    # BÆ°á»›c 2: Training SVM
    print("\nğŸ‹ï¸ BÆ¯á»šC 2: TRAINING SVM")
    print("-" * 50)
    
    trainer = SVMTrainer()
    
    # Training Level 1
    results_level1 = trainer.train_level1(output_csv)
    
    # Training Level 2
    results_level2 = trainer.train_level2(output_csv)
    
    # BÆ°á»›c 3: Evaluation
    print("\nğŸ“Š BÆ¯á»šC 3: EVALUATION")
    print("-" * 50)
    
    test_data_path = "data/processed/dataset_splits/test.csv"
    evaluation_results = evaluate_svm_models(test_data_path)
    
    # TÃ³m táº¯t káº¿t quáº£
    print("\nğŸ‰ TÃ“M Táº®T Káº¾T QUáº¢")
    print("=" * 80)
    print(f"ğŸ“Š Dataset: {len(df)} samples")
    print(f"ğŸ·ï¸ Level 1 Accuracy: {results_level1['accuracy']:.4f}")
    print(f"ğŸ·ï¸ Level 2 Accuracy: {results_level2['accuracy']:.4f}")
    print(f"ğŸ’¾ Models Ä‘Ã£ Ä‘Æ°á»£c lÆ°u trong thÆ° má»¥c models/")
    print(f"ğŸ“Š Káº¿t quáº£ evaluation Ä‘Ã£ Ä‘Æ°á»£c lÆ°u trong thÆ° má»¥c results/")
    
    print("\nâœ… PIPELINE HOÃ€N THÃ€NH THÃ€NH CÃ”NG!")
    print("ğŸš€ Báº¡n cÃ³ thá»ƒ tiáº¿p tá»¥c vá»›i training PhoBERT, BiLSTM hoáº·c Ensemble!")

if __name__ == "__main__":
    main() 