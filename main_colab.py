#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üöÄ viLegalBert - Main Pipeline cho Google Colab (Dataset C√≥ S·∫µn)
Ph√¢n lo·∫°i vƒÉn b·∫£n ph√°p lu·∫≠t Vi·ªát Nam v·ªõi ki·∫øn tr√∫c ph√¢n c·∫•p 2 t·∫ßng
"""

import os
import sys
import yaml
import pickle
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime
import logging
from typing import Dict, List, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# üì¶ INSTALL & IMPORT DEPENDENCIES
# ============================================================================

def install_dependencies():
    """C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt"""
    try:
        import sklearn
        print("‚úÖ scikit-learn ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t")
    except ImportError:
        print("üì¶ ƒêang c√†i ƒë·∫∑t scikit-learn...")
        os.system("pip install scikit-learn")
    
    try:
        import transformers
        print("‚úÖ transformers ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t")
    except ImportError:
        print("üì¶ ƒêang c√†i ƒë·∫∑t transformers...")
        os.system("pip install transformers")
    
    try:
        import torch
        print("‚úÖ PyTorch ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t")
    except ImportError:
        print("üì¶ ƒêang c√†i ƒë·∫∑t PyTorch...")
        os.system("pip install torch")
    
    try:
        import yaml
        print("‚úÖ PyYAML ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t")
    except ImportError:
        print("üì¶ ƒêang c√†i ƒë·∫∑t PyYAML...")
        os.system("pip install PyYAML")

# C√†i ƒë·∫∑t dependencies
install_dependencies()

# Import sau khi c√†i ƒë·∫∑t
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split
from sklearn.metrics import (
    classification_report, confusion_matrix, accuracy_score,
    precision_recall_fscore_support, roc_auc_score
)
from sklearn.calibration import CalibratedClassifierCV
from sklearn.pipeline import Pipeline
from sklearn.ensemble import VotingClassifier
import joblib

# ============================================================================
# üèóÔ∏è C·∫§U TR√öC PROJECT
# ============================================================================

def create_project_structure():
    """T·∫°o c·∫•u tr√∫c th∆∞ m·ª•c cho project"""
    directories = [
        'models/saved_models/level1_classifier/svm_level1',
        'models/saved_models/level2_classifier/svm_level2',
        'models/saved_models/level1_classifier/phobert_level1',
        'models/saved_models/level2_classifier/phobert_level2',
        'models/saved_models/level1_classifier/bilstm_level1',
        'models/saved_models/level2_classifier/bilstm_level2',
        'models/saved_models/hierarchical_models',
        'results/training_results',
        'results/evaluation_results',
        'logs'
    ]
    
    for directory in directories:
        Path(directory).mkdir(parents=True, exist_ok=True)
        print(f"‚úÖ T·∫°o th∆∞ m·ª•c: {directory}")

# ============================================================================
# üìä DATASET LOADING (C√≥ S·∫µn)
# ============================================================================

def load_existing_dataset(dataset_path: str = "data/processed/hierarchical_legal_dataset.csv"):
    """Load dataset c√≥ s·∫µn"""
    print("üìä Loading dataset c√≥ s·∫µn...")
    
    try:
        # Ki·ªÉm tra file dataset
        if not Path(dataset_path).exists():
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y dataset: {dataset_path}")
            print("üîç T√¨m ki·∫øm dataset trong c√°c th∆∞ m·ª•c...")
            
            # T√¨m ki·∫øm dataset trong c√°c th∆∞ m·ª•c kh√°c
            possible_paths = [
                "hierarchical_legal_dataset.csv",
                "data/hierarchical_legal_dataset.csv",
                "dataset.csv",
                "legal_dataset.csv"
            ]
            
            for path in possible_paths:
                if Path(path).exists():
                    dataset_path = path
                    print(f"‚úÖ T√¨m th·∫•y dataset: {dataset_path}")
                    break
            else:
                print("‚ùå Kh√¥ng t√¨m th·∫•y dataset n√†o. Vui l√≤ng upload dataset v√†o Colab")
                return None
        
        # Load dataset
        df = pd.read_csv(dataset_path, encoding='utf-8')
        print(f"‚úÖ ƒê√£ load dataset: {len(df)} samples")
        
        # Hi·ªÉn th·ªã th√¥ng tin dataset
        print(f"\nüìà TH√îNG TIN DATASET:")
        print(f"Shape: {df.shape}")
        print(f"Columns: {list(df.columns)}")
        
        # Ki·ªÉm tra columns c·∫ßn thi·∫øt
        required_columns = ['text', 'type_level1', 'domain_level2']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            print(f"‚ùå Thi·∫øu columns: {missing_columns}")
            print(f"üìã Columns c√≥ s·∫µn: {list(df.columns)}")
            return None
        
        # Hi·ªÉn th·ªã th·ªëng k√™
        print(f"\nüè∑Ô∏è PH√ÇN LO·∫†I T·∫¶NG 1 (Lo·∫°i vƒÉn b·∫£n):")
        level1_counts = df['type_level1'].value_counts()
        for doc_type, count in level1_counts.items():
            print(f"  - {doc_type}: {count}")
        
        print(f"\nüè∑Ô∏è PH√ÇN LO·∫†I T·∫¶NG 2 (Domain ph√°p l√Ω):")
        level2_counts = df['domain_level2'].value_counts()
        for domain, count in level2_counts.items():
            print(f"  - {domain}: {count}")
        
        return df
        
    except Exception as e:
        print(f"‚ùå L·ªói khi load dataset: {e}")
        return None

def check_dataset_splits():
    """Ki·ªÉm tra dataset splits c√≥ s·∫µn"""
    print("üîç Ki·ªÉm tra dataset splits...")
    
    splits_dir = "data/processed/dataset_splits"
    train_path = Path(splits_dir) / "train.csv"
    val_path = Path(splits_dir) / "validation.csv"
    test_path = Path(splits_dir) / "test.csv"
    
    if train_path.exists() and val_path.exists() and test_path.exists():
        print("‚úÖ Dataset splits ƒë√£ c√≥ s·∫µn")
        
        # Load v√† hi·ªÉn th·ªã th√¥ng tin splits
        train_df = pd.read_csv(train_path, encoding='utf-8')
        val_df = pd.read_csv(val_path, encoding='utf-8')
        test_df = pd.read_csv(test_path, encoding='utf-8')
        
        print(f"üìä Train set: {len(train_df)} samples")
        print(f"üìä Validation set: {len(val_df)} samples")
        print(f"üìä Test set: {len(test_df)} samples")
        
        return True
    else:
        print("‚ö†Ô∏è Dataset splits ch∆∞a c√≥, s·∫Ω t·∫°o m·ªõi...")
        return False

def create_training_splits_from_existing(dataset_path: str, splits_dir: str):
    """T·∫°o training splits t·ª´ dataset c√≥ s·∫µn"""
    print("üîÑ T·∫°o training splits t·ª´ dataset c√≥ s·∫µn...")
    
    # Load dataset
    df = pd.read_csv(dataset_path, encoding='utf-8')
    
    # Chia d·ªØ li·ªáu
    train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['type_level1'])
    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['type_level1'])
    
    # L∆∞u c√°c t·∫≠p
    train_path = Path(splits_dir) / "train.csv"
    val_path = Path(splits_dir) / "validation.csv"
    test_path = Path(splits_dir) / "test.csv"
    
    train_df.to_csv(train_path, index=False, encoding='utf-8')
    val_df.to_csv(val_path, index=False, encoding='utf-8')
    test_df.to_csv(test_path, index=False, encoding='utf-8')
    
    print(f"‚úÖ Train set: {len(train_df)} samples -> {train_path}")
    print(f"‚úÖ Validation set: {len(val_df)} samples -> {val_path}")
    print(f"‚úÖ Test set: {len(test_df)} samples -> {test_path}")

# ============================================================================
# üèãÔ∏è SVM TRAINER
# ============================================================================

class SVMTrainer:
    """Trainer cho m√¥ h√¨nh SVM"""
    
    def __init__(self):
        """Kh·ªüi t·∫°o trainer"""
        self.config = {
            'feature_extraction': {
                'tfidf': {
                    'max_features': 10000,
                    'min_df': 2,
                    'max_df': 0.95,
                    'ngram_range': [1, 2],
                    'stop_words': None
                }
            },
            'svm': {
                'kernel': 'rbf',
                'C': 1.0,
                'gamma': 'scale'
            },
            'feature_selection': {
                'k_best': 5000
            }
        }
        self.models = {}
        self.vectorizers = {}
        self.feature_selectors = {}
    
    def train_level1(self, data_path: str):
        """Training cho Level 1 (Lo·∫°i vƒÉn b·∫£n)"""
        print("üè∑Ô∏è Training Level 1 (Lo·∫°i vƒÉn b·∫£n)...")
        
        # Load data
        df = pd.read_csv(data_path, encoding='utf-8')
        X = df['text'].fillna('')
        y = df['type_level1']
        
        # Chia train/validation
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # TF-IDF Vectorization
        print("üìä TF-IDF Vectorization...")
        vectorizer = TfidfVectorizer(
            max_features=self.config['feature_extraction']['tfidf']['max_features'],
            min_df=self.config['feature_extraction']['tfidf']['min_df'],
            max_df=self.config['feature_extraction']['tfidf']['max_df'],
            ngram_range=tuple(self.config['feature_extraction']['tfidf']['ngram_range'])
        )
        
        X_train_tfidf = vectorizer.fit_transform(X_train)
        X_val_tfidf = vectorizer.transform(X_val)
        
        # Feature Selection
        print("üîç Feature Selection...")
        feature_selector = SelectKBest(chi2, k=self.config['feature_selection']['k_best'])
        X_train_selected = feature_selector.fit_transform(X_train_tfidf, y_train)
        X_val_selected = feature_selector.transform(X_val_tfidf)
        
        # SVM Training
        print("üèãÔ∏è Training SVM...")
        svm = SVC(
            kernel=self.config['svm']['kernel'],
            C=self.config['svm']['C'],
            gamma=self.config['svm']['gamma'],
            random_state=42,
            probability=True
        )
        
        svm.fit(X_train_selected, y_train)
        
        # Evaluation
        y_pred = svm.predict(X_val_selected)
        accuracy = accuracy_score(y_val, y_pred)
        
        print(f"‚úÖ Level 1 Training ho√†n th√†nh!")
        print(f"üìä Accuracy: {accuracy:.4f}")
        print(f"üìä Classification Report:")
        print(classification_report(y_val, y_pred))
        
        # L∆∞u model
        self.models['level1'] = svm
        self.vectorizers['level1'] = vectorizer
        self.feature_selectors['level1'] = feature_selector
        
        # L∆∞u model
        model_path = "models/saved_models/level1_classifier/svm_level1/svm_level1_model.pkl"
        Path(model_path).parent.mkdir(parents=True, exist_ok=True)
        
        model_data = {
            'model': svm,
            'vectorizer': vectorizer,
            'feature_selector': feature_selector
        }
        
        with open(model_path, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"üíæ Model ƒë√£ ƒë∆∞·ª£c l∆∞u: {model_path}")
        
        return {
            'accuracy': accuracy,
            'model_path': model_path,
            'classification_report': classification_report(y_val, y_pred, output_dict=True)
        }
    
    def train_level2(self, data_path: str):
        """Training cho Level 2 (Domain ph√°p l√Ω)"""
        print("üè∑Ô∏è Training Level 2 (Domain ph√°p l√Ω)...")
        
        # Load data
        df = pd.read_csv(data_path, encoding='utf-8')
        X = df['text'].fillna('')
        y = df['domain_level2']
        
        # Chia train/validation
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # TF-IDF Vectorization
        print("üìä TF-IDF Vectorization...")
        vectorizer = TfidfVectorizer(
            max_features=self.config['feature_extraction']['tfidf']['max_features'],
            min_df=self.config['feature_extraction']['tfidf']['min_df'],
            max_df=self.config['feature_extraction']['tfidf']['max_df'],
            ngram_range=tuple(self.config['feature_extraction']['tfidf']['ngram_range'])
        )
        
        X_train_tfidf = vectorizer.fit_transform(X_train)
        X_val_tfidf = vectorizer.transform(X_val)
        
        # Feature Selection
        print("üîç Feature Selection...")
        feature_selector = SelectKBest(chi2, k=self.config['feature_selection']['k_best'])
        X_train_selected = feature_selector.fit_transform(X_train_tfidf, y_train)
        X_val_selected = feature_selector.transform(X_val_tfidf)
        
        # SVM Training
        print("üèãÔ∏è Training SVM...")
        svm = SVC(
            kernel=self.config['svm']['kernel'],
            C=self.config['svm']['C'],
            gamma=self.config['svm']['gamma'],
            random_state=42,
            probability=True
        )
        
        svm.fit(X_train_selected, y_train)
        
        # Evaluation
        y_pred = svm.predict(X_val_selected)
        accuracy = accuracy_score(y_val, y_pred)
        
        print(f"‚úÖ Level 2 Training ho√†n th√†nh!")
        print(f"üìä Accuracy: {accuracy:.4f}")
        print(f"üìä Classification Report:")
        print(classification_report(y_val, y_pred))
        
        # L∆∞u model
        self.models['level2'] = svm
        self.vectorizers['level2'] = vectorizer
        self.feature_selectors['level2'] = feature_selector
        
        # L∆∞u model
        model_path = "models/saved_models/level2_classifier/svm_level2/svm_level2_model.pkl"
        Path(model_path).parent.mkdir(parents=True, exist_ok=True)
        
        model_data = {
            'model': svm,
            'vectorizer': vectorizer,
            'feature_selector': feature_selector
        }
        
        with open(model_path, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"üíæ Model ƒë√£ ƒë∆∞·ª£c l∆∞u: {model_path}")
        
        return {
            'accuracy': accuracy,
            'model_path': model_path,
            'classification_report': classification_report(y_val, y_pred, output_dict=True)
        }

# ============================================================================
# üìä EVALUATION
# ============================================================================

def evaluate_svm_models(test_data_path: str):
    """ƒê√°nh gi√° m√¥ h√¨nh SVM tr√™n test set"""
    print("üìä ƒê√°nh gi√° m√¥ h√¨nh SVM...")
    
    # Load test data
    test_df = pd.read_csv(test_data_path, encoding='utf-8')
    X_test = test_df['text'].fillna('')
    y_test_level1 = test_df['type_level1']
    y_test_level2 = test_df['domain_level2']
    
    # Load models
    try:
        # Level 1
        with open("models/saved_models/level1_classifier/svm_level1/svm_level1_model.pkl", 'rb') as f:
            level1_data = pickle.load(f)
        
        level1_model = level1_data['model']
        level1_vectorizer = level1_data['vectorizer']
        level1_feature_selector = level1_data['feature_selector']
        
        # Level 2
        with open("models/saved_models/level2_classifier/svm_level2/svm_level2_model.pkl", 'rb') as f:
            level2_data = pickle.load(f)
        
        level2_model = level2_data['model']
        level2_vectorizer = level2_data['vectorizer']
        level2_feature_selector = level2_data['feature_selector']
        
        # Evaluation Level 1
        print("\nüè∑Ô∏è EVALUATION LEVEL 1 (Lo·∫°i vƒÉn b·∫£n):")
        X_test_level1 = level1_vectorizer.transform(X_test)
        X_test_level1_selected = level1_feature_selector.transform(X_test_level1)
        y_pred_level1 = level1_model.predict(X_test_level1_selected)
        
        accuracy_level1 = accuracy_score(y_test_level1, y_pred_level1)
        print(f"üìä Accuracy: {accuracy_level1:.4f}")
        print(f"üìä Classification Report:")
        print(classification_report(y_test_level1, y_pred_level1))
        
        # Evaluation Level 2
        print("\nüè∑Ô∏è EVALUATION LEVEL 2 (Domain ph√°p l√Ω):")
        X_test_level2 = level2_vectorizer.transform(X_test)
        X_test_level2_selected = level2_feature_selector.transform(X_test_level2)
        y_pred_level2 = level2_model.predict(X_test_level2_selected)
        
        accuracy_level2 = accuracy_score(y_test_level2, y_pred_level2)
        print(f"üìä Accuracy: {accuracy_level2:.4f}")
        print(f"üìä Classification Report:")
        print(classification_report(y_test_level2, y_pred_level2))
        
        # L∆∞u k·∫øt qu·∫£
        results = {
            'level1': {
                'accuracy': accuracy_level1,
                'classification_report': classification_report(y_test_level1, y_pred_level1, output_dict=True)
            },
            'level2': {
                'accuracy': accuracy_level2,
                'classification_report': classification_report(y_test_level2, y_pred_level2, output_dict=True)
            }
        }
        
        results_path = "results/evaluation_results/svm_evaluation_results.pkl"
        Path(results_path).parent.mkdir(parents=True, exist_ok=True)
        
        with open(results_path, 'wb') as f:
            pickle.dump(results, f)
        
        print(f"\nüíæ K·∫øt qu·∫£ evaluation ƒë√£ ƒë∆∞·ª£c l∆∞u: {results_path}")
        
        return results
        
    except Exception as e:
        print(f"‚ùå L·ªói khi ƒë√°nh gi√°: {e}")
        return None

# ============================================================================
# üöÄ MAIN PIPELINE
# ============================================================================

def main():
    """H√†m ch√≠nh ch·∫°y pipeline"""
    print("üöÄ KH·ªûI ƒê·ªòNG VILEGALBERT PIPELINE CHO GOOGLE COLAB!")
    print("üìä S·ª¨ D·ª§NG DATASET C√ì S·∫¥N")
    print("=" * 80)
    
    # B∆∞·ªõc 1: T·∫°o c·∫•u tr√∫c project
    create_project_structure()
    
    # B∆∞·ªõc 2: Load dataset c√≥ s·∫µn
    print("\nüìä B∆Ø·ªöC 1: LOAD DATASET C√ì S·∫¥N")
    print("-" * 50)
    
    df = load_existing_dataset()
    if df is None:
        print("‚ùå Kh√¥ng th·ªÉ load dataset")
        return
    
    # B∆∞·ªõc 3: Ki·ªÉm tra v√† t·∫°o dataset splits
    print("\nüîÑ B∆Ø·ªöC 2: KI·ªÇM TRA DATASET SPLITS")
    print("-" * 50)
    
    if not check_dataset_splits():
        # T·∫°o splits m·ªõi t·ª´ dataset c√≥ s·∫µn
        dataset_path = "data/processed/hierarchical_legal_dataset.csv"
        splits_dir = "data/processed/dataset_splits"
        create_training_splits_from_existing(dataset_path, splits_dir)
    
    # B∆∞·ªõc 4: Training SVM
    print("\nüèãÔ∏è B∆Ø·ªöC 3: TRAINING SVM")
    print("-" * 50)
    
    trainer = SVMTrainer()
    
    # Training Level 1
    results_level1 = trainer.train_level1("data/processed/hierarchical_legal_dataset.csv")
    
    # Training Level 2
    results_level2 = trainer.train_level2("data/processed/hierarchical_legal_dataset.csv")
    
    # B∆∞·ªõc 5: Evaluation
    print("\nüìä B∆Ø·ªöC 4: EVALUATION")
    print("-" * 50)
    
    test_data_path = "data/processed/dataset_splits/test.csv"
    evaluation_results = evaluate_svm_models(test_data_path)
    
    # T√≥m t·∫Øt k·∫øt qu·∫£
    print("\nüéâ T√ìM T·∫ÆT K·∫æT QU·∫¢")
    print("=" * 80)
    print(f"üìä Dataset: {len(df)} samples")
    print(f"üè∑Ô∏è Level 1 Accuracy: {results_level1['accuracy']:.4f}")
    print(f"üè∑Ô∏è Level 2 Accuracy: {results_level2['accuracy']:.4f}")
    print(f"üíæ Models ƒë√£ ƒë∆∞·ª£c l∆∞u trong th∆∞ m·ª•c models/")
    print(f"üìä K·∫øt qu·∫£ evaluation ƒë√£ ƒë∆∞·ª£c l∆∞u trong th∆∞ m·ª•c results/")
    
    print("\n‚úÖ PIPELINE HO√ÄN TH√ÄNH TH√ÄNH C√îNG!")
    print("üöÄ B·∫°n c√≥ th·ªÉ ti·∫øp t·ª•c v·ªõi training PhoBERT, BiLSTM ho·∫∑c Ensemble!")

if __name__ == "__main__":
    main() 