"""
Script ch√≠nh ƒë·ªÉ ch·∫°y th√≠ nghi·ªám ph√¢n lo·∫°i vƒÉn b·∫£n ph√°p l√Ω
So s√°nh SVM, BiLSTM, v√† PhoBERT
"""
import os
import sys
import pandas as pd
import numpy as np
import torch
from transformers import AutoTokenizer
import warnings
warnings.filterwarnings('ignore')

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from classification.data_loader import DataLoader
from classification.models import SVMClassifier, PhoBERTClassifier, ModelTrainer
from classification.evaluator import ModelEvaluator
from classification.bilstm_model import BiLSTMPipeline
from classification.phobert_model import PhoBERTPipeline

class LegalTextClassificationExperiment:
    """Th√≠ nghi·ªám ph√¢n lo·∫°i vƒÉn b·∫£n ph√°p l√Ω"""
    
    def __init__(self, data_path, text_column='text', label_column='label'):
        self.data_path = data_path
        self.text_column = text_column
        self.label_column = label_column
        self.results = {}
        
        # Kh·ªüi t·∫°o data loader
        self.data_loader = DataLoader(data_path)
        
        # Device cho deep learning
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"S·ª≠ d·ª•ng device: {self.device}")
    
    def load_and_prepare_data(self, sample_size=10000):
        """T·∫£i v√† chu·∫©n b·ªã d·ªØ li·ªáu"""
        print("=== CHU·∫®N B·ªä D·ªÆ LI·ªÜU ===")
        
        # T·∫£i d·ªØ li·ªáu
        texts, labels, class_names = self.data_loader.load_data(
            text_column=self.text_column,
            label_column=self.label_column,
            sample_size=sample_size
        )
        
        # Chia d·ªØ li·ªáu
        X_train, X_val, X_test, y_train, y_val, y_test = self.data_loader.split_data(
            texts, labels, test_size=0.2, val_size=0.1
        )
        
        # L∆∞u d·ªØ li·ªáu
        self.X_train, self.X_val, self.X_test = X_train, X_val, X_test
        self.y_train, self.y_val, self.y_test = y_train, y_val, y_test
        self.class_names = class_names
        self.num_classes = len(class_names)
        
        print(f"S·ªë l·ªõp: {self.num_classes}")
        print(f"T√™n c√°c l·ªõp: {class_names}")
        
        return X_train, X_val, X_test, y_train, y_val, y_test, class_names
    
    def run_svm_experiment(self):
        """Ch·∫°y th√≠ nghi·ªám v·ªõi SVM"""
        print("\n=== TH√ç NGHI·ªÜM SVM ===")
        
        # Kh·ªüi t·∫°o m√¥ h√¨nh
        svm_model = SVMClassifier(max_features=5000, ngram_range=(1, 2))
        
        # Hu·∫•n luy·ªán
        svm_model.train(self.X_train, self.y_train)
        
        # D·ª± ƒëo√°n
        y_pred = svm_model.predict(self.X_test)
        y_pred_proba = svm_model.predict_proba(self.X_test)
        
        # L∆∞u k·∫øt qu·∫£
        self.results['SVM'] = {
            'model': svm_model,
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba
        }
        
        return y_pred, y_pred_proba
    
    def run_phobert_experiment(self, num_epochs=3, batch_size=16, learning_rate=2e-5):
        """Ch·∫°y th√≠ nghi·ªám v·ªõi PhoBERT"""
        print("\n=== TH√ç NGHI·ªÜM PHOBERT ===")
        
        # Kh·ªüi t·∫°o PhoBERT pipeline
        phobert_pipeline = PhoBERTPipeline(
            model_name='vinai/phobert-base',
            max_length=256,
            dropout=0.3
        )
        
        # Prepare data - convert numpy arrays to lists if needed
        if hasattr(self.X_train, 'tolist'):
            texts = self.X_train.tolist() + self.X_val.tolist() + self.X_test.tolist()
            labels = self.y_train.tolist() + self.y_val.tolist() + self.y_test.tolist()
        else:
            texts = list(self.X_train) + list(self.X_val) + list(self.X_test)
            labels = list(self.y_train) + list(self.y_val) + list(self.y_test)
        
        X_train_phobert, X_val_phobert, X_test_phobert, y_train_phobert, y_val_phobert, y_test_phobert = \
            phobert_pipeline.prepare_data(texts, labels)
        
        # Create data loaders
        train_loader, val_loader, test_loader = phobert_pipeline.create_data_loaders(
            X_train_phobert, X_val_phobert, X_test_phobert, 
            y_train_phobert, y_val_phobert, y_test_phobert,
            batch_size=batch_size
        )
        
        # Build and train model
        phobert_pipeline.build_model(device=self.device)
        
        # Training
        history = phobert_pipeline.train(train_loader, val_loader, num_epochs, learning_rate)
        
        # Evaluation
        test_loss, test_acc, y_pred, y_true = phobert_pipeline.evaluate(test_loader)
        
        print(f"Test Accuracy: {test_acc:.2f}%")
        
        # Convert predictions back to original labels
        y_pred_original = phobert_pipeline.label_encoder.inverse_transform(y_pred)
        y_true_original = phobert_pipeline.label_encoder.inverse_transform(y_true)
        
        # L∆∞u k·∫øt qu·∫£
        self.results['PhoBERT'] = {
            'model': phobert_pipeline,
            'y_pred': y_pred_original,
            'y_true': y_true_original,
            'training_history': history
        }
        
        return y_pred_original, y_true_original
    
    def run_bilstm_experiment(self, num_epochs=5, batch_size=32, learning_rate=0.001):
        """Ch·∫°y th√≠ nghi·ªám v·ªõi BiLSTM"""
        print("\n=== TH√ç NGHI·ªÜM BILSTM ===")
        
        # Kh·ªüi t·∫°o BiLSTM pipeline
        bilstm_pipeline = BiLSTMPipeline(
            max_vocab_size=8000,
            max_seq_length=128,
            embedding_dim=128,
            hidden_dim=64,
            num_layers=2,
            dropout=0.3
        )
        
        # Prepare data - convert numpy arrays to lists if needed
        if hasattr(self.X_train, 'tolist'):
            texts = self.X_train.tolist() + self.X_val.tolist() + self.X_test.tolist()
            labels = self.y_train.tolist() + self.y_val.tolist() + self.y_test.tolist()
        else:
            texts = list(self.X_train) + list(self.X_val) + list(self.X_test)
            labels = list(self.y_train) + list(self.y_val) + list(self.y_test)
        
        X_train_bilstm, X_val_bilstm, X_test_bilstm, y_train_bilstm, y_val_bilstm, y_test_bilstm = \
            bilstm_pipeline.prepare_data(texts, labels)
        
        # Create data loaders
        train_loader, val_loader, test_loader = bilstm_pipeline.create_data_loaders(
            X_train_bilstm, X_val_bilstm, X_test_bilstm, 
            y_train_bilstm, y_val_bilstm, y_test_bilstm,
            batch_size=batch_size
        )
        
        # Build and train model
        bilstm_pipeline.build_model(device=self.device)
        
        # Training
        history = bilstm_pipeline.train(train_loader, val_loader, num_epochs, learning_rate)
        
        # Evaluation
        test_loss, test_acc, y_pred, y_true = bilstm_pipeline.evaluate(test_loader)
        
        print(f"Test Accuracy: {test_acc:.2f}%")
        
        # Convert predictions back to original labels
        y_pred_original = bilstm_pipeline.label_encoder.inverse_transform(y_pred)
        y_true_original = bilstm_pipeline.label_encoder.inverse_transform(y_true)
        
        # L∆∞u k·∫øt qu·∫£
        self.results['BiLSTM'] = {
            'model': bilstm_pipeline,
            'y_pred': y_pred_original,
            'y_true': y_true_original,
            'training_history': history
        }
        
        return y_pred_original, y_true_original
    
    def evaluate_all_models(self):
        """ƒê√°nh gi√° t·∫•t c·∫£ c√°c m√¥ h√¨nh"""
        print("\n=== ƒê√ÅNH GI√Å C√ÅC M√î H√åNH ===")
        
        # Kh·ªüi t·∫°o evaluator
        evaluator = ModelEvaluator(class_names=self.class_names)
        
        # ƒê√°nh gi√° SVM
        if 'SVM' in self.results:
            svm_pred = self.results['SVM']['y_pred']
            evaluator.evaluate_model('SVM', self.y_test, svm_pred)
        
        # ƒê√°nh gi√° PhoBERT
        if 'PhoBERT' in self.results:
            phobert_pred = self.results['PhoBERT']['y_pred']
            phobert_true = self.results['PhoBERT']['y_true']
            evaluator.evaluate_model('PhoBERT', phobert_true, phobert_pred)
        
        # ƒê√°nh gi√° BiLSTM
        if 'BiLSTM' in self.results:
            bilstm_pred = self.results['BiLSTM']['y_pred']
            bilstm_true = self.results['BiLSTM']['y_true']
            evaluator.evaluate_model('BiLSTM', bilstm_true, bilstm_pred)
        
        # So s√°nh c√°c m√¥ h√¨nh
        comparison_df = evaluator.compare_models()
        
        # T·∫°o b√°o c√°o t·ªïng h·ª£p
        evaluator.generate_summary_report()
        
        # V·∫Ω bi·ªÉu ƒë·ªì so s√°nh
        evaluator.plot_comparison()
        
        # Confusion matrices
        for model_name in self.results.keys():
            evaluator.plot_confusion_matrix(model_name)
        
        return evaluator, comparison_df
    
    def run_full_experiment(self, sample_size=10000, phobert_epochs=3):
        """Ch·∫°y th√≠ nghi·ªám ƒë·∫ßy ƒë·ªß"""
        print("üöÄ B·∫ÆT ƒê·∫¶U TH√ç NGHI·ªÜM PH√ÇN LO·∫†I VƒÇN B·∫¢N PH√ÅP L√ù")
        print("="*60)
        
        # 1. Chu·∫©n b·ªã d·ªØ li·ªáu
        self.load_and_prepare_data(sample_size=sample_size)
        
        # 2. Ch·∫°y c√°c th√≠ nghi·ªám
        try:
            # SVM (baseline)
            self.run_svm_experiment()
        except Exception as e:
            print(f"L·ªói khi ch·∫°y SVM: {e}")
        
        try:
            # BiLSTM (deep learning baseline)
            self.run_bilstm_experiment(num_epochs=3, batch_size=32)
        except Exception as e:
            print(f"L·ªói khi ch·∫°y BiLSTM: {e}")
        
        try:
            # PhoBERT (n·∫øu c√≥ GPU/ƒë·ªß t√†i nguy√™n)
            if torch.cuda.is_available() or sample_size <= 5000:
                self.run_phobert_experiment(num_epochs=phobert_epochs)
            else:
                print("B·ªè qua PhoBERT do thi·∫øu GPU v√† dataset qu√° l·ªõn")
        except Exception as e:
            print(f"L·ªói khi ch·∫°y PhoBERT: {e}")
        
        # 3. ƒê√°nh gi√° v√† so s√°nh
        evaluator, results_df = self.evaluate_all_models()
        
        print("\n‚úÖ HO√ÄN TH√ÄNH TH√ç NGHI·ªÜM")
        
        return evaluator, results_df

def main():
    """H√†m ch√≠nh"""
    # C·∫•u h√¨nh th√≠ nghi·ªám
    DATA_PATH = "data/sent_truncated_vbpl_legal_only.csv"  # Thay ƒë·ªïi path ph√π h·ª£p
    TEXT_COLUMN = "text"  # Thay ƒë·ªïi t√™n c·ªôt ph√π h·ª£p
    LABEL_COLUMN = "label"  # Thay ƒë·ªïi t√™n c·ªôt ph√π h·ª£p
    SAMPLE_SIZE = 5000  # Gi·∫£m ƒë·ªÉ test nhanh
    
    # Ki·ªÉm tra file t·ªìn t·∫°i
    if not os.path.exists(DATA_PATH):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu: {DATA_PATH}")
        print("Vui l√≤ng c·∫≠p nh·∫≠t ƒë∆∞·ªùng d·∫´n file trong bi·∫øn DATA_PATH")
        return
    
    # Ch·∫°y th√≠ nghi·ªám
    experiment = LegalTextClassificationExperiment(
        data_path=DATA_PATH,
        text_column=TEXT_COLUMN,
        label_column=LABEL_COLUMN
    )
    
    evaluator, results_df = experiment.run_full_experiment(
        sample_size=SAMPLE_SIZE,
        phobert_epochs=2  # Gi·∫£m epochs ƒë·ªÉ test nhanh
    )
    
    # L∆∞u k·∫øt qu·∫£
    results_df.to_csv('classification_results.csv', index=False)
    print(f"\nüíæ ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o classification_results.csv")

if __name__ == "__main__":
    main() 