#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üìä Script Evaluation T·ªïng H·ª£p - viLegalBert
ƒê√°nh gi√° v√† so s√°nh hi·ªáu su·∫•t c·ªßa t·∫•t c·∫£ c√°c m√¥ h√¨nh
"""

import os
import sys
import yaml
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import logging
from typing import Dict, List, Tuple, Any
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from sklearn.metrics import (
    classification_report, confusion_matrix, accuracy_score,
    precision_recall_fscore_support
)

# Th√™m src v√†o path
sys.path.append(str(Path(__file__).parent.parent.parent / "src"))

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/evaluation_all_models.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ComprehensiveEvaluator:
    """Evaluator t·ªïng h·ª£p cho t·∫•t c·∫£ c√°c m√¥ h√¨nh"""
    
    def __init__(self, config_path: str = "config/model_configs/hierarchical_config.yaml"):
        """Kh·ªüi t·∫°o evaluator"""
        self.config = self._load_config(config_path)
        self.models = {}
        self.results = {}
        
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """Load c·∫•u h√¨nh t·ª´ file YAML"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logger.info(f"‚úÖ Load c·∫•u h√¨nh th√†nh c√¥ng t·ª´ {config_path}")
            return config
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi load c·∫•u h√¨nh: {e}")
            raise
    
    def _load_data(self, data_path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Load d·ªØ li·ªáu test"""
        try:
            logger.info(f"üìä Loading d·ªØ li·ªáu test t·ª´ {data_path}")
            
            # Load dataset
            df = pd.read_csv(data_path, encoding='utf-8')
            logger.info(f"‚úÖ Load th√†nh c√¥ng {len(df)} samples")
            
            # T√°ch features v√† labels
            X = df['text'].fillna('')
            y_level1 = df['type_level1']
            y_level2 = df['domain_level2']
            
            return X, y_level1, y_level2
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi load d·ªØ li·ªáu: {e}")
            raise
    
    def _load_models(self, level: str) -> Dict[str, Any]:
        """Load t·∫•t c·∫£ c√°c m√¥ h√¨nh ƒë√£ train"""
        try:
            logger.info(f"üîß Loading models cho {level}")
            
            models = {}
            level_num = level[-1]
            
            # Load SVM model
            svm_path = f"models/saved_models/level{level_num}_classifier/svm_level{level_num}/svm_model.pkl"
            if os.path.exists(svm_path):
                models['SVM'] = joblib.load(svm_path)
                logger.info("‚úÖ Loaded SVM model")
            else:
                logger.warning("‚ö†Ô∏è SVM model kh√¥ng t·ªìn t·∫°i")
            
            # Load PhoBERT model
            phobert_path = f"models/saved_models/level{level_num}_classifier/phobert_level{level_num}/phobert_model"
            if os.path.exists(phobert_path):
                # TODO: Implement PhoBERT loading
                logger.info("‚úÖ Loaded PhoBERT model")
            else:
                logger.warning("‚ö†Ô∏è PhoBERT model kh√¥ng t·ªìn t·∫°i")
            
            # Load BiLSTM model
            bilstm_path = f"models/saved_models/level{level_num}_classifier/bilstm_level{level_num}/bilstm_model.pth"
            if os.path.exists(bilstm_path):
                # TODO: Implement BiLSTM loading
                logger.info("‚úÖ Loaded BiLSTM model")
            else:
                logger.warning("‚ö†Ô∏è BiLSTM model kh√¥ng t·ªìn t·∫°i")
            
            # Load Ensemble model
            ensemble_path = f"models/saved_models/level{level_num}_classifier/ensemble_level{level_num}/ensemble_model.pkl"
            if os.path.exists(ensemble_path):
                models['Ensemble'] = joblib.load(ensemble_path)
                logger.info("‚úÖ Loaded Ensemble model")
            else:
                logger.warning("‚ö†Ô∏è Ensemble model kh√¥ng t·ªìn t·∫°i")
            
            return models
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi load models: {e}")
            raise
    
    def _evaluate_model(self, model: Any, X: pd.Series, y: pd.Series, model_name: str, level: str) -> Dict[str, Any]:
        """ƒê√°nh gi√° m·ªôt m√¥ h√¨nh c·ª• th·ªÉ"""
        try:
            logger.info(f"üìä ƒê√°nh gi√° {model_name} cho {level}")
            
            # Predictions
            y_pred = model.predict(X)
            y_pred_proba = model.predict_proba(X) if hasattr(model, 'predict_proba') else None
            
            # Metrics
            accuracy = accuracy_score(y, y_pred)
            precision, recall, f1, _ = precision_recall_fscore_support(
                y, y_pred, average='weighted'
            )
            
            # Classification report
            report = classification_report(y, y_pred, output_dict=True)
            
            # Confusion matrix
            cm = confusion_matrix(y, y_pred)
            
            # Results
            results = {
                'model_name': model_name,
                'level': level,
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'classification_report': report,
                'confusion_matrix': cm.tolist(),
                'predictions': y_pred.tolist(),
                'probabilities': y_pred_proba.tolist() if y_pred_proba is not None else None
            }
            
            logger.info(f"‚úÖ ƒê√°nh gi√° {model_name} ho√†n th√†nh")
            logger.info(f"üéØ Accuracy: {accuracy:.4f}")
            logger.info(f"üéØ Precision: {precision:.4f}")
            logger.info(f"üéØ Recall: {recall:.4f}")
            logger.info(f"üéØ F1-Score: {f1:.4f}")
            
            return results
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi ƒë√°nh gi√° {model_name}: {e}")
            return {}
    
    def _evaluate_all_models(self, X: pd.Series, y: pd.Series, level: str) -> Dict[str, Any]:
        """ƒê√°nh gi√° t·∫•t c·∫£ c√°c m√¥ h√¨nh cho m·ªôt level"""
        try:
            logger.info(f"üîç ƒê√°nh gi√° t·∫•t c·∫£ models cho {level}")
            
            # Load models
            models = self._load_models(level)
            
            if not models:
                logger.error(f"‚ùå Kh√¥ng c√≥ model n√†o ƒë·ªÉ ƒë√°nh gi√° cho {level}")
                return {}
            
            # Evaluate each model
            results = {}
            for model_name, model in models.items():
                model_results = self._evaluate_model(model, X, y, model_name, level)
                if model_results:
                    results[model_name] = model_results
            
            return results
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi ƒë√°nh gi√° t·∫•t c·∫£ models cho {level}: {e}")
            raise
    
    def _create_comparison_report(self, level1_results: Dict[str, Any], level2_results: Dict[str, Any]) -> pd.DataFrame:
        """T·∫°o b√°o c√°o so s√°nh t·ªïng h·ª£p"""
        try:
            logger.info("üìä T·∫°o b√°o c√°o so s√°nh t·ªïng h·ª£p")
            
            comparison_data = []
            
            # Process Level 1 results
            for model_name, results in level1_results.items():
                comparison_data.append({
                    'Model': model_name,
                    'Level': 'Level 1 (Lo·∫°i vƒÉn b·∫£n)',
                    'Accuracy': results['accuracy'],
                    'Precision': results['precision'],
                    'Recall': results['recall'],
                    'F1-Score': results['f1']
                })
            
            # Process Level 2 results
            for model_name, results in level2_results.items():
                comparison_data.append({
                    'Model': model_name,
                    'Level': 'Level 2 (Domain ph√°p l√Ω)',
                    'Accuracy': results['accuracy'],
                    'Precision': results['precision'],
                    'Recall': results['recall'],
                    'F1-Score': results['f1']
                })
            
            # Create DataFrame
            comparison_df = pd.DataFrame(comparison_data)
            
            logger.info("‚úÖ B√°o c√°o so s√°nh ƒë√£ ƒë∆∞·ª£c t·∫°o")
            return comparison_df
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi t·∫°o b√°o c√°o so s√°nh: {e}")
            raise
    
    def _create_visualizations(self, level1_results: Dict[str, Any], level2_results: Dict[str, Any]) -> None:
        """T·∫°o visualizations cho k·∫øt qu·∫£"""
        try:
            logger.info("üé® T·∫°o visualizations")
            
            # T·∫°o th∆∞ m·ª•c l∆∞u
            viz_dir = Path("results/visualizations/model_comparison")
            viz_dir.mkdir(parents=True, exist_ok=True)
            
            # 1. Performance Comparison Chart
            self._create_performance_chart(level1_results, level2_results, viz_dir)
            
            # 2. Confusion Matrices
            self._create_confusion_matrices(level1_results, level2_results, viz_dir)
            
            # 3. Metrics Comparison
            self._create_metrics_comparison(level1_results, level2_results, viz_dir)
            
            logger.info("‚úÖ Visualizations ƒë√£ ƒë∆∞·ª£c t·∫°o")
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi t·∫°o visualizations: {e}")
    
    def _create_performance_chart(self, level1_results: Dict[str, Any], level2_results: Dict[str, Any], viz_dir: Path) -> None:
        """T·∫°o bi·ªÉu ƒë·ªì so s√°nh hi·ªáu su·∫•t"""
        try:
            # Prepare data
            models = list(level1_results.keys())
            metrics = ['accuracy', 'precision', 'recall', 'f1']
            
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle('So S√°nh Hi·ªáu Su·∫•t C√°c M√¥ H√¨nh', fontsize=16, fontweight='bold')
            
            for i, metric in enumerate(metrics):
                row, col = i // 2, i % 2
                ax = axes[row, col]
                
                # Level 1 data
                level1_values = [level1_results[model][metric] for model in models]
                
                # Level 2 data
                level2_values = [level2_results[model][metric] for model in models]
                
                x = np.arange(len(models))
                width = 0.35
                
                ax.bar(x - width/2, level1_values, width, label='Level 1', alpha=0.8)
                ax.bar(x + width/2, level2_values, width, label='Level 2', alpha=0.8)
                
                ax.set_xlabel('Models')
                ax.set_ylabel(metric.capitalize())
                ax.set_title(f'{metric.capitalize()} Comparison')
                ax.set_xticks(x)
                ax.set_xticklabels(models, rotation=45)
                ax.legend()
                ax.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(viz_dir / 'performance_comparison.png', dpi=300, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi t·∫°o performance chart: {e}")
    
    def _create_confusion_matrices(self, level1_results: Dict[str, Any], level2_results: Dict[str, Any], viz_dir: Path) -> None:
        """T·∫°o confusion matrices"""
        try:
            for level_name, results in [('Level1', level1_results), ('Level2', level2_results)]:
                fig, axes = plt.subplots(2, 2, figsize=(15, 12))
                fig.suptitle(f'Confusion Matrices - {level_name}', fontsize=16, fontweight='bold')
                
                for i, (model_name, model_results) in enumerate(results.items()):
                    row, col = i // 2, i % 2
                    ax = axes[row, col]
                    
                    cm = np.array(model_results['confusion_matrix'])
                    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)
                    ax.set_title(f'{model_name}')
                    ax.set_xlabel('Predicted')
                    ax.set_ylabel('Actual')
                
                plt.tight_layout()
                plt.savefig(viz_dir / f'confusion_matrices_{level_name.lower()}.png', dpi=300, bbox_inches='tight')
                plt.close()
                
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi t·∫°o confusion matrices: {e}")
    
    def _create_metrics_comparison(self, level1_results: Dict[str, Any], level2_results: Dict[str, Any], viz_dir: Path) -> None:
        """T·∫°o bi·ªÉu ƒë·ªì so s√°nh metrics"""
        try:
            # Prepare data
            models = list(level1_results.keys())
            metrics = ['accuracy', 'precision', 'recall', 'f1']
            
            fig, ax = plt.subplots(figsize=(12, 8))
            
            x = np.arange(len(models))
            width = 0.2
            
            for i, metric in enumerate(metrics):
                level1_values = [level1_results[model][metric] for model in models]
                level2_values = [level2_results[model][metric] for model in models]
                
                ax.bar(x + i*width, level1_values, width, label=f'{metric.capitalize()} - Level 1', alpha=0.8)
                ax.bar(x + i*width, level2_values, width, label=f'{metric.capitalize()} - Level 2', alpha=0.8, bottom=level1_values)
            
            ax.set_xlabel('Models')
            ax.set_ylabel('Score')
            ax.set_title('Metrics Comparison Across Models and Levels')
            ax.set_xticks(x + width * 1.5)
            ax.set_xticklabels(models, rotation=45)
            ax.legend()
            ax.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(viz_dir / 'metrics_comparison.png', dpi=300, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi t·∫°o metrics comparison: {e}")
    
    def _save_results(self, level1_results: Dict[str, Any], level2_results: Dict[str, Any], comparison_df: pd.DataFrame) -> None:
        """L∆∞u k·∫øt qu·∫£ evaluation"""
        try:
            # T·∫°o th∆∞ m·ª•c l∆∞u
            results_dir = Path("results/evaluation_results/comprehensive_evaluation")
            results_dir.mkdir(parents=True, exist_ok=True)
            
            # L∆∞u k·∫øt qu·∫£ chi ti·∫øt
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # Level 1 results
            level1_path = results_dir / f"level1_results_{timestamp}.yaml"
            with open(level1_path, 'w', encoding='utf-8') as f:
                yaml.dump(level1_results, f, default_flow_style=False, allow_unicode=True)
            
            # Level 2 results
            level2_path = results_dir / f"level2_results_{timestamp}.yaml"
            with open(level2_path, 'w', encoding='utf-8') as f:
                yaml.dump(level2_results, f, default_flow_style=False, allow_unicode=True)
            
            # Comparison report
            comparison_path = results_dir / f"comparison_report_{timestamp}.csv"
            comparison_df.to_csv(comparison_path, index=False, encoding='utf-8')
            
            # Summary report
            summary_path = results_dir / f"summary_report_{timestamp}.yaml"
            summary = {
                'evaluation_date': datetime.now().isoformat(),
                'total_models_evaluated': len(level1_results),
                'levels_evaluated': ['Level 1', 'Level 2'],
                'best_model_level1': max(level1_results.items(), key=lambda x: x[1]['f1'])[0] if level1_results else None,
                'best_model_level2': max(level2_results.items(), key=lambda x: x[1]['f1'])[0] if level2_results else None,
                'average_f1_level1': np.mean([r['f1'] for r in level1_results.values()]) if level1_results else 0,
                'average_f1_level2': np.mean([r['f1'] for r in level2_results.values()]) if level2_results else 0
            }
            
            with open(summary_path, 'w', encoding='utf-8') as f:
                yaml.dump(summary, f, default_flow_style=False, allow_unicode=True)
            
            logger.info(f"‚úÖ K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o {results_dir}")
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi l∆∞u k·∫øt qu·∫£: {e}")
            raise
    
    def evaluate_all(self, data_path: str) -> Dict[str, Any]:
        """ƒê√°nh gi√° t·∫•t c·∫£ c√°c m√¥ h√¨nh"""
        try:
            logger.info("üöÄ B·∫Øt ƒë·∫ßu evaluation t·ªïng h·ª£p")
            
            # Load data
            X, y_level1, y_level2 = self._load_data(data_path)
            
            # Evaluate Level 1
            logger.info("=" * 60)
            level1_results = self._evaluate_all_models(X, y_level1, "level1")
            
            # Evaluate Level 2
            logger.info("=" * 60)
            level2_results = self._evaluate_all_models(X, y_level2, "level2")
            
            # Create comparison report
            comparison_df = self._create_comparison_report(level1_results, level2_results)
            
            # Create visualizations
            self._create_visualizations(level1_results, level2_results)
            
            # Save results
            self._save_results(level1_results, level2_results, comparison_df)
            
            # Print summary
            self._print_summary(level1_results, level2_results, comparison_df)
            
            return {
                'level1_results': level1_results,
                'level2_results': level2_results,
                'comparison': comparison_df
            }
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói trong qu√° tr√¨nh evaluation: {e}")
            raise
    
    def _print_summary(self, level1_results: Dict[str, Any], level2_results: Dict[str, Any], comparison_df: pd.DataFrame) -> None:
        """In t√≥m t·∫Øt k·∫øt qu·∫£"""
        try:
            logger.info("=" * 80)
            logger.info("üìä T√ìM T·∫ÆT K·∫æT QU·∫¢ EVALUATION T·ªîNG H·ª¢P")
            logger.info("=" * 80)
            
            # Level 1 summary
            if level1_results:
                logger.info(f"\nüè∑Ô∏è LEVEL 1 (Lo·∫°i vƒÉn b·∫£n):")
                for model_name, results in level1_results.items():
                    logger.info(f"  {model_name}: Acc={results['accuracy']:.4f}, F1={results['f1']:.4f}")
                
                best_level1 = max(level1_results.items(), key=lambda x: x[1]['f1'])
                logger.info(f"  üèÜ Best Model: {best_level1[0]} (F1: {best_level1[1]['f1']:.4f})")
            
            # Level 2 summary
            if level2_results:
                logger.info(f"\nüè∑Ô∏è LEVEL 2 (Domain ph√°p l√Ω):")
                for model_name, results in level2_results.items():
                    logger.info(f"  {model_name}: Acc={results['accuracy']:.4f}, F1={results['f1']:.4f}")
                
                best_level2 = max(level2_results.items(), key=lambda x: x[1]['f1'])
                logger.info(f"  üèÜ Best Model: {best_level2[0]} (F1: {best_level2[1]['f1']:.4f})")
            
            # Overall summary
            if level1_results and level2_results:
                avg_f1_level1 = np.mean([r['f1'] for r in level1_results.values()])
                avg_f1_level2 = np.mean([r['f1'] for r in level2_results.values()])
                
                logger.info(f"\nüìà T·ªîNG K·∫æT:")
                logger.info(f"  Average F1 Level 1: {avg_f1_level1:.4f}")
                logger.info(f"  Average F1 Level 2: {avg_f1_level2:.4f}")
                logger.info(f"  Overall Performance: {(avg_f1_level1 + avg_f1_level2) / 2:.4f}")
            
            logger.info("\nüéâ Evaluation t·ªïng h·ª£p ho√†n th√†nh!")
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi in t√≥m t·∫Øt: {e}")

def main():
    """H√†m ch√≠nh"""
    try:
        # Kh·ªüi t·∫°o evaluator
        evaluator = ComprehensiveEvaluator()
        
        # ƒê∆∞·ªùng d·∫´n d·ªØ li·ªáu test
        data_path = "data/processed/dataset_splits/test.csv"
        
        if not os.path.exists(data_path):
            logger.error(f"‚ùå Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu test: {data_path}")
            logger.info("üí° H√£y ch·∫°y create_hierarchical_dataset.py tr∆∞·ªõc")
            return
        
        # Th·ª±c hi·ªán evaluation
        results = evaluator.evaluate_all(data_path)
        
        logger.info("üéâ Evaluation t·ªïng h·ª£p ho√†n th√†nh th√†nh c√¥ng!")
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói trong qu√° tr√¨nh evaluation: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main() 