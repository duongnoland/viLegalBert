#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ Script Training cho mÃ´ hÃ¬nh SVM - viLegalBert
PhÃ¢n loáº¡i vÄƒn báº£n phÃ¡p luáº­t Viá»‡t Nam sá»­ dá»¥ng Support Vector Machine
"""

import os
import sys
import yaml
import pickle
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime
import logging
from typing import Dict, List, Tuple, Any

# ThÃªm src vÃ o path
sys.path.append(str(Path(__file__).parent.parent.parent))

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.metrics import (
    classification_report, confusion_matrix, accuracy_score,
    precision_recall_fscore_support, roc_auc_score
)
from sklearn.calibration import CalibratedClassifierCV
from sklearn.pipeline import Pipeline
import joblib

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/training_svm.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class SVMTrainer:
    """Trainer cho mÃ´ hÃ¬nh SVM"""
    
    def __init__(self, config_path: str = "config/model_configs/svm_config.yaml"):
        """Khá»Ÿi táº¡o trainer"""
        self.config = self._load_config(config_path)
        self.model = None
        self.vectorizer = None
        self.feature_selector = None
        self.pipeline = None
        
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """Load cáº¥u hÃ¬nh tá»« file YAML"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logger.info(f"âœ… Load cáº¥u hÃ¬nh thÃ nh cÃ´ng tá»« {config_path}")
            return config
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi load cáº¥u hÃ¬nh: {e}")
            raise
    
    def _load_data(self, data_path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Load dá»¯ liá»‡u training"""
        try:
            logger.info(f"ğŸ“Š Loading dá»¯ liá»‡u tá»« {data_path}")
            
            # Load dataset
            df = pd.read_csv(data_path, encoding='utf-8')
            logger.info(f"âœ… Load thÃ nh cÃ´ng {len(df)} samples")
            
            # TÃ¡ch features vÃ  labels
            X = df['text'].fillna('')
            y_level1 = df['type_level1']
            y_level2 = df['domain_level2']
            
            logger.info(f"ğŸ“ˆ Sá»‘ lÆ°á»£ng features: {len(X)}")
            logger.info(f"ğŸ·ï¸ Level 1 classes: {y_level1.nunique()}")
            logger.info(f"ğŸ·ï¸ Level 2 classes: {y_level2.nunique()}")
            
            return X, y_level1, y_level2
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi load dá»¯ liá»‡u: {e}")
            raise
    
    def _create_pipeline(self, level: str) -> Pipeline:
        """Táº¡o pipeline cho SVM"""
        try:
            # TF-IDF Vectorizer
            vectorizer = TfidfVectorizer(
                max_features=self.config['feature_extraction']['tfidf']['max_features'],
                min_df=self.config['feature_extraction']['tfidf']['min_df'],
                max_df=self.config['feature_extraction']['tfidf']['max_df'],
                ngram_range=tuple(self.config['feature_extraction']['tfidf']['ngram_range']),
                stop_words=self.config['feature_extraction']['tfidf']['stop_words']
            )
            
            # Feature Selection
            if self.config['feature_extraction']['feature_selection']['enable']:
                feature_selector = SelectKBest(
                    score_func=chi2,
                    k=self.config['feature_extraction']['feature_selection']['k_best']
                )
            else:
                feature_selector = None
            
            # SVM Classifier
            svm_config = self.config['svm']
            svm = SVC(
                kernel=svm_config['kernel'],
                C=svm_config['rbf']['C'] if svm_config['kernel'] == 'rbf' else svm_config['linear']['C'],
                gamma=svm_config['rbf']['gamma'] if svm_config['kernel'] == 'rbf' else 'scale',
                probability=True,
                class_weight='balanced',
                random_state=42,
                max_iter=2000,
                cache_size=200
            )
            
            # Táº¡o pipeline
            if feature_selector:
                pipeline = Pipeline([
                    ('vectorizer', vectorizer),
                    ('feature_selector', feature_selector),
                    ('classifier', svm)
                ])
            else:
                pipeline = Pipeline([
                    ('vectorizer', vectorizer),
                    ('classifier', svm)
                ])
            
            logger.info(f"âœ… Táº¡o pipeline thÃ nh cÃ´ng cho {level}")
            return pipeline
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi táº¡o pipeline: {e}")
            raise
    
    def _hyperparameter_tuning(self, pipeline: Pipeline, X: pd.Series, y: pd.Series) -> Pipeline:
        """Tá»‘i Æ°u hyperparameters"""
        try:
            if not self.config['training']['hyperparameter_tuning']['enable']:
                logger.info("â­ï¸ Bá» qua hyperparameter tuning")
                return pipeline
            
            logger.info("ğŸ” Báº¯t Ä‘áº§u hyperparameter tuning...")
            
            # Grid search parameters
            param_grid = self.config['training']['hyperparameter_tuning']['grid_search']['param_grid']
            
            # Äiá»u chá»‰nh param_grid dá»±a trÃªn pipeline
            if 'feature_selector' in [step[0] for step in pipeline.steps]:
                # Náº¿u cÃ³ feature selector, chá»‰ tune SVM parameters
                svm_params = {f'classifier__{k}': v for k, v in param_grid.items()}
                grid_search = GridSearchCV(
                    pipeline,
                    svm_params,
                    cv=self.config['training']['hyperparameter_tuning']['grid_search']['cv'],
                    scoring=self.config['training']['hyperparameter_tuning']['grid_search']['scoring'],
                    n_jobs=self.config['training']['hyperparameter_tuning']['grid_search']['n_jobs']
                )
            else:
                grid_search = GridSearchCV(
                    pipeline,
                    param_grid,
                    cv=self.config['training']['hyperparameter_tuning']['grid_search']['cv'],
                    scoring=self.config['training']['hyperparameter_tuning']['grid_search']['scoring'],
                    n_jobs=self.config['training']['hyperparameter_tuning']['grid_search']['n_jobs']
                )
            
            # Thá»±c hiá»‡n grid search
            grid_search.fit(X, y)
            
            logger.info(f"âœ… Hyperparameter tuning hoÃ n thÃ nh")
            logger.info(f"ğŸ¯ Best parameters: {grid_search.best_params_}")
            logger.info(f"ğŸ† Best score: {grid_search.best_score_:.4f}")
            
            return grid_search.best_estimator_
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi tuning hyperparameters: {e}")
            logger.warning("âš ï¸ Sá»­ dá»¥ng pipeline gá»‘c")
            return pipeline
    
    def _cross_validation(self, pipeline: Pipeline, X: pd.Series, y: pd.Series) -> None:
        """Thá»±c hiá»‡n cross-validation"""
        try:
            if not self.config['training']['cross_validation']['enable']:
                logger.info("â­ï¸ Bá» qua cross-validation")
                return
            
            logger.info("ğŸ”„ Thá»±c hiá»‡n cross-validation...")
            
            cv_scores = cross_val_score(
                pipeline,
                X,
                y,
                cv=self.config['training']['cross_validation']['cv'],
                scoring='f1_weighted',
                n_jobs=-1
            )
            
            logger.info(f"âœ… Cross-validation hoÃ n thÃ nh")
            logger.info(f"ğŸ“Š CV Scores: {cv_scores}")
            logger.info(f"ğŸ“ˆ Mean CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi thá»±c hiá»‡n cross-validation: {e}")
    
    def _evaluate_model(self, pipeline: Pipeline, X_test: pd.Series, y_test: pd.Series, level: str) -> Dict[str, float]:
        """ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh"""
        try:
            logger.info(f"ğŸ“Š ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh {level}...")
            
            # Predictions
            y_pred = pipeline.predict(X_test)
            y_pred_proba = pipeline.predict_proba(X_test)
            
            # Metrics
            accuracy = accuracy_score(y_test, y_pred)
            precision, recall, f1, _ = precision_recall_fscore_support(
                y_test, y_pred, average='weighted'
            )
            
            # Classification report
            report = classification_report(y_test, y_pred, output_dict=True)
            
            # Confusion matrix
            cm = confusion_matrix(y_test, y_pred)
            
            # Results
            results = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'classification_report': report,
                'confusion_matrix': cm.tolist()
            }
            
            logger.info(f"âœ… ÄÃ¡nh giÃ¡ hoÃ n thÃ nh cho {level}")
            logger.info(f"ğŸ¯ Accuracy: {accuracy:.4f}")
            logger.info(f"ğŸ¯ Precision: {precision:.4f}")
            logger.info(f"ğŸ¯ Recall: {recall:.4f}")
            logger.info(f"ğŸ¯ F1-Score: {f1:.4f}")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh: {e}")
            raise
    
    def _save_model(self, pipeline: Pipeline, level: str, results: Dict[str, float]) -> None:
        """LÆ°u mÃ´ hÃ¬nh vÃ  káº¿t quáº£"""
        try:
            # Táº¡o thÆ° má»¥c lÆ°u
            save_dir = Path(f"models/saved_models/level{level[-1]}_classifier/svm_level{level[-1]}")
            save_dir.mkdir(parents=True, exist_ok=True)
            
            # LÆ°u mÃ´ hÃ¬nh
            model_path = save_dir / "svm_model.pkl"
            joblib.dump(pipeline, model_path)
            
            # LÆ°u káº¿t quáº£
            results_path = save_dir / "evaluation_results.yaml"
            with open(results_path, 'w', encoding='utf-8') as f:
                yaml.dump(results, f, default_flow_style=False, allow_unicode=True)
            
            # LÆ°u metadata
            metadata = {
                'model_type': 'SVM',
                'level': level,
                'training_date': datetime.now().isoformat(),
                'config': self.config,
                'results': results
            }
            
            metadata_path = save_dir / "metadata.yaml"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                yaml.dump(metadata, f, default_flow_style=False, allow_unicode=True)
            
            logger.info(f"âœ… LÆ°u mÃ´ hÃ¬nh thÃ nh cÃ´ng vÃ o {model_path}")
            logger.info(f"âœ… LÆ°u káº¿t quáº£ vÃ o {results_path}")
            logger.info(f"âœ… LÆ°u metadata vÃ o {metadata_path}")
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi lÆ°u mÃ´ hÃ¬nh: {e}")
            raise
    
    def train_level1(self, data_path: str) -> Dict[str, float]:
        """Train mÃ´ hÃ¬nh cho táº§ng 1"""
        try:
            logger.info("ğŸš€ Báº¯t Ä‘áº§u training mÃ´ hÃ¬nh Level 1 (Loáº¡i vÄƒn báº£n)")
            
            # Load dá»¯ liá»‡u
            X, y_level1, _ = self._load_data(data_path)
            
            # Táº¡o pipeline
            pipeline = self._create_pipeline("level1")
            
            # Hyperparameter tuning
            pipeline = self._hyperparameter_tuning(pipeline, X, y_level1)
            
            # Cross-validation
            self._cross_validation(pipeline, X, y_level1)
            
            # Train mÃ´ hÃ¬nh
            logger.info("ğŸ‹ï¸ Training mÃ´ hÃ¬nh...")
            pipeline.fit(X, y_level1)
            
            # ÄÃ¡nh giÃ¡
            results = self._evaluate_model(pipeline, X, y_level1, "Level 1")
            
            # LÆ°u mÃ´ hÃ¬nh
            self._save_model(pipeline, "level1", results)
            
            logger.info("ğŸ‰ Training Level 1 hoÃ n thÃ nh!")
            return results
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi training Level 1: {e}")
            raise
    
    def train_level2(self, data_path: str) -> Dict[str, float]:
        """Train mÃ´ hÃ¬nh cho táº§ng 2"""
        try:
            logger.info("ğŸš€ Báº¯t Ä‘áº§u training mÃ´ hÃ¬nh Level 2 (Domain phÃ¡p lÃ½)")
            
            # Load dá»¯ liá»‡u
            X, _, y_level2 = self._load_data(data_path)
            
            # Táº¡o pipeline
            pipeline = self._create_pipeline("level2")
            
            # Hyperparameter tuning
            pipeline = self._hyperparameter_tuning(pipeline, X, y_level2)
            
            # Cross-validation
            self._cross_validation(pipeline, X, y_level2)
            
            # Train mÃ´ hÃ¬nh
            logger.info("ğŸ‹ï¸ Training mÃ´ hÃ¬nh...")
            pipeline.fit(X, y_level2)
            
            # ÄÃ¡nh giÃ¡
            results = self._evaluate_model(pipeline, X, y_level2, "Level 2")
            
            # LÆ°u mÃ´ hÃ¬nh
            self._save_model(pipeline, "level2", results)
            
            logger.info("ğŸ‰ Training Level 2 hoÃ n thÃ nh!")
            return results
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi training Level 2: {e}")
            raise

def main():
    """HÃ m chÃ­nh"""
    try:
        # Khá»Ÿi táº¡o trainer
        trainer = SVMTrainer()
        
        # ÄÆ°á»ng dáº«n dá»¯ liá»‡u
        data_path = "data/processed/hierarchical_dataset.csv"
        
        if not os.path.exists(data_path):
            logger.error(f"âŒ KhÃ´ng tÃ¬m tháº¥y file dá»¯ liá»‡u: {data_path}")
            logger.info("ğŸ’¡ HÃ£y cháº¡y create_hierarchical_dataset.py trÆ°á»›c")
            return
        
        # Training Level 1
        logger.info("=" * 60)
        results_level1 = trainer.train_level1(data_path)
        
        # Training Level 2
        logger.info("=" * 60)
        results_level2 = trainer.train_level2(data_path)
        
        # TÃ³m táº¯t káº¿t quáº£
        logger.info("=" * 60)
        logger.info("ğŸ“Š TÃ“M Táº®T Káº¾T QUáº¢ TRAINING")
        logger.info("=" * 60)
        logger.info(f"ğŸ¯ Level 1 - Accuracy: {results_level1['accuracy']:.4f}, F1: {results_level1['f1']:.4f}")
        logger.info(f"ğŸ¯ Level 2 - Accuracy: {results_level2['accuracy']:.4f}, F1: {results_level2['f1']:.4f}")
        logger.info("ğŸ‰ Training hoÃ n thÃ nh thÃ nh cÃ´ng!")
        
    except Exception as e:
        logger.error(f"âŒ Lá»—i trong quÃ¡ trÃ¬nh training: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main() 