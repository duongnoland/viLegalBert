#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ Script Training cho mÃ´ hÃ¬nh BiLSTM - viLegalBert
PhÃ¢n loáº¡i vÄƒn báº£n phÃ¡p luáº­t Viá»‡t Nam sá»­ dá»¥ng BiLSTM
"""

import os
import sys
import yaml
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime
import logging
from typing import Dict, List, Tuple, Any
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.feature_extraction.text import TfidfVectorizer
import pickle

# ThÃªm src vÃ o path
sys.path.append(str(Path(__file__).parent.parent.parent / "src"))

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/training_bilstm.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class BiLSTMClassifier(nn.Module):
    """BiLSTM Classifier cho phÃ¢n loáº¡i vÄƒn báº£n"""
    
    def __init__(self, vocab_size: int, embedding_dim: int, hidden_size: int, 
                 num_layers: int, num_classes: int, dropout: float = 0.2):
        super(BiLSTMClassifier, self).__init__()
        
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.lstm = nn.LSTM(
            embedding_dim, 
            hidden_size, 
            num_layers, 
            batch_first=True, 
            bidirectional=True,
            dropout=dropout if num_layers > 1 else 0
        )
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_size * 2, num_classes)  # *2 for bidirectional
        
    def forward(self, x, lengths):
        # x: (batch_size, seq_len)
        embedded = self.embedding(x)
        
        # Pack padded sequence
        packed = nn.utils.rnn.pack_padded_sequence(
            embedded, lengths, batch_first=True, enforce_sorted=False
        )
        
        # LSTM forward pass
        lstm_out, (hidden, cell) = self.lstm(packed)
        
        # Unpack sequence
        unpacked, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)
        
        # Get last hidden state
        last_hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)  # Concatenate forward and backward
        
        # Classification
        output = self.dropout(last_hidden)
        output = self.fc(output)
        
        return output

class BiLSTMTrainer:
    """Trainer cho mÃ´ hÃ¬nh BiLSTM"""
    
    def __init__(self, config_path: str = "config/model_configs/bilstm_config.yaml"):
        """Khá»Ÿi táº¡o trainer"""
        self.config = self._load_config(config_path)
        self.model = None
        self.vectorizer = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        logger.info(f"ğŸ”§ Sá»­ dá»¥ng device: {self.device}")
        
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """Load cáº¥u hÃ¬nh tá»« file YAML"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logger.info(f"âœ… Load cáº¥u hÃ¬nh thÃ nh cÃ´ng tá»« {config_path}")
            return config
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi load cáº¥u hÃ¬nh: {e}")
            raise
    
    def _load_data(self, data_path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Load dá»¯ liá»‡u training"""
        try:
            logger.info(f"ğŸ“Š Loading dá»¯ liá»‡u tá»« {data_path}")
            
            # Load dataset
            df = pd.read_csv(data_path, encoding='utf-8')
            logger.info(f"âœ… Load thÃ nh cÃ´ng {len(df)} samples")
            
            # TÃ¡ch features vÃ  labels
            X = df['text'].fillna('')
            y_level1 = df['type_level1']
            y_level2 = df['domain_level2']
            
            logger.info(f"ğŸ“ˆ Sá»‘ lÆ°á»£ng features: {len(X)}")
            logger.info(f"ğŸ·ï¸ Level 1 classes: {y_level1.nunique()}")
            logger.info(f"ğŸ·ï¸ Level 2 classes: {y_level2.nunique()}")
            
            return X, y_level1, y_level2
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi load dá»¯ liá»‡u: {e}")
            raise
    
    def _setup_vectorizer(self, texts: pd.Series) -> None:
        """Thiáº¿t láº­p TF-IDF vectorizer"""
        try:
            logger.info("ğŸ”§ Thiáº¿t láº­p TF-IDF vectorizer")
            
            self.vectorizer = TfidfVectorizer(
                max_features=self.config['embedding']['max_features'],
                min_df=self.config['embedding']['min_df'],
                max_df=self.config['embedding']['max_df'],
                ngram_range=tuple(self.config['embedding']['ngram_range'])
            )
            
            # Fit vectorizer
            self.vectorizer.fit(texts)
            
            logger.info(f"âœ… Vectorizer Ä‘Ã£ sáºµn sÃ ng vá»›i {len(self.vectorizer.vocabulary_)} features")
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi thiáº¿t láº­p vectorizer: {e}")
            raise
    
    def _text_to_sequence(self, texts: pd.Series, max_length: int = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """Chuyá»ƒn Ä‘á»•i text thÃ nh sequences"""
        try:
            if max_length is None:
                max_length = self.config['model']['max_length']
            
            logger.info(f"ğŸ”§ Chuyá»ƒn Ä‘á»•i {len(texts)} texts thÃ nh sequences")
            
            # Transform texts to TF-IDF features
            features = self.vectorizer.transform(texts)
            
            # Convert to dense array and then to tensor
            features_dense = features.toarray()
            
            # Pad sequences
            padded_features = []
            lengths = []
            
            for feature in features_dense:
                if len(feature) > max_length:
                    padded_features.append(feature[:max_length])
                    lengths.append(max_length)
                else:
                    padded = np.pad(feature, (0, max_length - len(feature)), 'constant')
                    padded_features.append(padded)
                    lengths.append(len(feature))
            
            # Convert to tensors
            features_tensor = torch.tensor(padded_features, dtype=torch.long)
            lengths_tensor = torch.tensor(lengths, dtype=torch.long)
            
            logger.info(f"âœ… Chuyá»ƒn Ä‘á»•i hoÃ n thÃ nh: {features_tensor.shape}")
            
            return features_tensor, lengths_tensor
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi chuyá»ƒn Ä‘á»•i text: {e}")
            raise
    
    def _setup_model(self, num_classes: int, level: str) -> None:
        """Thiáº¿t láº­p mÃ´ hÃ¬nh BiLSTM"""
        try:
            model_config = self.config['model']
            
            self.model = BiLSTMClassifier(
                vocab_size=self.config['embedding']['max_features'],
                embedding_dim=model_config['embedding_dim'],
                hidden_size=model_config['hidden_size'],
                num_layers=model_config['num_layers'],
                num_classes=num_classes,
                dropout=model_config['dropout']
            )
            
            # Move to device
            self.model.to(self.device)
            
            logger.info(f"âœ… MÃ´ hÃ¬nh BiLSTM Ä‘Ã£ sáºµn sÃ ng cho {level}")
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi thiáº¿t láº­p mÃ´ hÃ¬nh: {e}")
            raise
    
    def _prepare_data(self, texts: pd.Series, labels: pd.Series, level: str) -> Tuple[DataLoader, DataLoader]:
        """Chuáº©n bá»‹ dá»¯ liá»‡u cho training"""
        try:
            logger.info(f"ğŸ”§ Chuáº©n bá»‹ dá»¯ liá»‡u cho {level}")
            
            # Convert text to sequences
            features, lengths = self._text_to_sequence(texts)
            
            # Convert labels to tensor
            labels_tensor = torch.tensor(labels.astype('category').cat.codes.values, dtype=torch.long)
            
            # Split train/validation
            X_train, X_val, y_train, y_val, len_train, len_val = train_test_split(
                features, labels_tensor, lengths, test_size=0.2, random_state=42, stratify=labels_tensor
            )
            
            # Create datasets
            train_dataset = TensorDataset(X_train, y_train, len_train)
            val_dataset = TensorDataset(X_val, y_val, len_val)
            
            # Create dataloaders
            batch_size = self.config['training']['batch_size']
            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
            
            logger.info(f"âœ… Dá»¯ liá»‡u Ä‘Ã£ sáºµn sÃ ng: {len(train_loader)} train batches, {len(val_loader)} val batches")
            
            return train_loader, val_loader
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi chuáº©n bá»‹ dá»¯ liá»‡u: {e}")
            raise
    
    def _train_epoch(self, train_loader: DataLoader, criterion: nn.Module, 
                     optimizer: optim.Optimizer) -> float:
        """Train má»™t epoch"""
        self.model.train()
        total_loss = 0
        
        for batch_idx, (data, target, lengths) in enumerate(train_loader):
            data, target = data.to(self.device), target.to(self.device)
            
            optimizer.zero_grad()
            output = self.model(data, lengths)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
            if batch_idx % 100 == 0:
                logger.info(f"Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}")
        
        return total_loss / len(train_loader)
    
    def _validate(self, val_loader: DataLoader, criterion: nn.Module) -> Tuple[float, float]:
        """Validate mÃ´ hÃ¬nh"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target, lengths in val_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data, lengths)
                loss = criterion(output, target)
                
                total_loss += loss.item()
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total += target.size(0)
        
        accuracy = correct / total
        avg_loss = total_loss / len(val_loader)
        
        return avg_loss, accuracy
    
    def _train_model(self, train_loader: DataLoader, val_loader: DataLoader, 
                     num_classes: int, level: str) -> Dict[str, Any]:
        """Train mÃ´ hÃ¬nh"""
        try:
            logger.info(f"ğŸ‹ï¸ Báº¯t Ä‘áº§u training mÃ´ hÃ¬nh {level}")
            
            # Setup training components
            criterion = nn.CrossEntropyLoss()
            optimizer = optim.Adam(
                self.model.parameters(),
                lr=self.config['training']['learning_rate'],
                weight_decay=self.config['training']['weight_decay']
            )
            
            scheduler = optim.lr_scheduler.StepLR(
                optimizer, 
                step_size=self.config['training']['scheduler_step_size'],
                gamma=self.config['training']['scheduler_gamma']
            )
            
            # Training loop
            best_val_acc = 0
            train_losses = []
            val_losses = []
            val_accuracies = []
            
            for epoch in range(self.config['training']['num_epochs']):
                logger.info(f"Epoch {epoch+1}/{self.config['training']['num_epochs']}")
                
                # Train
                train_loss = self._train_epoch(train_loader, criterion, optimizer)
                train_losses.append(train_loss)
                
                # Validate
                val_loss, val_acc = self._validate(val_loader, criterion)
                val_losses.append(val_loss)
                val_accuracies.append(val_acc)
                
                # Update scheduler
                scheduler.step()
                
                logger.info(f"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")
                
                # Save best model
                if val_acc > best_val_acc:
                    best_val_acc = val_acc
                    best_model_state = self.model.state_dict().copy()
            
            # Load best model
            self.model.load_state_dict(best_model_state)
            
            # Final evaluation
            final_val_loss, final_val_acc = self._validate(val_loader, criterion)
            
            results = {
                'best_val_accuracy': best_val_acc,
                'final_val_accuracy': final_val_acc,
                'final_val_loss': final_val_loss,
                'train_losses': train_losses,
                'val_losses': val_losses,
                'val_accuracies': val_accuracies
            }
            
            logger.info(f"âœ… Training hoÃ n thÃ nh cho {level}")
            logger.info(f"ğŸ“Š Best validation accuracy: {best_val_acc:.4f}")
            logger.info(f"ğŸ“Š Final validation accuracy: {final_val_acc:.4f}")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi training mÃ´ hÃ¬nh: {e}")
            raise
    
    def _save_model(self, level: str, results: Dict[str, Any]) -> None:
        """LÆ°u mÃ´ hÃ¬nh vÃ  káº¿t quáº£"""
        try:
            # Táº¡o thÆ° má»¥c lÆ°u
            save_dir = Path(f"models/saved_models/level{level[-1]}_classifier/bilstm_level{level[-1]}")
            save_dir.mkdir(parents=True, exist_ok=True)
            
            # LÆ°u mÃ´ hÃ¬nh
            model_path = save_dir / "bilstm_model.pth"
            torch.save(self.model.state_dict(), model_path)
            
            # LÆ°u vectorizer
            vectorizer_path = save_dir / "vectorizer.pkl"
            with open(vectorizer_path, 'wb') as f:
                pickle.dump(self.vectorizer, f)
            
            # LÆ°u káº¿t quáº£
            results_path = save_dir / "evaluation_results.yaml"
            with open(results_path, 'w', encoding='utf-8') as f:
                yaml.dump(results, f, default_flow_style=False, allow_unicode=True)
            
            # LÆ°u metadata
            metadata = {
                'model_type': 'BiLSTM',
                'level': level,
                'training_date': datetime.now().isoformat(),
                'config': self.config,
                'results': results,
                'device': str(self.device)
            }
            
            metadata_path = save_dir / "metadata.yaml"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                yaml.dump(metadata, f, default_flow_style=False, allow_unicode=True)
            
            logger.info(f"âœ… LÆ°u mÃ´ hÃ¬nh thÃ nh cÃ´ng vÃ o {model_path}")
            logger.info(f"âœ… LÆ°u vectorizer vÃ o {vectorizer_path}")
            logger.info(f"âœ… LÆ°u káº¿t quáº£ vÃ o {results_path}")
            logger.info(f"âœ… LÆ°u metadata vÃ o {metadata_path}")
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi lÆ°u mÃ´ hÃ¬nh: {e}")
            raise
    
    def train_level1(self, data_path: str) -> Dict[str, Any]:
        """Train mÃ´ hÃ¬nh cho táº§ng 1"""
        try:
            logger.info("ğŸš€ Báº¯t Ä‘áº§u training mÃ´ hÃ¬nh Level 1 (Loáº¡i vÄƒn báº£n)")
            
            # Load dá»¯ liá»‡u
            X, y_level1, _ = self._load_data(data_path)
            
            # Setup vectorizer
            self._setup_vectorizer(X)
            
            # Setup model
            num_classes = y_level1.nunique()
            self._setup_model(num_classes, "level1")
            
            # Prepare data
            train_loader, val_loader = self._prepare_data(X, y_level1, "level1")
            
            # Train model
            results = self._train_model(train_loader, val_loader, num_classes, "level1")
            
            # Save model
            self._save_model("level1", results)
            
            logger.info("ğŸ‰ Training Level 1 hoÃ n thÃ nh!")
            return results
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi training Level 1: {e}")
            raise
    
    def train_level2(self, data_path: str) -> Dict[str, Any]:
        """Train mÃ´ hÃ¬nh cho táº§ng 2"""
        try:
            logger.info("ğŸš€ Báº¯t Ä‘áº§u training mÃ´ hÃ¬nh Level 2 (Domain phÃ¡p lÃ½)")
            
            # Load dá»¯ liá»‡u
            X, _, y_level2 = self._load_data(data_path)
            
            # Setup vectorizer (náº¿u chÆ°a cÃ³)
            if self.vectorizer is None:
                self._setup_vectorizer(X)
            
            # Setup model
            num_classes = y_level2.nunique()
            self._setup_model(num_classes, "level2")
            
            # Prepare data
            train_loader, val_loader = self._prepare_data(X, y_level2, "level2")
            
            # Train model
            results = self._train_model(train_loader, val_loader, num_classes, "level2")
            
            # Save model
            self._save_model("level2", results)
            
            logger.info("ğŸ‰ Training Level 2 hoÃ n thÃ nh!")
            return results
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi training Level 2: {e}")
            raise

def main():
    """HÃ m chÃ­nh"""
    try:
        # Khá»Ÿi táº¡o trainer
        trainer = BiLSTMTrainer()
        
        # ÄÆ°á»ng dáº«n dá»¯ liá»‡u
        data_path = "data/processed/hierarchical_legal_dataset.csv"
        
        if not os.path.exists(data_path):
            logger.error(f"âŒ KhÃ´ng tÃ¬m tháº¥y file dá»¯ liá»‡u: {data_path}")
            logger.info("ğŸ’¡ HÃ£y cháº¡y create_hierarchical_dataset.py trÆ°á»›c")
            return
        
        # Training Level 1
        logger.info("=" * 60)
        results_level1 = trainer.train_level1(data_path)
        
        # Training Level 2
        logger.info("=" * 60)
        results_level2 = trainer.train_level2(data_path)
        
        # TÃ³m táº¯t káº¿t quáº£
        logger.info("=" * 60)
        logger.info("ğŸ“Š TÃ“M Táº®T Káº¾T QUáº¢ TRAINING BILSTM")
        logger.info("=" * 60)
        logger.info(f"ğŸ¯ Level 1 - Best Val Acc: {results_level1['best_val_accuracy']:.4f}")
        logger.info(f"ğŸ¯ Level 2 - Best Val Acc: {results_level2['best_val_accuracy']:.4f}")
        logger.info("ğŸ‰ Training BiLSTM hoÃ n thÃ nh thÃ nh cÃ´ng!")
        
    except Exception as e:
        logger.error(f"âŒ Lá»—i trong quÃ¡ trÃ¬nh training: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main() 