#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üöÄ Script Training cho m√¥ h√¨nh PhoBERT - viLegalBert
Ph√¢n lo·∫°i vƒÉn b·∫£n ph√°p lu·∫≠t Vi·ªát Nam s·ª≠ d·ª•ng PhoBERT
"""

import os
import sys
import yaml
import torch
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime
import logging
from typing import Dict, List, Tuple, Any
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Th√™m src v√†o path
sys.path.append(str(Path(__file__).parent.parent.parent / "src"))

from transformers import (
    AutoTokenizer, AutoModel, 
    TrainingArguments, Trainer,
    DataCollatorWithPadding
)
from datasets import Dataset
import evaluate

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/training_phobert.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class PhoBERTTrainer:
    """Trainer cho m√¥ h√¨nh PhoBERT"""
    
    def __init__(self, config_path: str = "config/model_configs/phobert_config.yaml"):
        """Kh·ªüi t·∫°o trainer"""
        self.config = self._load_config(config_path)
        self.tokenizer = None
        self.model = None
        self.trainer = None
        
        # Setup device
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"üîß S·ª≠ d·ª•ng device: {self.device}")
        
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """Load c·∫•u h√¨nh t·ª´ file YAML"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logger.info(f"‚úÖ Load c·∫•u h√¨nh th√†nh c√¥ng t·ª´ {config_path}")
            return config
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi load c·∫•u h√¨nh: {e}")
            raise
    
    def _load_data(self, data_path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Load d·ªØ li·ªáu training"""
        try:
            logger.info(f"üìä Loading d·ªØ li·ªáu t·ª´ {data_path}")
            
            # Load dataset
            df = pd.read_csv(data_path, encoding='utf-8')
            logger.info(f"‚úÖ Load th√†nh c√¥ng {len(df)} samples")
            
            # T√°ch features v√† labels
            X = df['text'].fillna('')
            y_level1 = df['type_level1']
            y_level2 = df['domain_level2']
            
            logger.info(f"üìà S·ªë l∆∞·ª£ng features: {len(X)}")
            logger.info(f"üè∑Ô∏è Level 1 classes: {y_level1.nunique()}")
            logger.info(f"üè∑Ô∏è Level 2 classes: {y_level2.nunique()}")
            
            return X, y_level1, y_level2
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi load d·ªØ li·ªáu: {e}")
            raise
    
    def _setup_tokenizer(self) -> None:
        """Thi·∫øt l·∫≠p tokenizer"""
        try:
            model_name = self.config['tokenizer']['model_name']
            logger.info(f"üîß Thi·∫øt l·∫≠p tokenizer: {model_name}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            
            # Th√™m padding token n·∫øu c·∫ßn
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                
            logger.info(f"‚úÖ Tokenizer ƒë√£ s·∫µn s√†ng")
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi thi·∫øt l·∫≠p tokenizer: {e}")
            raise
    
    def _setup_model(self, num_classes: int, level: str) -> None:
        """Thi·∫øt l·∫≠p m√¥ h√¨nh PhoBERT"""
        try:
            model_name = self.config['model']['model_name']
            logger.info(f"üîß Thi·∫øt l·∫≠p m√¥ h√¨nh: {model_name}")
            
            # Load pretrained model
            base_model = AutoModel.from_pretrained(model_name)
            
            # T·∫°o classifier head
            from transformers import AutoModelForSequenceClassification
            
            self.model = AutoModelForSequenceClassification.from_pretrained(
                model_name,
                num_labels=num_classes,
                problem_type="single_label_classification"
            )
            
            # Move to device
            self.model.to(self.device)
            
            logger.info(f"‚úÖ M√¥ h√¨nh ƒë√£ s·∫µn s√†ng cho {level}")
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi thi·∫øt l·∫≠p m√¥ h√¨nh: {e}")
            raise
    
    def _tokenize_data(self, texts: pd.Series, max_length: int = None) -> Dataset:
        """Tokenize d·ªØ li·ªáu"""
        try:
            if max_length is None:
                max_length = self.config['tokenizer']['max_length']
            
            logger.info(f"üîß Tokenizing {len(texts)} texts v·ªõi max_length={max_length}")
            
            # Tokenize texts
            tokenized = self.tokenizer(
                texts.tolist(),
                truncation=True,
                padding=True,
                max_length=max_length,
                return_tensors="pt"
            )
            
            # Convert to Dataset
            dataset = Dataset.from_dict({
                'input_ids': tokenized['input_ids'],
                'attention_mask': tokenized['attention_mask'],
                'labels': None  # Will be set later
            })
            
            logger.info(f"‚úÖ Tokenization ho√†n th√†nh")
            return dataset
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi tokenize d·ªØ li·ªáu: {e}")
            raise
    
    def _prepare_dataset(self, texts: pd.Series, labels: pd.Series, level: str) -> Dataset:
        """Chu·∫©n b·ªã dataset cho training"""
        try:
            logger.info(f"üîß Chu·∫©n b·ªã dataset cho {level}")
            
            # Tokenize texts
            dataset = self._tokenize_data(texts)
            
            # Add labels
            dataset = dataset.add_column('labels', labels.tolist())
            
            # Split train/validation
            train_size = int(0.8 * len(dataset))
            val_size = len(dataset) - train_size
            
            train_dataset, val_dataset = torch.utils.data.random_split(
                dataset, [train_size, val_size]
            )
            
            logger.info(f"‚úÖ Dataset ƒë√£ s·∫µn s√†ng: {len(train_dataset)} train, {len(val_dataset)} val")
            
            return train_dataset, val_dataset
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi chu·∫©n b·ªã dataset: {e}")
            raise
    
    def _setup_training_args(self, level: str) -> TrainingArguments:
        """Thi·∫øt l·∫≠p training arguments"""
        try:
            training_config = self.config['training']
            
            # T·∫°o output directory
            output_dir = f"models/checkpoints/level{level[-1]}_checkpoints/phobert_{level}"
            Path(output_dir).mkdir(parents=True, exist_ok=True)
            
            training_args = TrainingArguments(
                output_dir=output_dir,
                num_train_epochs=training_config['num_epochs'],
                per_device_train_batch_size=training_config['batch_size'],
                per_device_eval_batch_size=training_config['batch_size'],
                warmup_steps=training_config['warmup_steps'],
                weight_decay=training_config['weight_decay'],
                logging_dir=f"logs/training_logs/phobert_{level}",
                logging_steps=training_config['logging_steps'],
                evaluation_strategy="steps",
                eval_steps=training_config['eval_steps'],
                save_steps=training_config['save_steps'],
                save_total_limit=training_config['save_total_limit'],
                load_best_model_at_end=True,
                metric_for_best_model="eval_f1",
                greater_is_better=True,
                report_to=None,  # Disable wandb for now
                dataloader_num_workers=4,
                dataloader_pin_memory=True,
                fp16=torch.cuda.is_available(),  # Use mixed precision if available
            )
            
            logger.info(f"‚úÖ Training arguments ƒë√£ s·∫µn s√†ng")
            return training_args
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi thi·∫øt l·∫≠p training arguments: {e}")
            raise
    
    def _setup_trainer(self, train_dataset: Dataset, val_dataset: Dataset, level: str) -> Trainer:
        """Thi·∫øt l·∫≠p trainer"""
        try:
            logger.info(f"üîß Thi·∫øt l·∫≠p trainer cho {level}")
            
            # Training arguments
            training_args = self._setup_training_args(level)
            
            # Data collator
            data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)
            
            # Metrics
            metric = evaluate.load("f1")
            
            def compute_metrics(eval_pred):
                predictions, labels = eval_pred
                predictions = np.argmax(predictions, axis=1)
                return metric.compute(predictions=predictions, references=labels, average="weighted")
            
            # Trainer
            self.trainer = Trainer(
                model=self.model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=val_dataset,
                tokenizer=self.tokenizer,
                data_collator=data_collator,
                compute_metrics=compute_metrics,
            )
            
            logger.info(f"‚úÖ Trainer ƒë√£ s·∫µn s√†ng")
            return self.trainer
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi thi·∫øt l·∫≠p trainer: {e}")
            raise
    
    def _train_model(self, level: str) -> Dict[str, Any]:
        """Train m√¥ h√¨nh"""
        try:
            logger.info(f"üèãÔ∏è B·∫Øt ƒë·∫ßu training m√¥ h√¨nh {level}")
            
            # Train
            train_result = self.trainer.train()
            
            # Evaluate
            eval_result = self.trainer.evaluate()
            
            # Save model
            self.trainer.save_model()
            
            logger.info(f"‚úÖ Training ho√†n th√†nh cho {level}")
            logger.info(f"üìä Train loss: {train_result.training_loss:.4f}")
            logger.info(f"üìä Eval loss: {eval_result['eval_loss']:.4f}")
            logger.info(f"üìä Eval F1: {eval_result['eval_f1']:.4f}")
            
            return {
                'train_loss': train_result.training_loss,
                'eval_loss': eval_result['eval_loss'],
                'eval_f1': eval_result['eval_f1'],
                'eval_accuracy': eval_result.get('eval_accuracy', 0.0),
                'eval_precision': eval_result.get('eval_precision', 0.0),
                'eval_recall': eval_result.get('eval_recall', 0.0),
            }
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi training m√¥ h√¨nh: {e}")
            raise
    
    def _save_model(self, level: str, results: Dict[str, Any]) -> None:
        """L∆∞u m√¥ h√¨nh v√† k·∫øt qu·∫£"""
        try:
            # T·∫°o th∆∞ m·ª•c l∆∞u
            save_dir = Path(f"models/saved_models/level{level[-1]}_classifier/phobert_level{level[-1]}")
            save_dir.mkdir(parents=True, exist_ok=True)
            
            # L∆∞u m√¥ h√¨nh
            model_path = save_dir / "phobert_model"
            self.trainer.save_model(str(model_path))
            
            # L∆∞u tokenizer
            tokenizer_path = save_dir / "tokenizer"
            self.tokenizer.save_pretrained(str(tokenizer_path))
            
            # L∆∞u k·∫øt qu·∫£
            results_path = save_dir / "evaluation_results.yaml"
            with open(results_path, 'w', encoding='utf-8') as f:
                yaml.dump(results, f, default_flow_style=False, allow_unicode=True)
            
            # L∆∞u metadata
            metadata = {
                'model_type': 'PhoBERT',
                'level': level,
                'training_date': datetime.now().isoformat(),
                'config': self.config,
                'results': results,
                'device': str(self.device)
            }
            
            metadata_path = save_dir / "metadata.yaml"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                yaml.dump(metadata, f, default_flow_style=False, allow_unicode=True)
            
            logger.info(f"‚úÖ L∆∞u m√¥ h√¨nh th√†nh c√¥ng v√†o {model_path}")
            logger.info(f"‚úÖ L∆∞u tokenizer v√†o {tokenizer_path}")
            logger.info(f"‚úÖ L∆∞u k·∫øt qu·∫£ v√†o {results_path}")
            logger.info(f"‚úÖ L∆∞u metadata v√†o {metadata_path}")
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi l∆∞u m√¥ h√¨nh: {e}")
            raise
    
    def train_level1(self, data_path: str) -> Dict[str, Any]:
        """Train m√¥ h√¨nh cho t·∫ßng 1"""
        try:
            logger.info("üöÄ B·∫Øt ƒë·∫ßu training m√¥ h√¨nh Level 1 (Lo·∫°i vƒÉn b·∫£n)")
            
            # Load d·ªØ li·ªáu
            X, y_level1, _ = self._load_data(data_path)
            
            # Setup tokenizer
            self._setup_tokenizer()
            
            # Setup model
            num_classes = y_level1.nunique()
            self._setup_model(num_classes, "level1")
            
            # Prepare dataset
            train_dataset, val_dataset = self._prepare_dataset(X, y_level1, "level1")
            
            # Setup trainer
            self._setup_trainer(train_dataset, val_dataset, "level1")
            
            # Train model
            results = self._train_model("level1")
            
            # Save model
            self._save_model("level1", results)
            
            logger.info("üéâ Training Level 1 ho√†n th√†nh!")
            return results
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi training Level 1: {e}")
            raise
    
    def train_level2(self, data_path: str) -> Dict[str, Any]:
        """Train m√¥ h√¨nh cho t·∫ßng 2"""
        try:
            logger.info("üöÄ B·∫Øt ƒë·∫ßu training m√¥ h√¨nh Level 2 (Domain ph√°p l√Ω)")
            
            # Load d·ªØ li·ªáu
            X, _, y_level2 = self._load_data(data_path)
            
            # Setup tokenizer (n·∫øu ch∆∞a c√≥)
            if self.tokenizer is None:
                self._setup_tokenizer()
            
            # Setup model
            num_classes = y_level2.nunique()
            self._setup_model(num_classes, "level2")
            
            # Prepare dataset
            train_dataset, val_dataset = self._prepare_dataset(X, y_level2, "level2")
            
            # Setup trainer
            self._setup_trainer(train_dataset, val_dataset, "level2")
            
            # Train model
            results = self._train_model("level2")
            
            # Save model
            self._save_model("level2", results)
            
            logger.info("üéâ Training Level 2 ho√†n th√†nh!")
            return results
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi training Level 2: {e}")
            raise

def main():
    """H√†m ch√≠nh"""
    try:
        # Kh·ªüi t·∫°o trainer
        trainer = PhoBERTTrainer()
        
        # ƒê∆∞·ªùng d·∫´n d·ªØ li·ªáu
        data_path = "data/processed/hierarchical_legal_dataset.csv"
        
        if not os.path.exists(data_path):
            logger.error(f"‚ùå Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu: {data_path}")
            logger.info("üí° H√£y ch·∫°y create_hierarchical_dataset.py tr∆∞·ªõc")
            return
        
        # Training Level 1
        logger.info("=" * 60)
        results_level1 = trainer.train_level1(data_path)
        
        # Training Level 2
        logger.info("=" * 60)
        results_level2 = trainer.train_level2(data_path)
        
        # T√≥m t·∫Øt k·∫øt qu·∫£
        logger.info("=" * 60)
        logger.info("üìä T√ìM T·∫ÆT K·∫æT QU·∫¢ TRAINING PHOBERT")
        logger.info("=" * 60)
        logger.info(f"üéØ Level 1 - F1: {results_level1['eval_f1']:.4f}, Loss: {results_level1['eval_loss']:.4f}")
        logger.info(f"üéØ Level 2 - F1: {results_level2['eval_f1']:.4f}, Loss: {results_level2['eval_loss']:.4f}")
        logger.info("üéâ Training PhoBERT ho√†n th√†nh th√†nh c√¥ng!")
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói trong qu√° tr√¨nh training: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main() 