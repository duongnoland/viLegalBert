#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ Script Training cho mÃ´ hÃ¬nh PhoBERT - viLegalBert
PhÃ¢n loáº¡i vÄƒn báº£n phÃ¡p luáº­t Viá»‡t Nam sá»­ dá»¥ng PhoBERT
"""

import os
import sys
import yaml
import torch
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime
import logging
from typing import Dict, List, Tuple, Any
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# ThÃªm src vÃ o path
sys.path.append(str(Path(__file__).parent.parent.parent / "src"))

from transformers import (
    AutoTokenizer, AutoModel, 
    TrainingArguments, Trainer,
    DataCollatorWithPadding
)
from datasets import Dataset
import evaluate

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/training_phobert.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class PhoBERTTrainer:
    """Trainer cho mÃ´ hÃ¬nh PhoBERT"""
    
    def __init__(self, config_path: str = "config/model_configs/phobert_config.yaml"):
        """Khá»Ÿi táº¡o trainer"""
        self.config = self._load_config(config_path)
        self.tokenizer = None
        self.model = None
        self.trainer = None
        
        # Setup device
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"ğŸ”§ Sá»­ dá»¥ng device: {self.device}")
        
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """Load cáº¥u hÃ¬nh tá»« file YAML"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logger.info(f"âœ… Load cáº¥u hÃ¬nh thÃ nh cÃ´ng tá»« {config_path}")
            return config
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi load cáº¥u hÃ¬nh: {e}")
            raise
    
    def _load_data(self, data_path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Load dá»¯ liá»‡u training"""
        try:
            logger.info(f"ğŸ“Š Loading dá»¯ liá»‡u tá»« {data_path}")
            
            # Load dataset
            df = pd.read_csv(data_path, encoding='utf-8')
            logger.info(f"âœ… Load thÃ nh cÃ´ng {len(df)} samples")
            
            # TÃ¡ch features vÃ  labels
            X = df['text'].fillna('')
            y_level1 = df['type_level1']
            y_level2 = df['domain_level2']
            
            logger.info(f"ğŸ“ˆ Sá»‘ lÆ°á»£ng features: {len(X)}")
            logger.info(f"ğŸ·ï¸ Level 1 classes: {y_level1.nunique()}")
            logger.info(f"ğŸ·ï¸ Level 2 classes: {y_level2.nunique()}")
            
            return X, y_level1, y_level2
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi load dá»¯ liá»‡u: {e}")
            raise
    
    def _setup_tokenizer(self) -> None:
        """Thiáº¿t láº­p tokenizer"""
        try:
            model_name = self.config['tokenizer']['model_name']
            logger.info(f"ğŸ”§ Thiáº¿t láº­p tokenizer: {model_name}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            
            # ThÃªm padding token náº¿u cáº§n
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                
            logger.info(f"âœ… Tokenizer Ä‘Ã£ sáºµn sÃ ng")
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi thiáº¿t láº­p tokenizer: {e}")
            raise
    
    def _setup_model(self, num_classes: int, level: str) -> None:
        """Thiáº¿t láº­p mÃ´ hÃ¬nh PhoBERT"""
        try:
            model_name = self.config['model']['model_name']
            logger.info(f"ğŸ”§ Thiáº¿t láº­p mÃ´ hÃ¬nh: {model_name}")
            
            # Load pretrained model
            base_model = AutoModel.from_pretrained(model_name)
            
            # Táº¡o classifier head
            from transformers import AutoModelForSequenceClassification
            
            self.model = AutoModelForSequenceClassification.from_pretrained(
                model_name,
                num_labels=num_classes,
                problem_type="single_label_classification"
            )
            
            # Move to device
            self.model.to(self.device)
            
            logger.info(f"âœ… MÃ´ hÃ¬nh Ä‘Ã£ sáºµn sÃ ng cho {level}")
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi thiáº¿t láº­p mÃ´ hÃ¬nh: {e}")
            raise
    
    def _tokenize_data(self, texts: pd.Series, max_length: int = None) -> Dataset:
        """Tokenize dá»¯ liá»‡u"""
        try:
            if max_length is None:
                max_length = self.config['tokenizer']['max_length']
            
            logger.info(f"ğŸ”§ Tokenizing {len(texts)} texts vá»›i max_length={max_length}")
            
            # Tokenize texts
            tokenized = self.tokenizer(
                texts.tolist(),
                truncation=True,
                padding=True,
                max_length=max_length,
                return_tensors="pt"
            )
            
            # Convert to Dataset
            dataset = Dataset.from_dict({
                'input_ids': tokenized['input_ids'],
                'attention_mask': tokenized['attention_mask'],
                'labels': None  # Will be set later
            })
            
            logger.info(f"âœ… Tokenization hoÃ n thÃ nh")
            return dataset
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi tokenize dá»¯ liá»‡u: {e}")
            raise
    
    def _prepare_dataset(self, texts: pd.Series, labels: pd.Series, level: str) -> Dataset:
        """Chuáº©n bá»‹ dataset cho training"""
        try:
            logger.info(f"ğŸ”§ Chuáº©n bá»‹ dataset cho {level}")
            
            # Tokenize texts
            dataset = self._tokenize_data(texts)
            
            # Add labels
            dataset = dataset.add_column('labels', labels.tolist())
            
            # Split train/validation
            train_size = int(0.8 * len(dataset))
            val_size = len(dataset) - train_size
            
            train_dataset, val_dataset = torch.utils.data.random_split(
                dataset, [train_size, val_size]
            )
            
            logger.info(f"âœ… Dataset Ä‘Ã£ sáºµn sÃ ng: {len(train_dataset)} train, {len(val_dataset)} val")
            
            return train_dataset, val_dataset
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi chuáº©n bá»‹ dataset: {e}")
            raise
    
    def _setup_training_args(self, level: str) -> TrainingArguments:
        """Thiáº¿t láº­p training arguments"""
        try:
            training_config = self.config['training']
            
            # Táº¡o output directory
            output_dir = f"models/checkpoints/level{level[-1]}_checkpoints/phobert_{level}"
            Path(output_dir).mkdir(parents=True, exist_ok=True)
            
            training_args = TrainingArguments(
                output_dir=output_dir,
                num_train_epochs=training_config['num_epochs'],
                per_device_train_batch_size=training_config['batch_size'],
                per_device_eval_batch_size=training_config['batch_size'],
                warmup_steps=training_config['warmup_steps'],
                weight_decay=training_config['weight_decay'],
                logging_dir=f"logs/training_logs/phobert_{level}",
                logging_steps=training_config['logging_steps'],
                evaluation_strategy="steps",
                eval_steps=training_config['eval_steps'],
                save_steps=training_config['save_steps'],
                save_total_limit=training_config['save_total_limit'],
                load_best_model_at_end=True,
                metric_for_best_model="eval_f1",
                greater_is_better=True,
                report_to=None,  # Disable wandb for now
                dataloader_num_workers=4,
                dataloader_pin_memory=True,
                fp16=torch.cuda.is_available(),  # Use mixed precision if available
            )
            
            logger.info(f"âœ… Training arguments Ä‘Ã£ sáºµn sÃ ng")
            return training_args
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi thiáº¿t láº­p training arguments: {e}")
            raise
    
    def _setup_trainer(self, train_dataset: Dataset, val_dataset: Dataset, level: str) -> Trainer:
        """Thiáº¿t láº­p trainer"""
        try:
            logger.info(f"ğŸ”§ Thiáº¿t láº­p trainer cho {level}")
            
            # Training arguments
            training_args = self._setup_training_args(level)
            
            # Data collator
            data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)
            
            # Metrics
            metric = evaluate.load("f1")
            
            def compute_metrics(eval_pred):
                predictions, labels = eval_pred
                predictions = np.argmax(predictions, axis=1)
                return metric.compute(predictions=predictions, references=labels, average="weighted")
            
            # Trainer
            self.trainer = Trainer(
                model=self.model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=val_dataset,
                tokenizer=self.tokenizer,
                data_collator=data_collator,
                compute_metrics=compute_metrics,
            )
            
            logger.info(f"âœ… Trainer Ä‘Ã£ sáºµn sÃ ng")
            return self.trainer
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi thiáº¿t láº­p trainer: {e}")
            raise
    
    def _train_model(self, level: str) -> Dict[str, Any]:
        """Train mÃ´ hÃ¬nh"""
        try:
            logger.info(f"ğŸ‹ï¸ Báº¯t Ä‘áº§u training mÃ´ hÃ¬nh {level}")
            
            # Train
            train_result = self.trainer.train()
            
            # Evaluate
            eval_result = self.trainer.evaluate()
            
            # Save model
            self.trainer.save_model()
            
            logger.info(f"âœ… Training hoÃ n thÃ nh cho {level}")
            logger.info(f"ğŸ“Š Train loss: {train_result.training_loss:.4f}")
            logger.info(f"ğŸ“Š Eval loss: {eval_result['eval_loss']:.4f}")
            logger.info(f"ğŸ“Š Eval F1: {eval_result['eval_f1']:.4f}")
            
            return {
                'train_loss': train_result.training_loss,
                'eval_loss': eval_result['eval_loss'],
                'eval_f1': eval_result['eval_f1'],
                'eval_accuracy': eval_result.get('eval_accuracy', 0.0),
                'eval_precision': eval_result.get('eval_precision', 0.0),
                'eval_recall': eval_result.get('eval_recall', 0.0),
            }
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi training mÃ´ hÃ¬nh: {e}")
            raise
    
    def _save_model(self, level: str, results: Dict[str, Any]) -> None:
        """LÆ°u mÃ´ hÃ¬nh vÃ  káº¿t quáº£"""
        try:
            # Táº¡o thÆ° má»¥c lÆ°u
            save_dir = Path(f"models/saved_models/level{level[-1]}_classifier/phobert_level{level[-1]}")
            save_dir.mkdir(parents=True, exist_ok=True)
            
            # LÆ°u mÃ´ hÃ¬nh
            model_path = save_dir / "phobert_model"
            self.trainer.save_model(str(model_path))
            
            # LÆ°u tokenizer
            tokenizer_path = save_dir / "tokenizer"
            self.tokenizer.save_pretrained(str(tokenizer_path))
            
            # LÆ°u káº¿t quáº£
            results_path = save_dir / "evaluation_results.yaml"
            with open(results_path, 'w', encoding='utf-8') as f:
                yaml.dump(results, f, default_flow_style=False, allow_unicode=True)
            
            # LÆ°u metadata
            metadata = {
                'model_type': 'PhoBERT',
                'level': level,
                'training_date': datetime.now().isoformat(),
                'config': self.config,
                'results': results,
                'device': str(self.device)
            }
            
            metadata_path = save_dir / "metadata.yaml"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                yaml.dump(metadata, f, default_flow_style=False, allow_unicode=True)
            
            logger.info(f"âœ… LÆ°u mÃ´ hÃ¬nh thÃ nh cÃ´ng vÃ o {model_path}")
            logger.info(f"âœ… LÆ°u tokenizer vÃ o {tokenizer_path}")
            logger.info(f"âœ… LÆ°u káº¿t quáº£ vÃ o {results_path}")
            logger.info(f"âœ… LÆ°u metadata vÃ o {metadata_path}")
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi lÆ°u mÃ´ hÃ¬nh: {e}")
            raise
    
    def train_level1(self, data_path: str) -> Dict[str, Any]:
        """Train mÃ´ hÃ¬nh cho táº§ng 1"""
        try:
            logger.info("ğŸš€ Báº¯t Ä‘áº§u training mÃ´ hÃ¬nh Level 1 (Loáº¡i vÄƒn báº£n)")
            
            # Load dá»¯ liá»‡u
            X, y_level1, _ = self._load_data(data_path)
            
            # Setup tokenizer
            self._setup_tokenizer()
            
            # Setup model
            num_classes = y_level1.nunique()
            self._setup_model(num_classes, "level1")
            
            # Prepare dataset
            train_dataset, val_dataset = self._prepare_dataset(X, y_level1, "level1")
            
            # Setup trainer
            self._setup_trainer(train_dataset, val_dataset, "level1")
            
            # Train model
            results = self._train_model("level1")
            
            # Save model
            self._save_model("level1", results)
            
            logger.info("ğŸ‰ Training Level 1 hoÃ n thÃ nh!")
            return results
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi training Level 1: {e}")
            raise
    
    def train_level2(self, data_path: str) -> Dict[str, Any]:
        """Train mÃ´ hÃ¬nh cho táº§ng 2"""
        try:
            logger.info("ğŸš€ Báº¯t Ä‘áº§u training mÃ´ hÃ¬nh Level 2 (Domain phÃ¡p lÃ½)")
            
            # Load dá»¯ liá»‡u
            X, _, y_level2 = self._load_data(data_path)
            
            # Setup tokenizer (náº¿u chÆ°a cÃ³)
            if self.tokenizer is None:
                self._setup_tokenizer()
            
            # Setup model
            num_classes = y_level2.nunique()
            self._setup_model(num_classes, "level2")
            
            # Prepare dataset
            train_dataset, val_dataset = self._prepare_dataset(X, y_level2, "level2")
            
            # Setup trainer
            self._setup_trainer(train_dataset, val_dataset, "level2")
            
            # Train model
            results = self._train_model("level2")
            
            # Save model
            self._save_model("level2", results)
            
            logger.info("ğŸ‰ Training Level 2 hoÃ n thÃ nh!")
            return results
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i khi training Level 2: {e}")
            raise

def main():
    """HÃ m chÃ­nh"""
    try:
        # Khá»Ÿi táº¡o trainer
        trainer = PhoBERTTrainer()
        
        # ÄÆ°á»ng dáº«n dá»¯ liá»‡u
        data_path = "data/processed/hierarchical_legal_dataset.csv"
        
        if not os.path.exists(data_path):
            logger.error(f"âŒ KhÃ´ng tÃ¬m tháº¥y file dá»¯ liá»‡u: {data_path}")
            logger.info("ğŸ’¡ HÃ£y cháº¡y create_hierarchical_dataset.py trÆ°á»›c")
            return
        
        # Training Level 1
        logger.info("=" * 60)
        results_level1 = trainer.train_level1(data_path)
        
        # Training Level 2
        logger.info("=" * 60)
        results_level2 = trainer.train_level2(data_path)
        
        # TÃ³m táº¯t káº¿t quáº£
        logger.info("=" * 60)
        logger.info("ğŸ“Š TÃ“M Táº®T Káº¾T QUáº¢ TRAINING PHOBERT")
        logger.info("=" * 60)
        logger.info(f"ğŸ¯ Level 1 - F1: {results_level1['eval_f1']:.4f}, Loss: {results_level1['eval_loss']:.4f}")
        logger.info(f"ğŸ¯ Level 2 - F1: {results_level2['eval_f1']:.4f}, Loss: {results_level2['eval_loss']:.4f}")
        logger.info("ğŸ‰ Training PhoBERT hoÃ n thÃ nh thÃ nh cÃ´ng!")
        
    except Exception as e:
        logger.error(f"âŒ Lá»—i trong quÃ¡ trÃ¬nh training: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main() 