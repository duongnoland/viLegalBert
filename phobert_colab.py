#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ‹ï¸ PhoBERT Trainer cho Google Colab (Dataset CÃ³ Sáºµn)
PhÃ¢n loáº¡i vÄƒn báº£n phÃ¡p luáº­t Viá»‡t Nam vá»›i PhoBERT
"""

import os
import sys
import json
import pickle
import numpy as np
import pandas as pd
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# CÃ i Ä‘áº·t dependencies
def install_deps():
    try:
        import transformers
        print("âœ… transformers Ä‘Ã£ sáºµn sÃ ng")
    except:
        os.system("pip install transformers")
        print("ğŸ“¦ ÄÃ£ cÃ i Ä‘áº·t transformers")
    
    try:
        import torch
        print("âœ… PyTorch Ä‘Ã£ sáºµn sÃ ng")
    except:
        os.system("pip install torch")
        print("ğŸ“¦ ÄÃ£ cÃ i Ä‘áº·t PyTorch")
    
    try:
        import datasets
        print("âœ… datasets Ä‘Ã£ sáºµn sÃ ng")
    except:
        os.system("pip install datasets")
        print("ğŸ“¦ ÄÃ£ cÃ i Ä‘áº·t datasets")

# Import sau khi cÃ i Ä‘áº·t
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    TrainingArguments, Trainer, DataCollatorWithPadding
)
from datasets import Dataset
import torch
from torch.utils.data import DataLoader
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split

class PhoBERTTrainer:
    """Trainer cho mÃ´ hÃ¬nh PhoBERT"""
    
    def __init__(self):
        """Khá»Ÿi táº¡o trainer"""
        self.model_name = "vinai/phobert-base"
        self.tokenizer = None
        self.model = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸš€ Sá»­ dá»¥ng device: {self.device}")
        
        # Cáº¥u hÃ¬nh training
        self.config = {
            'max_length': 512,
            'batch_size': 8,
            'learning_rate': 2e-5,
            'num_epochs': 3,
            'warmup_steps': 500,
            'weight_decay': 0.01,
            'logging_steps': 100,
            'eval_steps': 500,
            'save_steps': 1000
        }
    
    def load_tokenizer_and_model(self, num_labels):
        """Load tokenizer vÃ  model"""
        print(f"ğŸ“¥ Loading PhoBERT tokenizer vÃ  model...")
        
        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        
        # Load model
        self.model = AutoModelForSequenceClassification.from_pretrained(
            self.model_name,
            num_labels=num_labels,
            problem_type="single_label_classification"
        )
        
        # Chuyá»ƒn model lÃªn device
        self.model.to(self.device)
        
        print(f"âœ… ÄÃ£ load PhoBERT vá»›i {num_labels} labels")
    
    def prepare_dataset(self, texts, labels, max_length=512):
        """Chuáº©n bá»‹ dataset cho PhoBERT"""
        print("ğŸ”„ Chuáº©n bá»‹ dataset...")
        
        # Tokenize texts
        encodings = self.tokenizer(
            texts,
            truncation=True,
            padding=True,
            max_length=max_length,
            return_tensors="pt"
        )
        
        # Táº¡o dataset
        dataset = Dataset.from_dict({
            'input_ids': encodings['input_ids'],
            'attention_mask': encodings['attention_mask'],
            'labels': labels
        })
        
        return dataset
    
    def train_level1(self, data_path: str):
        """Training cho Level 1 (Loáº¡i vÄƒn báº£n)"""
        print("ğŸ·ï¸ Training Level 1 (Loáº¡i vÄƒn báº£n) vá»›i PhoBERT...")
        
        # Load data
        df = pd.read_csv(data_path, encoding='utf-8')
        texts = df['text'].fillna('').tolist()
        
        # Encode labels
        from sklearn.preprocessing import LabelEncoder
        label_encoder = LabelEncoder()
        labels = label_encoder.fit_transform(df['type_level1'])
        num_labels = len(label_encoder.classes_)
        
        print(f"ğŸ“Š Sá»‘ labels: {num_labels}")
        print(f"ğŸ“Š Classes: {label_encoder.classes_}")
        
        # Load model
        self.load_tokenizer_and_model(num_labels)
        
        # Chia data
        train_texts, val_texts, train_labels, val_labels = train_test_split(
            texts, labels, test_size=0.2, random_state=42, stratify=labels
        )
        
        # Chuáº©n bá»‹ datasets
        train_dataset = self.prepare_dataset(train_texts, train_labels, self.config['max_length'])
        val_dataset = self.prepare_dataset(val_labels, val_labels, self.config['max_length'])
        
        # Cáº¥u hÃ¬nh training
        training_args = TrainingArguments(
            output_dir="./phobert_level1_results",
            num_train_epochs=self.config['num_epochs'],
            per_device_train_batch_size=self.config['batch_size'],
            per_device_eval_batch_size=self.config['batch_size'],
            warmup_steps=self.config['warmup_steps'],
            weight_decay=self.config['weight_decay'],
            logging_dir="./phobert_level1_logs",
            logging_steps=self.config['logging_steps'],
            evaluation_strategy="steps",
            eval_steps=self.config['eval_steps'],
            save_steps=self.config['save_steps'],
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            save_total_limit=3,
            report_to=None
        )
        
        # Data collator
        data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)
        
        # Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            tokenizer=self.tokenizer,
            data_collator=data_collator
        )
        
        # Training
        print("ğŸ‹ï¸ Báº¯t Ä‘áº§u training...")
        trainer.train()
        
        # Evaluation
        print("ğŸ“Š Evaluation...")
        eval_results = trainer.evaluate()
        
        # LÆ°u model
        model_path = "models/saved_models/level1_classifier/phobert_level1/phobert_level1_model"
        Path(model_path).parent.mkdir(parents=True, exist_ok=True)
        
        trainer.save_model(model_path)
        self.tokenizer.save_pretrained(model_path)
        
        # LÆ°u label encoder
        with open(f"{model_path}/label_encoder.pkl", 'wb') as f:
            pickle.dump(label_encoder, f)
        
        print(f"ğŸ’¾ Model Ä‘Ã£ Ä‘Æ°á»£c lÆ°u: {model_path}")
        
        return {
            'model_path': model_path,
            'eval_results': eval_results,
            'label_encoder': label_encoder
        }
    
    def train_level2(self, data_path: str):
        """Training cho Level 2 (Domain phÃ¡p lÃ½)"""
        print("ğŸ·ï¸ Training Level 2 (Domain phÃ¡p lÃ½) vá»›i PhoBERT...")
        
        # Load data
        df = pd.read_csv(data_path, encoding='utf-8')
        texts = df['text'].fillna('').tolist()
        
        # Encode labels
        from sklearn.preprocessing import LabelEncoder
        label_encoder = LabelEncoder()
        labels = label_encoder.fit_transform(df['domain_level2'])
        num_labels = len(label_encoder.classes_)
        
        print(f"ğŸ“Š Sá»‘ labels: {num_labels}")
        print(f"ğŸ“Š Classes: {label_encoder.classes_}")
        
        # Load model
        self.load_tokenizer_and_model(num_labels)
        
        # Chia data
        train_texts, val_texts, train_labels, val_labels = train_test_split(
            texts, labels, test_size=0.2, random_state=42, stratify=labels
        )
        
        # Chuáº©n bá»‹ datasets
        train_dataset = self.prepare_dataset(train_texts, train_labels, self.config['max_length'])
        val_dataset = self.prepare_dataset(val_texts, val_labels, self.config['max_length'])
        
        # Cáº¥u hÃ¬nh training
        training_args = TrainingArguments(
            output_dir="./phobert_level2_results",
            num_train_epochs=self.config['num_epochs'],
            per_device_train_batch_size=self.config['batch_size'],
            per_device_eval_batch_size=self.config['batch_size'],
            warmup_steps=self.config['warmup_steps'],
            weight_decay=self.config['weight_decay'],
            logging_dir="./phobert_level2_logs",
            logging_steps=self.config['logging_steps'],
            evaluation_strategy="steps",
            eval_steps=self.config['eval_steps'],
            save_steps=self.config['save_steps'],
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            save_total_limit=3,
            report_to=None
        )
        
        # Data collator
        data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)
        
        # Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            tokenizer=self.tokenizer,
            data_collator=data_collator
        )
        
        # Training
        print("ğŸ‹ï¸ Báº¯t Ä‘áº§u training...")
        trainer.train()
        
        # Evaluation
        print("ğŸ“Š Evaluation...")
        eval_results = trainer.evaluate()
        
        # LÆ°u model
        model_path = "models/saved_models/level2_classifier/phobert_level2/phobert_level2_model"
        Path(model_path).parent.mkdir(parents=True, exist_ok=True)
        
        trainer.save_model(model_path)
        self.tokenizer.save_pretrained(model_path)
        
        # LÆ°u label encoder
        with open(f"{model_path}/label_encoder.pkl", 'wb') as f:
            pickle.dump(label_encoder, f)
        
        print(f"ğŸ’¾ Model Ä‘Ã£ Ä‘Æ°á»£c lÆ°u: {model_path}")
        
        return {
            'model_path': model_path,
            'eval_results': eval_results,
            'label_encoder': label_encoder
        }

def main():
    """HÃ m chÃ­nh"""
    print("ğŸ‹ï¸ PHOBERT TRAINER CHO GOOGLE COLAB!")
    print("ğŸ“Š Sá»¬ Dá»¤NG DATASET CÃ“ Sáº´N")
    print("=" * 50)
    
    # CÃ i Ä‘áº·t dependencies
    install_deps()
    
    # Táº¡o cáº¥u trÃºc thÆ° má»¥c
    from pathlib import Path
    Path("models/saved_models/level1_classifier/phobert_level1").mkdir(parents=True, exist_ok=True)
    Path("models/saved_models/level2_classifier/phobert_level2").mkdir(parents=True, exist_ok=True)
    
    # Kiá»ƒm tra dataset cÃ³ sáºµn
    dataset_path = "data/processed/hierarchical_legal_dataset.csv"
    if not Path(dataset_path).exists():
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y dataset: {dataset_path}")
        print("ğŸ” TÃ¬m kiáº¿m dataset trong cÃ¡c thÆ° má»¥c...")
        
        possible_paths = [
            "hierarchical_legal_dataset.csv",
            "data/hierarchical_legal_dataset.csv",
            "dataset.csv",
            "legal_dataset.csv"
        ]
        
        for path in possible_paths:
            if Path(path).exists():
                dataset_path = path
                print(f"âœ… TÃ¬m tháº¥y dataset: {dataset_path}")
                break
        else:
            print("âŒ KhÃ´ng tÃ¬m tháº¥y dataset nÃ o. Vui lÃ²ng upload dataset vÃ o Colab")
            return
    
    # Khá»Ÿi táº¡o trainer
    trainer = PhoBERTTrainer()
    
    # Training Level 1
    print("\nğŸ·ï¸ TRAINING LEVEL 1...")
    results_level1 = trainer.train_level1(dataset_path)
    
    # Training Level 2
    print("\nğŸ·ï¸ TRAINING LEVEL 2...")
    results_level2 = trainer.train_level2(dataset_path)
    
    print("\nğŸ‰ PHOBERT TRAINING HOÃ€N THÃ€NH!")
    print(f"ğŸ“Š Level 1 model: {results_level1['model_path']}")
    print(f"ğŸ“Š Level 2 model: {results_level2['model_path']}")

if __name__ == "__main__":
    main() 