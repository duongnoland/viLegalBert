#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üèãÔ∏è PhoBERT Trainer cho Google Colab (GPU Optimized)
Ph√¢n lo·∫°i vƒÉn b·∫£n ph√°p lu·∫≠t Vi·ªát Nam v·ªõi PhoBERT
"""

import os
import pickle
import pandas as pd
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# üöÄ GPU CONFIGURATION
# ============================================================================

def setup_gpu():
    """Thi·∫øt l·∫≠p GPU cho Colab"""
    try:
        import torch
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            
            print(f"‚úÖ GPU: {gpu_name} ({gpu_memory:.1f} GB)")
            
            # Optimize PyTorch
            torch.backends.cudnn.benchmark = True
            os.environ['CUDA_LAUNCH_BLOCKING'] = '0'
            
            return True
        else:
            print("‚ö†Ô∏è GPU kh√¥ng kh·∫£ d·ª•ng, s·ª≠ d·ª•ng CPU")
            return False
            
    except ImportError:
        print("‚ö†Ô∏è PyTorch ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t")
        return False

# ============================================================================
# üì¶ INSTALL DEPENDENCIES
# ============================================================================

def install_deps():
    """C√†i ƒë·∫∑t dependencies c·∫ßn thi·∫øt"""
    try:
        import transformers
        print("‚úÖ transformers ƒë√£ s·∫µn s√†ng")
    except ImportError:
        os.system("pip install transformers")
        print("üì¶ ƒê√£ c√†i ƒë·∫∑t transformers")
    
    try:
        import torch
        if torch.cuda.is_available():
            print("‚úÖ PyTorch v·ªõi CUDA ƒë√£ s·∫µn s√†ng")
        else:
            os.system("pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
    except ImportError:
        os.system("pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
    
    try:
        import datasets
        print("‚úÖ datasets ƒë√£ s·∫µn s√†ng")
    except ImportError:
        os.system("pip install datasets")
        print("üì¶ ƒê√£ c√†i ƒë·∫∑t datasets")

# Import sau khi c√†i ƒë·∫∑t
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    TrainingArguments, Trainer, DataCollatorWithPadding
)
from datasets import Dataset
import torch
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split

class PhoBERTTrainer:
    """Trainer cho m√¥ h√¨nh PhoBERT v·ªõi GPU optimization"""
    
    def __init__(self):
        self.model_name = "vinai/phobert-base"
        self.tokenizer = None
        self.model = None
        
        # Ki·ªÉm tra GPU
        self.use_gpu = setup_gpu()
        self.device = torch.device("cuda" if self.use_gpu else "cpu")
        print(f"üöÄ S·ª≠ d·ª•ng device: {self.device}")
        
        # C·∫•u h√¨nh training t·ªëi ∆∞u cho GPU/CPU
        if self.use_gpu:
            self.config = {
                'max_length': 512,
                'batch_size': 16,
                'learning_rate': 2e-5,
                'num_epochs': 3,
                'warmup_steps': 500,
                'weight_decay': 0.01,
                'gradient_accumulation_steps': 2,
                'fp16': True,
                'dataloader_num_workers': 4
            }
        else:
            self.config = {
                'max_length': 512,
                'batch_size': 8,
                'learning_rate': 2e-5,
                'num_epochs': 3,
                'warmup_steps': 500,
                'weight_decay': 0.01,
                'gradient_accumulation_steps': 1,
                'fp16': False,
                'dataloader_num_workers': 2
            }
        
        print(f"üöÄ PhoBERTTrainer - GPU: {'‚úÖ' if self.use_gpu else '‚ùå'}")
    
    def load_model(self, num_labels):
        """Load tokenizer v√† model"""
        print(f"üì• Loading PhoBERT v·ªõi {num_labels} labels...")
        
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            self.model_name, num_labels=num_labels, problem_type="single_label_classification"
        )
        
        # Chuy·ªÉn model l√™n device
        self.model.to(self.device)
        
        # GPU optimization
        if self.use_gpu:
            if self.config['fp16']:
                self.model = self.model.half()
            torch.cuda.empty_cache()
            print(f"üöÄ Model ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u cho GPU")
        
        print(f"‚úÖ ƒê√£ load PhoBERT")
        print(f"üöÄ Device: {self.device}")
    
    def prepare_dataset(self, texts, labels, max_length=512):
        """Chu·∫©n b·ªã dataset cho PhoBERT"""
        print("üîÑ Chu·∫©n b·ªã dataset...")
        
        encodings = self.tokenizer(
            texts, truncation=True, padding=True, max_length=max_length, return_tensors="pt"
        )
        
        dataset = Dataset.from_dict({
            'input_ids': encodings['input_ids'],
            'attention_mask': encodings['attention_mask'],
            'labels': labels
        })
        
        return dataset
    
    def train_level1(self, data_path):
        """Training cho Level 1"""
        print("üè∑Ô∏è Training Level 1...")
        
        # Load data
        df = pd.read_csv(data_path, encoding='utf-8')
        texts = df['text'].fillna('').tolist()
        
        # Encode labels
        from sklearn.preprocessing import LabelEncoder
        label_encoder = LabelEncoder()
        labels = label_encoder.fit_transform(df['type_level1'])
        num_labels = len(label_encoder.classes_)
        
        print(f"üìä S·ªë labels: {num_labels}")
        print(f"üìä Classes: {label_encoder.classes_}")
        
        # Load model
        self.load_model(num_labels)
        
        # Chia data
        train_texts, val_texts, train_labels, val_labels = train_test_split(
            texts, labels, test_size=0.2, random_state=42, stratify=labels
        )
        
        # Chu·∫©n b·ªã datasets
        train_dataset = self.prepare_dataset(train_texts, train_labels, self.config['max_length'])
        val_dataset = self.prepare_dataset(val_texts, val_labels, self.config['max_length'])
        
        # C·∫•u h√¨nh training
        training_args = TrainingArguments(
            output_dir="./phobert_level1_results",
            num_train_epochs=self.config['num_epochs'],
            per_device_train_batch_size=self.config['batch_size'],
            per_device_eval_batch_size=self.config['batch_size'],
            warmup_steps=self.config['warmup_steps'],
            weight_decay=self.config['weight_decay'],
            logging_dir="./phobert_level1_logs",
            logging_steps=100,
            evaluation_strategy="steps",
            eval_steps=500,
            save_steps=1000,
            save_strategy="steps",
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            save_total_limit=3,
            gradient_accumulation_steps=self.config['gradient_accumulation_steps'],
            fp16=self.config['fp16'],
            dataloader_num_workers=self.config['dataloader_num_workers'],
            report_to=None,
            dataloader_pin_memory=True if self.use_gpu else False,
            remove_unused_columns=False
        )
        
        # Data collator v√† trainer
        data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)
        trainer = Trainer(
            model=self.model, args=training_args, train_dataset=train_dataset,
            eval_dataset=val_dataset, tokenizer=self.tokenizer, data_collator=data_collator
        )
        
        # Training
        print("üèãÔ∏è B·∫Øt ƒë·∫ßu training...")
        if self.use_gpu:
            print(f"üöÄ GPU: Batch size {self.config['batch_size']}, Mixed precision {'‚úÖ' if self.config['fp16'] else '‚ùå'}")
        
        trainer.train()
        
        # Evaluation
        print("üìä Evaluation...")
        eval_results = trainer.evaluate()
        
        # L∆∞u model
        model_path = "models/saved_models/level1_classifier/phobert_level1/phobert_level1_model"
        Path(model_path).parent.mkdir(parents=True, exist_ok=True)
        
        trainer.save_model(model_path)
        self.tokenizer.save_pretrained(model_path)
        
        # L∆∞u label encoder v√† config
        with open(f"{model_path}/label_encoder.pkl", 'wb') as f:
            pickle.dump(label_encoder, f)
        
        with open(f"{model_path}/training_config.pkl", 'wb') as f:
            pickle.dump(self.config, f)
        
        print(f"üíæ Model ƒë√£ l∆∞u: {model_path}")
        return {
            'model_path': model_path,
            'eval_results': eval_results,
            'label_encoder': label_encoder,
            'gpu_optimized': self.use_gpu
        }
    
    def train_level2(self, data_path):
        """Training cho Level 2"""
        print("üè∑Ô∏è Training Level 2...")
        
        # Load data
        df = pd.read_csv(data_path, encoding='utf-8')
        texts = df['text'].fillna('').tolist()
        
        # Encode labels
        from sklearn.preprocessing import LabelEncoder
        label_encoder = LabelEncoder()
        labels = label_encoder.fit_transform(df['domain_level2'])
        num_labels = len(label_encoder.classes_)
        
        print(f"üìä S·ªë labels: {num_labels}")
        print(f"üìä Classes: {label_encoder.classes_}")
        
        # Load model
        self.load_model(num_labels)
        
        # Chia data
        train_texts, val_texts, train_labels, val_labels = train_test_split(
            texts, labels, test_size=0.2, random_state=42, stratify=labels
        )
        
        # Chu·∫©n b·ªã datasets
        train_dataset = self.prepare_dataset(train_texts, train_labels, self.config['max_length'])
        val_dataset = self.prepare_dataset(val_texts, val_labels, self.config['max_length'])
        
        # C·∫•u h√¨nh training
        training_args = TrainingArguments(
            output_dir="./phobert_level2_results",
            num_train_epochs=self.config['num_epochs'],
            per_device_train_batch_size=self.config['batch_size'],
            per_device_eval_batch_size=self.config['batch_size'],
            warmup_steps=self.config['warmup_steps'],
            weight_decay=self.config['weight_decay'],
            logging_dir="./phobert_level2_logs",
            logging_steps=100,
            evaluation_strategy="steps",
            eval_steps=500,
            save_steps=1000,
            save_strategy="steps",
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            save_total_limit=3,
            gradient_accumulation_steps=self.config['gradient_accumulation_steps'],
            fp16=self.config['fp16'],
            dataloader_num_workers=self.config['dataloader_num_workers'],
            report_to=None,
            dataloader_pin_memory=True if self.use_gpu else False,
            remove_unused_columns=False
        )
        
        # Data collator v√† trainer
        data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)
        trainer = Trainer(
            model=self.model, args=training_args, train_dataset=train_dataset,
            eval_dataset=val_dataset, tokenizer=self.tokenizer, data_collator=data_collator
        )
        
        # Training
        print("üèãÔ∏è B·∫Øt ƒë·∫ßu training...")
        if self.use_gpu:
            print(f"üöÄ GPU: Batch size {self.config['batch_size']}, Mixed precision {'‚úÖ' if self.config['fp16'] else '‚ùå'}")
        
        trainer.train()
        
        # Evaluation
        print("üìä Evaluation...")
        eval_results = trainer.evaluate()
        
        # L∆∞u model
        model_path = "models/saved_models/level2_classifier/phobert_level2/phobert_level2_model"
        Path(model_path).parent.mkdir(parents=True, exist_ok=True)
        
        trainer.save_model(model_path)
        self.tokenizer.save_pretrained(model_path)
        
        # L∆∞u label encoder v√† config
        with open(f"{model_path}/label_encoder.pkl", 'wb') as f:
            pickle.dump(label_encoder, f)
        
        with open(f"{model_path}/training_config.pkl", 'wb') as f:
            pickle.dump(self.config, f)
        
        print(f"üíæ Model ƒë√£ l∆∞u: {model_path}")
        return {
            'model_path': model_path,
            'eval_results': eval_results,
            'label_encoder': label_encoder,
            'gpu_optimized': self.use_gpu
        }

def main():
    """H√†m ch√≠nh"""
    print("üèãÔ∏è PHOBERT TRAINER - GPU OPTIMIZED")
    print("=" * 50)
    
    # B∆∞·ªõc 1: GPU setup
    print("\nüöÄ B∆Ø·ªöC 1: GPU SETUP")
    gpu_available = setup_gpu()
    
    # B∆∞·ªõc 2: C√†i ƒë·∫∑t dependencies
    print("\nüì¶ B∆Ø·ªöC 2: C√ÄI ƒê·∫∂T DEPENDENCIES")
    install_deps()
    
    # B∆∞·ªõc 3: T·∫°o th∆∞ m·ª•c
    print("\nüèóÔ∏è B∆Ø·ªöC 3: T·∫†O TH∆Ø M·ª§C")
    Path("models/saved_models/level1_classifier/phobert_level1").mkdir(parents=True, exist_ok=True)
    Path("models/saved_models/level2_classifier/phobert_level2").mkdir(parents=True, exist_ok=True)
    
    # B∆∞·ªõc 4: Ki·ªÉm tra dataset
    print("\nüìä B∆Ø·ªöC 4: KI·ªÇM TRA DATASET")
    dataset_path = "data/processed/hierarchical_legal_dataset.csv"
    if not Path(dataset_path).exists():
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y dataset: {dataset_path}")
        return
    
    # B∆∞·ªõc 5: Ki·ªÉm tra splits
    print("\nüîÑ B∆Ø·ªöC 5: KI·ªÇM TRA SPLITS")
    splits_dir = "data/processed/dataset_splits"
    train_path = Path(splits_dir) / "train.csv"
    val_path = Path(splits_dir) / "validation.csv"
    test_path = Path(splits_dir) / "test.csv"
    
    if train_path.exists() and val_path.exists() and test_path.exists():
        # Load v√† hi·ªÉn th·ªã th√¥ng tin splits
        train_df = pd.read_csv(train_path, encoding='utf-8')
        val_df = pd.read_csv(val_path, encoding='utf-8')
        test_df = pd.read_csv(test_path, encoding='utf-8')
        
        print(f"‚úÖ Dataset splits ƒë√£ c√≥ s·∫µn:")
        print(f"üìä Train set: {len(train_df)} samples")
        print(f"üìä Validation set: {len(val_df)} samples")
        print(f"üìä Test set: {len(test_df)} samples")
    else:
        print("‚ö†Ô∏è Dataset splits ch∆∞a c√≥, vui l√≤ng ch·∫°y main pipeline tr∆∞·ªõc")
        return
    
    # B∆∞·ªõc 6: Kh·ªüi t·∫°o trainer
    print("\nüèãÔ∏è B∆Ø·ªöC 6: KH·ªûI T·∫†O TRAINER")
    trainer = PhoBERTTrainer()
    
    # B∆∞·ªõc 7: Training Level 1
    print("\nüè∑Ô∏è TRAINING LEVEL 1...")
    results_level1 = trainer.train_level1(dataset_path)
    
    # B∆∞·ªõc 8: Training Level 2
    print("\nüè∑Ô∏è TRAINING LEVEL 2...")
    results_level2 = trainer.train_level2(dataset_path)
    
    # T√≥m t·∫Øt k·∫øt qu·∫£
    print("\nüéâ PHOBERT TRAINING HO√ÄN TH√ÄNH!")
    print("=" * 80)
    print(f"üìä Level 1 model: {results_level1['model_path']}")
    print(f"üìä Level 2 model: {results_level2['model_path']}")
    print(f"üöÄ GPU Status: {'‚úÖ Available' if gpu_available else '‚ùå Not Available'}")

if __name__ == "__main__":
    main() 