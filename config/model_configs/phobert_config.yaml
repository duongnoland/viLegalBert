# ðŸš€ Cáº¥u hÃ¬nh mÃ´ hÃ¬nh PhoBERT cho viLegalBert
# PhÃ¢n loáº¡i vÄƒn báº£n phÃ¡p luáº­t Viá»‡t Nam

# ============================================================================
# THÃ”NG TIN MÃ” HÃŒNH
# ============================================================================
model_info:
  name: "PhoBERT"
  type: "Transformer-based"
  base_model: "vinai/phobert-base"
  language: "Vietnamese"
  architecture: "RoBERTa-based"
  
# ============================================================================
# Cáº¤U HÃŒNH TOKENIZER
# ============================================================================
tokenizer:
  model_name: "vinai/phobert-base"
  max_length: 512
  truncation: true
  padding: "max_length"
  return_tensors: "pt"
  
# ============================================================================
# Cáº¤U HÃŒNH ENCODER
# ============================================================================
encoder:
  hidden_size: 768
  num_hidden_layers: 12
  num_attention_heads: 12
  intermediate_size: 3072
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  
# ============================================================================
# Cáº¤U HÃŒNH CLASSIFIER HEAD
# ============================================================================
classifier:
  # Level 1: Loáº¡i vÄƒn báº£n cÆ¡ báº£n
  level1:
    input_size: 768
    hidden_size: 256
    num_classes: 10
    dropout: 0.2
    activation: "relu"
    
  # Level 2: Domain phÃ¡p lÃ½
  level2:
    input_size: 768
    hidden_size: 256
    num_classes: 15
    dropout: 0.2
    activation: "relu"
    
# ============================================================================
# Cáº¤U HÃŒNH TRAINING
# ============================================================================
training:
  # Hyperparameters
  batch_size: 8
  learning_rate: 1e-5
  num_epochs: 15
  warmup_ratio: 0.1
  
  # Optimizer
  optimizer: "AdamW"
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1e-8
  
  # Scheduler
  scheduler: "linear"
  num_training_steps: 1000
  
  # Gradient clipping
  max_grad_norm: 1.0
  
  # Mixed precision
  fp16: true
  
# ============================================================================
# Cáº¤U HÃŒNH FINE-TUNING
# ============================================================================
fine_tuning:
  # Layer freezing
  freeze_encoder_layers: [0, 1, 2]  # Freeze first 3 layers
  
  # Learning rate scheduling
  layer_decay: 0.9  # Decay LR for deeper layers
  
  # Gradual unfreezing
  unfreeze_epochs: [5, 10, 15]  # Unfreeze layers at specific epochs
  
# ============================================================================
# Cáº¤U HÃŒNH REGULARIZATION
# ============================================================================
regularization:
  dropout: 0.1
  label_smoothing: 0.1
  mixup: false
  cutmix: false
  
# ============================================================================
# Cáº¤U HÃŒNH EVALUATION
# ============================================================================
evaluation:
  metrics: ["accuracy", "precision", "recall", "f1", "confusion_matrix"]
  eval_batch_size: 16
  
# ============================================================================
# Cáº¤U HÃŒNH SAVING & LOADING
# ============================================================================
checkpointing:
  save_dir: "models/saved_models/"
  save_steps: 500
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: "f1"
  
# ============================================================================
# Cáº¤U HÃŒNH INFERENCE
# ============================================================================
inference:
  batch_size: 1
  use_cache: true
  return_dict: true 