# üöÄ C·∫•u h√¨nh m√¥ h√¨nh ph√¢n c·∫•p 2 t·∫ßng cho viLegalBert
# Ph√¢n lo·∫°i vƒÉn b·∫£n ph√°p lu·∫≠t Vi·ªát Nam

# ============================================================================
# TH√îNG TIN M√î H√åNH
# ============================================================================
model_info:
  name: "Hierarchical Legal Classifier"
  type: "Multi-level Classification"
  architecture: "Two-stage hierarchical classifier"
  language: "Vietnamese"
  description: "Ph√¢n lo·∫°i vƒÉn b·∫£n ph√°p lu·∫≠t theo 2 t·∫ßng: lo·∫°i vƒÉn b·∫£n v√† domain"
  
# ============================================================================
# C·∫§U H√åNH KI·∫æN TR√öC PH√ÇN C·∫§P
# ============================================================================
architecture:
  # T·∫ßng 1: Ph√¢n lo·∫°i lo·∫°i vƒÉn b·∫£n c∆° b·∫£n
  level1:
    name: "Document Type Classifier"
    classes: [
      "LU·∫¨T",
      "NGH·ªä ƒê·ªäNH", 
      "TH√îNG T∆Ø",
      "NGH·ªä QUY·∫æT",
      "QUY·∫æT ƒê·ªäNH",
      "CH·ªà TH·ªä",
      "PH√ÅP L·ªÜNH",
      "NGH·ªä QUY·∫æT LI√äN T·ªäCH",
      "TH√îNG T∆Ø LI√äN T·ªäCH",
      "NGH·ªä ƒê·ªäNH LI√äN T·ªäCH"
    ]
    num_classes: 10
    description: "Ph√¢n lo·∫°i theo lo·∫°i vƒÉn b·∫£n ph√°p l√Ω"
    
  # T·∫ßng 2: Ph√¢n lo·∫°i domain ph√°p l√Ω
  level2:
    name: "Legal Domain Classifier"
    classes: [
      "H√åNH S·ª∞",
      "D√ÇN S·ª∞",
      "H√ÄNH CH√çNH",
      "LAO ƒê·ªòNG",
      "THU·∫æ",
      "DOANH NGHI·ªÜP",
      "ƒê·∫§T ƒêAI",
      "X√ÇY D·ª∞NG",
      "GIAO TH√îNG",
      "Y T·∫æ",
      "GI√ÅO D·ª§C",
      "T√ÄI CH√çNH",
      "M√îI TR∆Ø·ªúNG",
      "AN NINH",
      "KH√ÅC"
    ]
    num_classes: 15
    description: "Ph√¢n lo·∫°i theo lƒ©nh v·ª±c ph√°p l√Ω chuy√™n bi·ªát"
    
# ============================================================================
# C·∫§U H√åNH BACKBONE MODELS
# ============================================================================
backbone:
  # PhoBERT backbone
  phobert:
    model_name: "vinai/phobert-base"
    max_length: 512
    hidden_size: 768
    use_pooling: true
    pooling_method: "cls"  # cls, mean, max
    
  # BiLSTM backbone
  bilstm:
    hidden_size: 256
    num_layers: 2
    bidirectional: true
    dropout: 0.2
    
  # SVM backbone
  svm:
    kernel: "rbf"
    C: 1.0
    gamma: "scale"
    max_features: 10000
    ngram_range: [1, 3]
    feature_selection: true
    selection_method: "chi2"
    k_best: 5000
    
  # Ensemble approach
  ensemble:
    use_ensemble: true
    ensemble_method: "weighted_average"  # simple_average, weighted_average, voting
    weights: [0.4, 0.3, 0.3]  # [phobert_weight, bilstm_weight, svm_weight]
    
# ============================================================================
# C·∫§U H√åNH FEATURE EXTRACTION
# ============================================================================
feature_extraction:
  # Text preprocessing
  preprocessing:
    remove_special_chars: true
    normalize_unicode: true
    lowercase: false  # Gi·ªØ nguy√™n ch·ªØ hoa cho vƒÉn b·∫£n ph√°p l√Ω
    remove_numbers: false
    max_length: 512
    
  # Feature engineering
  features:
    use_legal_keywords: true
    use_structure_features: true  # chapter, article, etc.
    use_length_features: true
    use_ministry_features: true
    
  # Legal domain keywords
  legal_keywords:
    enable: true
    keyword_weight: 0.3
    update_keywords: true
    
  # TF-IDF features (for SVM)
  tfidf_features:
    enable: true
    max_features: 10000
    ngram_range: [1, 3]
    min_df: 2
    max_df: 0.95
    
# ============================================================================
# C·∫§U H√åNH CLASSIFIER HEADS
# ============================================================================
classifiers:
  # Level 1 Classifier
  level1:
    input_size: 768  # PhoBERT hidden size
    hidden_layers: [512, 256]
    dropout: 0.2
    activation: "relu"
    batch_norm: true
    
  # Level 2 Classifier
  level2:
    input_size: 768  # PhoBERT hidden size + level1 features
    hidden_layers: [512, 256]
    dropout: 0.2
    activation: "relu"
    batch_norm: true
    
    # Conditional classification
    conditional_on_level1: true
    level1_conditioning: "embedding"  # embedding, one_hot, softmax
    
# ============================================================================
# C·∫§U H√åNH TRAINING PH√ÇN C·∫§P
# ============================================================================
training:
  # Training strategy
  strategy: "joint_training"  # separate, joint, curriculum
  
  # Curriculum learning
  curriculum:
    enable: false
    stages: [
      {"epochs": 5, "train_level1": true, "train_level2": false},
      {"epochs": 10, "train_level1": true, "train_level2": true}
    ]
    
  # Loss functions
  loss:
    level1_loss: "CrossEntropyLoss"
    level2_loss: "CrossEntropyLoss"
    joint_loss: "weighted_sum"
    loss_weights: [0.4, 0.6]  # [level1_weight, level2_weight]
    
  # Metrics
  metrics:
    level1: ["accuracy", "precision", "recall", "f1"]
    level2: ["accuracy", "precision", "recall", "f1"]
    joint: ["hierarchical_accuracy", "hierarchical_f1"]
    
# ============================================================================
# C·∫§U H√åNH EVALUATION PH√ÇN C·∫§P
# ============================================================================
evaluation:
  # Hierarchical metrics
  hierarchical_metrics:
    enable: true
    metrics: ["hierarchical_accuracy", "hierarchical_f1", "hierarchical_precision"]
    
  # Error analysis
  error_analysis:
    enable: true
    analyze_level1_errors: true
    analyze_level2_errors: true
    analyze_hierarchical_errors: true
    
  # Confusion matrices
  confusion_matrices:
    level1: true
    level2: true
    hierarchical: true
    
# ============================================================================
# C·∫§U H√åNH INFERENCE
# ============================================================================
inference:
  # Prediction pipeline
  pipeline:
    enable_level1: true
    enable_level2: true
    confidence_threshold: 0.7
    
  # Post-processing
  post_processing:
    enable: true
    methods: ["confidence_filtering", "hierarchical_constraints"]
    
  # Hierarchical constraints
  constraints:
    enable: true
    rules: [
      "level2_prediction_depends_on_level1",
      "domain_specific_constraints"
    ]
    
# ============================================================================
# C·∫§U H√åNH DEPLOYMENT
# ============================================================================
deployment:
  # Model serving
  serving:
    framework: "torchserve"  # torchserve, triton, custom
    batch_size: 1
    max_batch_delay: 100
    
  # API endpoints
  api:
    level1_endpoint: "/predict/level1"
    level2_endpoint: "/predict/level2"
    hierarchical_endpoint: "/predict/hierarchical"
    
  # Monitoring
  monitoring:
    enable: true
    metrics: ["latency", "throughput", "accuracy", "error_rate"] 